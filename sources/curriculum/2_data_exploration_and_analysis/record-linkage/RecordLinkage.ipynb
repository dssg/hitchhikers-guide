{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Record Linkage \n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "When you combine information about from multiple sources, you have to determine whether two individuals in two separate datasets are the same. You also might have multiple individuals with the same name in one dataset and need to decide whether to treat them as the same person or not. This has important implications for your analysis. Record linkage also goes by the terms data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora in various fields. \n",
    "\n",
    "There are several approaches to record linkage that include **exact matching** (for example, joining records based on social security number), **rule-based linking** (applying a hierarchical set of rules that reflect domain knowledge; for example, if two people have the same first and last name and the same birthday they are considered the same); and **probabilistic linking**, or estimating the likelihood that two entities are the same and then deciding on a threshold above which two individuals will be considered to be the same. \n",
    "\n",
    "This tutorial will cover preprocessing data, rule-based linkage and probabilitic linkage using the Felligi-Sunter model. It was adapted from the [Big Data and Social Science Data Linkage tutorial](https://github.com/CSSIP-AIR/Big-Data-Workbooks/blob/master/08.%20Data%20Linkage/Record%20Linkage.ipynb). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Load the Data](#Load-the-Data)\n",
    "2. [Explore the Data](#Data-Exploration)\n",
    "3. [Preprocess the Data](#Preprocess:-clean-Up-Names-and-separate-to-first-middle-and-last-name)\n",
    "4. [Explore Metrics](#String-Comparators)\n",
    "5. [Rule-Based Linkage](#rule-based-linkage)\n",
    "6. [Probabilistic Linkage](#probabilistic-linkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range\n",
    "import pandas as pd\n",
    "import jellyfish\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at grant funding using UC employee data and NSF award data. Given the name of an NSF award, we want to find the job title and employee ID. Our first dataset is all NSF grants awarded between 2010-2012. \n",
    "\n",
    "Given the name of an award match their record with the UC database to get their position and employee id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in NSF awards data\n",
    "df_nsf_awards = pd.read_csv('./data/nsf_awards_2010-2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a first look at the data\n",
    "df_nsf_awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second dataset is a list of all employees in the UC System in 2011. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in UC data\n",
    "df_ucpay = pd.read_csv('./data/ucpay2011.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at what the UC data contains\n",
    "df_ucpay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are some redacted names - some entries just have `********` in the `name` column. How might this affect our analysis? Let's do some data exploration to find out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first 5 rows of the UC data above, we only see 2011 in the `year` column. Let's double check and see if there are any other years contained in this dataset. Looking at all the *unique* values of a column tells us about all the possible values that column can take on, and is a helpful starting point in exploring the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique entries in the 'year' column\n",
    "df_ucpay.year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we were right about the `year` - the UC dataset only covers 2011. How about the `campus` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique entries in the 'campus' column\n",
    "df_ucpay.campus.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a few UC campuses that we recognize, plus `DANR` and `UCOP`. How many records do we have for each campus? We'll do a `groupby` to find out, and plot it so we can visualize the makeup of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at number of entries by campus in the dataset\n",
    "df_ucpay.groupby('campus').size().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique titles\n",
    "df_ucpay.title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how many unique titles are present in the data\n",
    "len(df_ucpay.title.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 2626 unique positions in the system, but we'd expect that people getting grants from NSF are probably a very limited fraction of those. Most likely, NSF grants would go to people whose titles are along the lines of \"Professor,\" \"Postdoctoral Researcher,\" or \"Research Professional\" - it's unlikely that \"Athletics Manager 4\" is applying for or receiving too much NSF funding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows and columns of UC dataset\n",
    "df_ucpay.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only match individuals in the UC data to individuals in the NSF data if we know their names. That means that we'll have to remove all of those redeacted names from the UC dataset - they won't be useful in our analysis. \n",
    "\n",
    "*Note: Whenever you remove observations from your data, think about if the missingness or redaction is informative. Which people are more likely to have their names redacted? Is that related to our outcome of interest?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a mask to keep only entries that do NOT have stars instead of a name\n",
    "mask = df_ucpay.name != \"***********\" \n",
    "\n",
    "# Look at size of this mask\n",
    "df_ucpay[mask].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the number of rows of the mask is 163,429, as opposed to 259,043 in the original UC dataset. This means that about 100,000 rows were missing names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_ucpay with only named entries\n",
    "df_ucpay = df_ucpay[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 15 entries in the updated dataset with redacted names removed\n",
    "df_ucpay.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like that worked - now we've filtered out all the redacted names. \n",
    "\n",
    "For the purposes of our analysis, we only want to know a researcher's name, job title, and the campus where they work. Let's toss the other columns so we have something more manageable to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep \n",
    "selected_columns = ['ID','campus', 'name', 'title']\n",
    "\n",
    "# Subset data frame to only include those columns\n",
    "df_ucpay = df_ucpay[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the updated dataframe\n",
    "df_ucpay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're combining the NSF data with University of *California* data, we really only care about the awards in the dataset that went to people in California. Let's filter the NSF data to only contain entries with `StateCode` 'CA'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mask = df_nsf_awards['StateCode'] == 'CA'\n",
    "df_nsf_awards = df_nsf_awards[state_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have what we need to do our analysis, and not a whole lot of extraneous information. Let's move on to preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocess:-clean-Up-Names-and-separate-to-first-middle-and-last-name'></a>\n",
    "## Preprocessing the Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to connect the NSF and UC data. To do this, we need to figure out which people are the same in both datasets. We need to clean up names and make sure they're in the same format. \n",
    "\n",
    "First let's take a look at the names in the UC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the values in the \"name\" column in the df_ucpay dataframe \n",
    "names_ucpay = df_ucpay.name.values\n",
    "print(names_ucpay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `name` field in the UC data is structured as *LASTNAME* , *FIRSTNAME* *MIDDLE*, where *MIDDLE* can be a middle initial or a full middle name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the values in the \"FirstName\" column in the df_nsf_awards dataframe\n",
    "firstnames_nsf_awards = df_nsf_awards.FirstName.values\n",
    "print(firstnames_nsf_awards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the values in the \"LastName\" column in the df_nsf_awards dataframe\n",
    "lastnames_nsf_awards = df_nsf_awards.LastName.values\n",
    "print(lastnames_nsf_awards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NSF names are formatted differently - we have two separate fields for first and last name, and unlike the UC names (which were all capitalized), these names have the first letter capitalized, and the rest lower case. To be able to directly compare them, we'll have to make them match each other - let's make all the letters lower case, and split the UC names (which are one field) into two fields: first and last name. \n",
    "\n",
    "Remember the rule we noticed about the UC names - they always have the last name, followed by a comma, followed by the first name. If we split each name into two fields at the comma, then the first field will be the last name, and the second field will be the first name (and any middle names). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first name from the UC dataset\n",
    "test_name = names_ucpay[0]\n",
    "\n",
    "# Make UC names lower case \n",
    "test_name = test_name.lower()\n",
    "\n",
    "# Split UC names on comma\n",
    "test_name = test_name.split(',')\n",
    "\n",
    "print(test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! OK, now let's combine all of that into a **function** so we can do this efficiently for *all* of the names in the UC data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_names(name):\n",
    "    \"\"\"\n",
    "    Splits names fields into first, middle and last names\n",
    "    and return lower case values. \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    name: str\n",
    "        e.g. SHAPIRO, JORDAN ISAAC\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (first, middle, last): str\n",
    "        e.g. (jordan, isaac, shapiro)\n",
    "    \"\"\"\n",
    "    \n",
    "    # make the name lower case\n",
    "    name=name.lower()\n",
    "    \n",
    "    # split the name into two fields at the comma\n",
    "    ls_name = name.split(',')\n",
    "\n",
    "    # Since the last name came first, we save the first entry as the last name\n",
    "    # and second entry as the first name\n",
    "    last_name = ls_name[0]\n",
    "    first_middle_name = ls_name[1]\n",
    "    \n",
    "    #split by space to get the first and middle name\n",
    "    ls_first_middle_name = first_middle_name.split()\n",
    "    if len(ls_first_middle_name) > 1:\n",
    "        first_name = ls_first_middle_name[0]\n",
    "        middle_name = ls_first_middle_name[1]\n",
    "    else: \n",
    "        first_name = ls_first_middle_name[0]\n",
    "        middle_name = ''\n",
    "    return unicode(first_name.strip()), unicode(middle_name.strip()), unicode(last_name.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the function above we're taking for granted that a first name can only be one word - we assume that any space means that the following word is the middle name. Of course, there could be someone whose first name contains a space, but we are using the best rule of thumb that we have for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our function to all the names in the UC dataset\n",
    "ls_cleaned_names = [split_names(name) for name in names_ucpay]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we have this huge list of names - that's a little bit unwieldy. What we really want is one list of first names, one list of middle names, and one list of last names (and for them to be in the correct order). Luckily, it's easy to do that in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_first, ls_middle, ls_last = zip(*ls_cleaned_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put colums in the UC dataset for first, middle, and last name\n",
    "df_ucpay['first'] = ls_first\n",
    "df_ucpay['middle'] = ls_middle\n",
    "df_ucpay['last'] = ls_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ucpay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to the NSF data - remember, that dataset had one field for first name, and one field for last name. If we don't have a first name or don't have a last name for anyone, we're not going to be able to identify that person across the two datasets. Let's remove, or \"drop,\" all of the entries in `df_nsf_awards` that have **NA**s, or missing values, for the `FirstName` or `LastName` field.\n",
    "\n",
    "The `inplace=True` option means that we'll just drop the NAs from the existing `df_nsf_awards` dataframe - we don't want to save a copy with NAs for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards.dropna(subset=['FirstName','LastName'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make all of the names lower case, so the format of these names matches the format of the UC names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nsf_awards['first'] = [unicode(name.lower()) for name in df_nsf_awards['FirstName'].values]\n",
    "df_nsf_awards['last'] = [unicode(name.lower()) for name in df_nsf_awards['LastName'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Include a link to some documentation about  encoding*\n",
    ">**Note**: In python2 we have to explicitly tell Python we want a string to be encoding in unicode. In Python3 all strings \n",
    ">are by default unicode. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_nsf_awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Comparators\n",
    "\n",
    "Now that we have clean first and last names for every entry in each dataset, let's explore how we actually match the string fields. It's easy to see how we'd compare numerical fields - if we wanted to compare grant awards, we might subtract one dollar amount from another to see the monetary difference, or look at the number of days between two dates to see how close they were in time.  \n",
    "\n",
    "Comparing text fields, or strings, is a little bit trickier. It's easy to say if two words are *exactly* the same: if they have all of the same letters, in the same order, they're the same word. But how do you quantify how *far away* two words are from each other? One metric is the **edit distance**, or the minimum number of edits it would take to transform one word into the other. The number of insertions (adding a letter), deletions (removing a letter) and substitutions (swapping one letter for another) to transform one string to another is known as the *Levenshtein distance* between the two words. The *Levenshtein-Damerau distance* builds on the Levenshtein distance by including one more step, or transposing two adjacent letters. \n",
    "\n",
    "The *Jaro-Winkler* distance is a fast-to-compute distance metric that returns a normalized score between zero and one: one indicates the words match exactly, and zero indicates no similarity between the two words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringComparators():\n",
    "    \"\"\"\n",
    "    Test various string comparators \n",
    "    \"\"\"\n",
    "\n",
    "    def test_levenshtein_distance():\n",
    "        assert jellyfish.levenshtein_distance('John', 'John') == 0\n",
    "        assert jellyfish.levenshtein_distance('Jon', \"John\") == 2\n",
    "        assert jellyfish.levenshtein_distance('Joseph', 'Joesph') == 1\n",
    "        \n",
    "    def test_damerau_levenshtein():\n",
    "        assert jellyfish.damerau_levenshtein_distance('Joseph', 'Joesph') == 1\n",
    "\n",
    "    def test_jaro_winklear():\n",
    "        assert (np.isclose(jellyfish.jaro_winkler('Joseph', 'Joesph'), 0.955555))\n",
    "        assert (np.isclose(jellyfish.jaro_winkler('Chris', 'Christoper'), 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare all of the first names in the UC dataset with all of the first names in the NSF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the unique names from NSF and UC \n",
    "nsf_firstnames = set( df_nsf_awards['first'].values ) \n",
    "\n",
    "# grab the uc_names\n",
    "uc_firstnames = df_ucpay['first'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of records\n",
    "testname = unicode(uc_firstnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should document this better and uc_names an argument\n",
    "def get_matching_first_name(testname, NUM_NAMES=10):\n",
    "    \"\"\"\n",
    "    get top 10 first names that match\n",
    "    \"\"\"\n",
    "    dict_name_pair = {}\n",
    "    for name in uc_firstnames:\n",
    "        name = unicode(name)\n",
    "        dist = jellyfish.jaro_winkler(testname,name)\n",
    "        dict_name_pair[name] = dist\n",
    "\n",
    "    orddict_dict_name_pair = OrderedDict(\n",
    "                                sorted(dict_name_pair.items(), key=lambda x: x[1]))\n",
    "\n",
    "    ls_sorted_name = list(orddict_dict_name_pair.keys())\n",
    "\n",
    "\n",
    "    return ls_sorted_name[-NUM_NAMES:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testname, get_matching_first_name(testname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nm in uc_firstnames[:25]:\n",
    "    testname = unicode(nm)\n",
    "    print(testname, get_matching_first_name(testname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#rule-based-linkage'></a>\n",
    "## Rule-Based Matching \n",
    "\n",
    "Let's try to merge data with the following rules. \n",
    "\n",
    "1. The first name Jaro-Winkler score has to be greater than 0.90\n",
    "2. The last name Jaro-Winkler score has to be greater then 0.90\n",
    "\n",
    "This rule essentially means that the names have to match with very minor typos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nsf_awards = df_nsf_awards[:10].to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rule_mask(nsf_first_name, \n",
    "                     nsf_last_name,\n",
    "                     df_ucpay,\n",
    "                     first_name_thresh=0.90,\n",
    "                     last_name_thresh=0.90):\n",
    "    \"\"\"\n",
    "    Returns a boolean array of records to match based on a\n",
    "    fixed threshold. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    (nsf_first_name, nsf_last_name): str\n",
    "        first and last name in the NSF dataset\n",
    "        \n",
    "    df_ucpay: DataFrame\n",
    "        DataFrame of the UC directory\n",
    "        \n",
    "    (first_name_thresh,last_name_thresh): int\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    jaro_mask: ls[bool]\n",
    "        boolean list of records to match\n",
    "    \"\"\"\n",
    "    compare_first = lambda x: jellyfish.jaro_winkler(nsf_first_name,x)\n",
    "    compare_last = lambda x: jellyfish.jaro_winkler(nsf_last_name,x)\n",
    "\n",
    "    jaro_first = df_ucpay['first'].map(compare_first) \n",
    "    jaro_last = df_ucpay['last'].map(compare_last)\n",
    "\n",
    "    jaro_mask = (jaro_first > first_name_thresh) & (jaro_last > last_name_thresh)\n",
    "    \n",
    "    return jaro_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_records(dict_nsf_awards, df_ucpay, f_create_rule_mask):\n",
    "    \"\"\"\n",
    "    match records from the nsf and uc datasets based on the fields 'first' and 'last' name\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    dict_nsf_awards: dict\n",
    "        dictionary of nsf awards\n",
    "    df_ucpay: DataFrame\n",
    "        DataFrame of UC employees\n",
    "    create_rule_mask: function\n",
    "        Function that takes a first name, last name and df_ucpay\n",
    "        and returns a Boolean array of whether or not to match \n",
    "        records\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_linked_data: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_linked_data = pd.DataFrame()\n",
    "    for key in dict_nsf_awards.keys():\n",
    "        dict_test_row = dict_nsf_awards[key]\n",
    "    \n",
    "        nsf_first_name = dict_test_row['first']\n",
    "        nsf_last_name = dict_test_row['last']\n",
    "\n",
    "        jaro_mask = f_create_rule_mask(nsf_first_name, nsf_last_name, df_ucpay)\n",
    "    \n",
    "        df_matches = df_ucpay[jaro_mask]\n",
    "        if len(df_matches) == 0:\n",
    "            print('No Match: {} {}'.format(nsf_first_name,nsf_last_name))\n",
    "        for row in df_matches.iterrows():\n",
    "            dict_test_row['ID'] = row[1]['ID']\n",
    "            dict_test_row['campus'] = row[1]['campus']\n",
    "            dict_test_row['name'] = row[1]['name']\n",
    "            dict_test_row['title'] = row[1]['title']\n",
    "            df_linked_data = df_linked_data.append(dict_test_row, ignore_index=True)\n",
    "            \n",
    "    return df_linked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_data = match_records(dict_nsf_awards, df_ucpay, create_rule_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['AwardId', 'CityName', 'FirstName', 'ID', 'LastName', 'Name', 'campus', 'title', 'first', 'last']\n",
    "df_linked_data[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see 4 records in the NSF database had no matches in the UC database. Also if we examine the output of the record linkage more closely we see we have a few false postives, Paul Davies, Joseph Pasquale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several shortcomings to this approach:\n",
    "    \n",
    "1. There is no threshold that can be adjusted for the proper tolerance of false postives and false negatives\n",
    "2. As you apply more and more rules is can become unclear what the combination of rules has on the final linkage\n",
    "3. Rules also often reflect the creator's domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#probabilistic-linkage'></a>\n",
    "## Probabilistic Record Linkage \n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final way is matching records *probabilistically*.  The **Fellegi-Sunter** model compares selected similiar fields in two records and calculates a similarity score, or a weighted probablity of the two records being the same entity.  \n",
    "\n",
    "The algorithm is the following: two fields are first compared using a metric, in this case, the Jaro-Winkler distance (which can be between 0 and 1). The Jaro-Winkler distance is binned into one of three categories: exact match, close match, or no match. Each category has an associated distribution, based on known matches and unmatches. The log probability of being a match and the log probability of being a non-match are calculated for each pair. The final score is the log probablity of being a match minus the log probablity of being a non-match. If the final score is greater than a threshold, then the records are considered to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should probably just make these simple functions instead of objects\n",
    "class FellegiSunter():\n",
    "    \"\"\"\n",
    "    class to implement Fellegi Sunter model\n",
    "    \"\"\"\n",
    "    \n",
    "    m_weights = {'first_name': (0.01,0.14,0.85),\n",
    "                 'last_name': (0.01,0.09,0.90)}\n",
    "    \n",
    "    u_weights = {'first_name': (0.88,0.10,0.02),\n",
    "                 'last_name': (0.91,0.08,0.01)}\n",
    "    \n",
    "    \n",
    "    def fuzzy_match(self,name1,name2):\n",
    "        \"\"\"\n",
    "        Compares two strings using jaro-winker and\n",
    "        outputs and returns one of three match\n",
    "        levels.\n",
    "        \n",
    "        * exact match is a jaro-winkler score >= 0.92\n",
    "        * close match is a jaro-winkler score > 0.85\n",
    "        * no match is a jaro-winkler score < 0.85\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        (name1, name2): str\n",
    "            two text strings to output\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        match_level: int\n",
    "            one of three match levels\n",
    "            2 - exact match \n",
    "            1 - close match\n",
    "            0 - not a match\n",
    "        \"\"\"\n",
    "        score = jellyfish.jaro_winkler(name1,name2)\n",
    "        if score >= 0.92:\n",
    "            return 2\n",
    "        elif score > 0.85:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def match_score(self,record1,record2):\n",
    "        \"\"\"\n",
    "        computes the match score between a pair\n",
    "        of records\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        (record1, record2): tuple(str)\n",
    "            tuples of records to be compared\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        match_score: int\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            tuples need to be the same size \n",
    "        \"\"\"\n",
    "        if not(len(record1) == len(record2)):\n",
    "            raise Exception('records need to be same size')\n",
    "        \n",
    "        scores = [self.fuzzy_match(rec1,rec2) for rec1, rec2 in zip(record1, record2)]\n",
    "        \n",
    "        first_name_score, last_name_score = scores\n",
    "        \n",
    "        #grab the m and u weights\n",
    "        \n",
    "        first_name_m_weight = self.m_weights['first_name'][first_name_score]\n",
    "        first_name_u_weight = self.u_weights['last_name'][first_name_score]\n",
    "        \n",
    "        last_name_m_weight = self.m_weights['first_name'][last_name_score]\n",
    "        last_name_u_weight = self.u_weights['last_name'][last_name_score]\n",
    "        \n",
    "        log_prob_match = math.log(first_name_m_weight) + math.log(last_name_m_weight)\n",
    "        log_prob_umatch = math.log(first_name_u_weight) + math.log(last_name_u_weight)\n",
    "        \n",
    "        match_score = log_prob_match - log_prob_umatch\n",
    "        \n",
    "        return match_score\n",
    "        \n",
    "    def link_record(self, record1, record2, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Returns True if records should be linked,\n",
    "        False otherwise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        (record1, record2): tuple(str)\n",
    "            tuples of records to be compared. must\n",
    "            be the same length\n",
    "        threshold: int\n",
    "            threshold for linking or not\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        link: bool\n",
    "            bool on whether to link two records or not\n",
    "        \"\"\"\n",
    "        \n",
    "        match_score = self.match_score(record1,record2)\n",
    "        if match_score > threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FellegiSunter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( fs.link_record(('Avishek','Kumar'), ('Avishek','Kumar')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( fs.link_record( ('Avishek','Kumar'), ('Anup','Kumar') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take this new function for a spin\n",
    "print('jonathon', 'jonthon', fs.fuzzy_match('jonathon','jonthon') )\n",
    "print('john', 'mark', fs.fuzzy_match('john','mark') )\n",
    "print('fred', 'frederick', fs.fuzzy_match('fred', 'frederick'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jaro_mask(nsf_first_name, nsf_last_name, df_ucpay):\n",
    "    \"\"\"\n",
    "    create a boolean array for whether to link records based on\n",
    "    the jaro-winkler distance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    (nsf_first_name, nsf_last_name): str\n",
    "        first and last name in the NSF dataset\n",
    "        \n",
    "    df_ucpay: DataFrame\n",
    "        DataFrame of the UC directory\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    jaro_mask: ls[bool]\n",
    "        boolean list of records to match\n",
    "    \"\"\"\n",
    "    record = (nsf_first_name, nsf_last_name)\n",
    "    uc_records = df_ucpay[['first','last']].values\n",
    "    \n",
    "    jaro_mask = [fs.link_record(record, uc_record) for uc_record in uc_records]\n",
    "    \n",
    "    return jaro_mask\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked_data = match_records(dict_nsf_awards, df_ucpay, create_jaro_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_col = ['AwardId', 'CityName', 'FirstName', 'ID', 'LastName', 'Name', 'campus', 'title', 'first', 'last']\n",
    "df_linked_data[sel_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the matching using probablistic matching. We can change the thresholds do see how results will vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to Table of Contents](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
