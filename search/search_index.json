{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Hitchhiker's Guide to Data Science for Social Good. # What is the Data Science for Social Good Fellowship? # The Data Science for Social Good Fellowship (DSSG) is a hands-on and project-based summer program that launched in 2013 at the University of Chicago and has now expanded to multiple locations globally. It brings data science fellows from across the world to work on machine learning, artificial intelligence, and data science projects that have a social impact. From a pool of typically over 800 applicants, 20-40 fellows are selected from diverse computational and quantitative disciplines including computer science, statistics, math, engineering, sociology, economics, and public policy backgrounds. The fellows work in small, cross-disciplinary teams on social good projects spanning education, health, energy, transportation, criminal justice, social services, economic development and international development in collaboration with global government agencies and non-profits. This work is done under close and hands-on mentorship from full-time, dedicated data science mentors as well as dedicated project managers, with industry experience. The result is highly trained fellows, improved data science capacity of the social good organization, and a high quality data science project that is ready for field trial and implementation, delivered at the end of the program. In addition to hands-on project-based training, the summer program also consists of workshops, tutorials, and discussion groups based on our data science for social good curriculum designed to train the fellows in doing practical data science and artificial intelligence for social impact. Who is this guide for? # The primary audience for this guide is the set of fellows coming to DSSG but we want everything we create to be open and accessible to larger world. We hope this is useful to people beyond the summer fellows coming to DSSG. If you are applying to the program or have been accepted as a fellow, check out the manual to see how you can prepare before arriving, what orientation and training will cover, and what to expect from the summer. If you are interested in learning at home, check out the tutorials and teach-outs developed by our staff and fellows throughout the summer, and to suggest or contribute additional resources. *Another one of our goals is to encourage collaborations. Anyone interested in doing this type of work, or starting a DSSG program, to build on what we've learned by using and contributing to these resources. What is in this guide? # Our number one priority at DSSG is to train fellows to do data science for social good work . This curriculum includes many things you'd find in a data science course or bootcamp, but with an emphasis on solving problems with social impact, integrating data science with the social sciences, discussing ethical implications of the work, as well as privacy, and confidentiality issues. We have spent many (sort of) early mornings waxing existential over Dunkin' Donuts while trying to define what makes a \"data scientist for social good,\" that enigmatic breed combining one part data scientist, one part consultant, one part educator, and one part bleeding heart idealist. We've come to a rough working definition in the form of the skills and knowledge one would need, which we categorize as follows: Programming, because you'll need to tell your computer what to do, usually by writing code. Computer science, because you'll need to understand how your data is - and should be - structured, as well as the algorithms you use to analyze it. Math and stats, because everything else in life is just applied math , and numerical results are meaningless without some measure of uncertainty. Machine learning, because you'll want to build predictive or descriptive models that can learn, evolve, and improve over time. Social science, because you'll need to know how to design experiments to validate your models in the field, and to understand when correlation can plausibly suggest causation, and sometimes even do causal inference. Problem and Project Scoping, because you'll need to be able to go from a vague and fuzzy project description to a problem you can solve, understand the goals of the project, the interventions you are informing, the data you have and need, and the analysis that needs to be done. Project management, to make progress as a team, to work effectively with your project partner, and work with a team to make that useful solution actually happen. Privacy and security, because data is people and needs to be kept secure and confidential. Ethics, fairness, bias, and transparency, because your work has the potential to be misused or have a negative impact on people's lives, so you have to consider the biases in your data and analyses, the ethical and fairness implications, and how to make your work interpretable and transparent to the users and to the people impacted by it. Communications, because you'll need to be able to tell the story of why what you're doing matters and the methods you're using to a broad audience. Social issues, because you're doing this work to help people, and you don't live or work in a vacuum, so you need to understand the context and history surrounding the people, places and issues you want to impact. All material is licensed under CC-BY 4.0","title":"Home"},{"location":"#welcome-to-the-hitchhikers-guide-to-data-science-for-social-good","text":"","title":"Welcome to the Hitchhiker's Guide to Data Science for Social Good."},{"location":"#what-is-the-data-science-for-social-good-fellowship","text":"The Data Science for Social Good Fellowship (DSSG) is a hands-on and project-based summer program that launched in 2013 at the University of Chicago and has now expanded to multiple locations globally. It brings data science fellows from across the world to work on machine learning, artificial intelligence, and data science projects that have a social impact. From a pool of typically over 800 applicants, 20-40 fellows are selected from diverse computational and quantitative disciplines including computer science, statistics, math, engineering, sociology, economics, and public policy backgrounds. The fellows work in small, cross-disciplinary teams on social good projects spanning education, health, energy, transportation, criminal justice, social services, economic development and international development in collaboration with global government agencies and non-profits. This work is done under close and hands-on mentorship from full-time, dedicated data science mentors as well as dedicated project managers, with industry experience. The result is highly trained fellows, improved data science capacity of the social good organization, and a high quality data science project that is ready for field trial and implementation, delivered at the end of the program. In addition to hands-on project-based training, the summer program also consists of workshops, tutorials, and discussion groups based on our data science for social good curriculum designed to train the fellows in doing practical data science and artificial intelligence for social impact.","title":"What is the Data Science for Social Good Fellowship?"},{"location":"#who-is-this-guide-for","text":"The primary audience for this guide is the set of fellows coming to DSSG but we want everything we create to be open and accessible to larger world. We hope this is useful to people beyond the summer fellows coming to DSSG. If you are applying to the program or have been accepted as a fellow, check out the manual to see how you can prepare before arriving, what orientation and training will cover, and what to expect from the summer. If you are interested in learning at home, check out the tutorials and teach-outs developed by our staff and fellows throughout the summer, and to suggest or contribute additional resources. *Another one of our goals is to encourage collaborations. Anyone interested in doing this type of work, or starting a DSSG program, to build on what we've learned by using and contributing to these resources.","title":"Who is this guide for?"},{"location":"#what-is-in-this-guide","text":"Our number one priority at DSSG is to train fellows to do data science for social good work . This curriculum includes many things you'd find in a data science course or bootcamp, but with an emphasis on solving problems with social impact, integrating data science with the social sciences, discussing ethical implications of the work, as well as privacy, and confidentiality issues. We have spent many (sort of) early mornings waxing existential over Dunkin' Donuts while trying to define what makes a \"data scientist for social good,\" that enigmatic breed combining one part data scientist, one part consultant, one part educator, and one part bleeding heart idealist. We've come to a rough working definition in the form of the skills and knowledge one would need, which we categorize as follows: Programming, because you'll need to tell your computer what to do, usually by writing code. Computer science, because you'll need to understand how your data is - and should be - structured, as well as the algorithms you use to analyze it. Math and stats, because everything else in life is just applied math , and numerical results are meaningless without some measure of uncertainty. Machine learning, because you'll want to build predictive or descriptive models that can learn, evolve, and improve over time. Social science, because you'll need to know how to design experiments to validate your models in the field, and to understand when correlation can plausibly suggest causation, and sometimes even do causal inference. Problem and Project Scoping, because you'll need to be able to go from a vague and fuzzy project description to a problem you can solve, understand the goals of the project, the interventions you are informing, the data you have and need, and the analysis that needs to be done. Project management, to make progress as a team, to work effectively with your project partner, and work with a team to make that useful solution actually happen. Privacy and security, because data is people and needs to be kept secure and confidential. Ethics, fairness, bias, and transparency, because your work has the potential to be misused or have a negative impact on people's lives, so you have to consider the biases in your data and analyses, the ethical and fairness implications, and how to make your work interpretable and transparent to the users and to the people impacted by it. Communications, because you'll need to be able to tell the story of why what you're doing matters and the methods you're using to a broad audience. Social issues, because you're doing this work to help people, and you don't live or work in a vacuum, so you need to understand the context and history surrounding the people, places and issues you want to impact. All material is licensed under CC-BY 4.0","title":"What is in this guide?"},{"location":"host-cities/","text":"Host cities # Year Host City Country 2013 Chicago USA 2014 Chicago USA 2015 Chicago USA 2016 Chicago USA 2017 Lisbon Portugal 2018 Lisbon Portugal 2018 Chicago USA 2019 Warwick UK 2019 London UK 2022 Pittsburgh USA","title":"DSSG Locations"},{"location":"host-cities/#host-cities","text":"Year Host City Country 2013 Chicago USA 2014 Chicago USA 2015 Chicago USA 2016 Chicago USA 2017 Lisbon Portugal 2018 Lisbon Portugal 2018 Chicago USA 2019 Warwick UK 2019 London UK 2022 Pittsburgh USA","title":"Host cities"},{"location":"what-is-dssg/","text":"We have spent many (sort of) early mornings waxing existential over Dunkin' Donuts while trying to define what makes a \"data scientist for social good,\" that enigmatic breed combining one part data scientist, one part consultant, one part educator, and one part bleeding heart idealist. We've come to a rough working definition in the form of the skills and knowledge one would need, which we categorize as follows: Programming, because you'll need to tell your computer what to do, usually by writing code. Computer science, because you'll need to understand how your data is - and should be - structured, as well as the algorithms you use to analyze it. Math and stats, because everything else in life is just applied math , and numerical results are meaningless without some measure of uncertainty. Machine learning, because you'll want to build predictive or descriptive models that can learn, evolve, and improve over time. Social science, because you'll need to know how to design experiments to validate your models in the field, and to understand when correlation can plausibly suggest causation, and sometimes even do causal inference. Problem and Project Scoping, because you'll need to be able to go from a vague and fuzzy project description to a problem you can solve, understand the goals of the project, the interventions you are informing, the data you have and need, and the analysis that needs to be done. Project management, to make progress as a team, to work effectively with your project partner, and work with a team to make that useful solution actually happen. Privacy and security, because data is people and needs to be kept secure and confidential. Ethics, fairness, bias, and transparency, because your work has the potential to be misused or have a negative impact on people's lives, so you have to consider the biases in your data and analyses, the ethical and fairness implications, and how to make your work interpretable and transparent to the users and to the people impacted by it. Communications, because you'll need to be able to tell the story of why what you're doing matters and the methods you're using to a broad audience. Social issues, because you're doing this work to help people, and you don't live or work in a vacuum, so you need to understand the context and history surrounding the people, places and issues you want to impact.","title":"What is in this curriculum?"},{"location":"curriculum/","text":"Before You Start # So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you decide on a project, or refine a project you already have in mind. Next, check out Pipelines and Project Workflow for an overview of how the steps of your project (and, therefore, your code) will be organized. Getting and Keeping Data # Data comes in many forms, from many sources - you may get a database dump or CSV files directly from a project partner , or you may need to scrape data from the web . Either way, once you've got your hands on some data, you'll need to bring it into a database , and start cleaning and \"wrangling\" it. You'll definitely want to keep track of the steps to take your data from its original, raw form to being model-ready, so check out Reproducible ETL . Command line tools will start to come in handy here. Finally, data science for social good projects often involve sensitive data about real humans, which is what makes this work both interesting and important, but also makes it extra important to keep security in mind, so make sure to check out the Data Security Primer . Data Exploration and Analysis # Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data . Modeling and Machine Learning # Now you're ready to make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through. Programming Best Practices # As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in. Presentations and Communications # Remember that there's no point to doing data science for social good work if no one understands what you did and can put it to good use. You'll need to be able to communicate your work: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable. How to Contribute # We welcome contributions and collaboration! If you find a broken link, or have a request or suggestion, please submit an issue. If you have materials to contribute, please submit a pull request. New tutorials should follow our tutorial template , and keep in mind the teaching philosophy outlined below. Teaching Philosophy # Our guiding teaching philosophy is as follows: - You get out what you put in. Fellows are encouraged to take an active role in teaching and shaping the curriculum, as well as learning from it. Learning also takes initiative and participation on the student side. - Clearly motivate topics and tools. For more technical topics: what actual task that a data scientist does will require this tool? What other options are there to do similar tasks? What are pitfalls, and what will it look like when something goes wrong? What are common mistakes or bugs? For conceptual topics: Why do we feel the need to communicate this topic? What are some concrete examples of where it's been done well or poorly in the past? - Lessons should be user friendly. Lectures should be concise - 45 minutes at the outside - and materials should be practical. Slides should be accompanied by a worksheet or exercises so fellows can follow along and learn by doing, and a cheat sheet with relevant commands or code snippets should be included where possible.","title":"What is in this curriculum"},{"location":"curriculum/#before-you-start","text":"So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you decide on a project, or refine a project you already have in mind. Next, check out Pipelines and Project Workflow for an overview of how the steps of your project (and, therefore, your code) will be organized.","title":"Before You Start"},{"location":"curriculum/#getting-and-keeping-data","text":"Data comes in many forms, from many sources - you may get a database dump or CSV files directly from a project partner , or you may need to scrape data from the web . Either way, once you've got your hands on some data, you'll need to bring it into a database , and start cleaning and \"wrangling\" it. You'll definitely want to keep track of the steps to take your data from its original, raw form to being model-ready, so check out Reproducible ETL . Command line tools will start to come in handy here. Finally, data science for social good projects often involve sensitive data about real humans, which is what makes this work both interesting and important, but also makes it extra important to keep security in mind, so make sure to check out the Data Security Primer .","title":"Getting and Keeping Data"},{"location":"curriculum/#data-exploration-and-analysis","text":"Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data .","title":"Data Exploration and Analysis"},{"location":"curriculum/#modeling-and-machine-learning","text":"Now you're ready to make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through.","title":"Modeling and Machine Learning"},{"location":"curriculum/#programming-best-practices","text":"As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in.","title":"Programming Best Practices"},{"location":"curriculum/#presentations-and-communications","text":"Remember that there's no point to doing data science for social good work if no one understands what you did and can put it to good use. You'll need to be able to communicate your work: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable.","title":"Presentations and Communications"},{"location":"curriculum/#how-to-contribute","text":"We welcome contributions and collaboration! If you find a broken link, or have a request or suggestion, please submit an issue. If you have materials to contribute, please submit a pull request. New tutorials should follow our tutorial template , and keep in mind the teaching philosophy outlined below.","title":"How to Contribute"},{"location":"curriculum/#teaching-philosophy","text":"Our guiding teaching philosophy is as follows: - You get out what you put in. Fellows are encouraged to take an active role in teaching and shaping the curriculum, as well as learning from it. Learning also takes initiative and participation on the student side. - Clearly motivate topics and tools. For more technical topics: what actual task that a data scientist does will require this tool? What other options are there to do similar tasks? What are pitfalls, and what will it look like when something goes wrong? What are common mistakes or bugs? For conceptual topics: Why do we feel the need to communicate this topic? What are some concrete examples of where it's been done well or poorly in the past? - Lessons should be user friendly. Lectures should be concise - 45 minutes at the outside - and materials should be practical. Slides should be accompanied by a worksheet or exercises so fellows can follow along and learn by doing, and a cheat sheet with relevant commands or code snippets should be included where possible.","title":"Teaching Philosophy"},{"location":"curriculum/sample/","text":"monday tuesday wednesday thursday friday every week Fellowship-wide Check-in (15-30 minutes in the morning) short updates from every team deep dives - 2 teams every week Code Review External Talk Ethics Discussions week 1 Software installation and check logins Prpject Scoping How does a project go over the summer technical environment setup , cmd line , workflow git Interacting with project partners Python for Data Analysis ML Pipeline Communications for the summer Databases and SQL week 2 Data Maturity Assessment DSSG Project Deliverables Ethics Overview more dbs and ETL Working in a team Good software practices no deep dive - partner session Data Exploration (two sessions) - viz, pandas, sql, spatial week 3 Policy Problem templates Machine Learning Overview - formulation and validation Intro to Social Sciences sql Record linkage ML overview - validation case study from previous dssg week 4 Machine Learning overview - methods TBD Non-technical session Causal Inference with Observational Data Feature Engineering ML overview - methods User interfaces and usability week 5 Text Analysis ML Pipelines - Deeper Dive Feature Engineering workshop Optimization week 6 Model Validation Model Interpretation Communications - speaking Audition and Overview of postmodeling Bias and Fairness week 7 Post-modeling Analysis Causal Inference - experiments Case Study Post-Modeling Analysis week 8 Recap of what needs to be done for the rest of the summer Experimental design TBD technical session - TBD week 9 Recap of what needs to be done Social Science methods TBD technical session - TBD week 10 Bias and Fairness in ML Communications - writing Image/Video analysis Network Analysis week 11 week 12","title":"Sample curriculum for Summer 2019"},{"location":"curriculum/0_before_you_start/","text":"Before You Start # So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and Software Setup and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you scope or refine your project. Then, check out Pipelines and Project Workflow Don\u2019t forget to keep best practices for your day to day workflow in mind.","title":"Before You Start"},{"location":"curriculum/0_before_you_start/#before-you-start","text":"So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and Software Setup and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you scope or refine your project. Then, check out Pipelines and Project Workflow Don\u2019t forget to keep best practices for your day to day workflow in mind.","title":"Before You Start"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/","text":"Technical Workflow and Best Practices # This tutorial is designed to help you understand how to get started with setting up your computing environment, how to decide what to use your local laptop/desktop for, what to do on the server (and how), and how to go back and forth between different environments and tools on your laptop, the server, and your remote database (an other data resources). We assume a GNU/linux (Ubuntu) server that's been set up for you, and access to a database (PostgreSQL). Looking at this before the summer? Many of the specific instructions here rely on the server and database we'll have set up for you to use during the summer, so you may not be able to follow along yet, but please do read through the workflow here so you'll have an idea what to expect. 1. What should you have on your laptop? # You'll need a few tools (such as SSH, a good text editor, a database utility, etc) installed on your local machine (whether it's a MacOS, windows, or GNU/Linux). If you haven't already done so, be sure to follow the setup instructions here to get these installed on your laptop. 2. What should you set up on the server? # Decide which shell you're using. You have bash by default, but some people may prefer zsh (if you're new to working at the linux command line, stikcing with bash is a reasonable thing to do). Optionally, set up dotfiles (these are configuration files that start with a . and allow you to specify shortcuts and change the behavior of your environment and various tools). you can clone this repo with Adolfo's dotfiles as a starting point to work from. Danger You should never blindly copy lines to your dotfiles that you don't understand. Check the files in dotfiles repository and adapt/adopt what suits your needs and tastes Configure git Decide on your editor (vim or GNU/Emacs). Note that this is the editor you can use to edit files directly on the server. Some local text editors, such as VSCode, will allow you to edit files remotely on the server but through a GUI interface on your laptop. For vim users Get a good .vimrc file to make life easier for yourself if you choose vim. See for example this If you prefer GNU/Emacs There are several options and depends in your taste, but Emacs prelude is a good start Create configuration files with your database credentials: .pg_service.conf and .pgpass files, which should live in your home directory and have 600 permissions (e.g., chmod 600 .pgpass && chmod 600 .pg_service.conf ) so only you can read/write it. Learn about pyenv and virtual environments and set one up (if it hasn't been set up for you). Learn how to install new python packages through pip install 3. Workflow: How should you work day to day with your laptop and the remote server? # screen / tmux : When you log in to your remote machine, run screen (note: it will already be installed, so you can ignore those details; also here's a quick video intro ) or tmux and work from a screen/tmux session (Optional) When using the database for any reason from your laptop (to connect with tableau or dbeaver or for any other application), open an ssh tunnel from your local machine to the remote server : ssh -N -L localhost:8888:localhost:8888 username@[projectname].dssg.io Note that many GUI tools like dbeaver or dbvisualizer have a built-in interface for establishing an SSH tunnel that you can use as well. Writing and Running Code Because your data needs to stay in the secure environment we've set up for it, you'll only be able to run your code on the server. As such, you have three options for how/where you want to write code: [Reccomended (especially if you're new to remote workflows)] Using an GUI editor on your laptop (such as VSCode) that allows you to remotely edit files stored on the server over SSH. Using another editor on your laptop (sublime, atom, etc) to edit code stored locally, then use git to commit and push to the repo and then do a git pull on the server to get your code there. Editing code on the server directly, using a text-based editor such as vim or GNU/Emacs in a terminal window. git commit often. Every time you finish a chunk of work, do a git commit . git push when you've tested it and it is doing what you intended for it to do. Do not push code to master if it breaks. You will annoy your teammates :) Later in the summer, we'll talk more about how to create git branches. Every time you resume working , do a git pull to get be sure you're starting from the latest version of the code. If you need to copy files from your laptop to server, use scp . Danger Other way around, i.e. from the server to your laptop , DON'T! All the data needs to stay on the remote server. If you're writing (or running) your code in jupyter notebooks, then you should: create a no-browser jupyter session on the server jupyter lab --no-browser --port=8889 You may need to chage the port number to avoid conflicts with other teammates using the same port. On your local machine, create an SSH tunnel that forwards the port for Jupyter Lab ( 8889 in the above command) on the remote machine to a port on the local machine (also 8888 above) so that we can access it using our local browser. ssh -N -L localhost:8888:localhost:8889 username@projectname.dssg.io Access the remote jupyter server via your local browser. Open your browser and go to http://0.0.0.0:8888 you may need to copy and paste the longer URL with a token that is generated when you run the command in step 1) that looks like http://localhost:8889/?token=343vdfvdfggdfgfdt345&token=fdsfdf345353vc See More detailed instructions 4. Other Workflow Considerations # When should you use Jupyter lab, versus when you should use .py files to write code When to use psql versus DBeaver When to use SQL versus when to use Python and/or Pandas 5. Other Tips # Tunneling to the DB for Tableau (or another app like QGIS): ssh -L 5433:databaseservername:5432 username@projectservername BELOW COPIED FROM OLD setup_session_guide.md # Reaching the Database Server # The database server runs PostgreSQL. MacOS Make sure you have the psql client installed; on Mac, this would be $ brew tap-pin dbcli/tap $ brew install pgcli Note, we are installing pgcli instead of psql , but apparently there is no way of install just the client without installing the whole database server. If you still want to give it a shot: $ brew postgres GNU/Linux On Debian based distros: sudo apt install postgresql-client libpq-dev Once you have the postgres client installed, you can access the training database with it. However, the database server only allows access from the training server. Thus, you need to set up an SSH tunnel through the training server to the Postgres server: $ ssh -NL localhost:8888:POSTGRESURL:5432 ec2username@EC2URL where you need to substitute POSTGRESURL , ec2username , and EC2URL with the postgres server's URL, your username on the training server, and the training server's URL respectively. Also, you should substitute 8888 with a random number in the 8000-65000 range of your choice (port 8888 might be in use already). This command forwards your laptop's port 8888 through your account on the EC2 (EC2URL) to the Postgres server port 5432. So if you access your local port 8888 in the next step, you get forwarded to the Postgres server's port 5432 - but from the Postgres server's view, the traffic is now coming from the training server (instead of your laptop), and the training server is the only IP address that is allowed to access the postgres server. Figure. A graphical representation of a ssh tunnel. Not quite our situation -they are using a MySQL db who knows why-, but it is close enough. Courtesy from this Medium post . Connect to the Postgres database on the forwarded port $ psql -h localhost -p 8888 -U USERNAME -d DBNAME where you need to replace USERNAME with the postgres [!] username, DBNAME with the name of your database, and the 8888 with the number you chose in the previous step. You then get prompted for a password. This is now the postgres server asking, so you need to reply with the corresponding password! This should drop you into a SQL shell on the database server. Note In some configurations, you'll need to explicitly assume a role to do anything beyond connecting to the database. To make changes to the training database, use the training_write role. Let's test it by creating and dropping a schema: set role training_write; create schema jsmith; drop schema jsmith; PRO tip You could save a lot of keystrokes if you setup a .pgservice.conf file and a .pgpass file in your $HOME folder. Then you could simply type $ psql service=mydb # mydb is the name of the dbservice I really prefer a GUI if you want a graphical interface to databases - you might want to use DBeaver .","title":"Technical workflow"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#technical-workflow-and-best-practices","text":"This tutorial is designed to help you understand how to get started with setting up your computing environment, how to decide what to use your local laptop/desktop for, what to do on the server (and how), and how to go back and forth between different environments and tools on your laptop, the server, and your remote database (an other data resources). We assume a GNU/linux (Ubuntu) server that's been set up for you, and access to a database (PostgreSQL). Looking at this before the summer? Many of the specific instructions here rely on the server and database we'll have set up for you to use during the summer, so you may not be able to follow along yet, but please do read through the workflow here so you'll have an idea what to expect.","title":"Technical Workflow and Best Practices"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#1-what-should-you-have-on-your-laptop","text":"You'll need a few tools (such as SSH, a good text editor, a database utility, etc) installed on your local machine (whether it's a MacOS, windows, or GNU/Linux). If you haven't already done so, be sure to follow the setup instructions here to get these installed on your laptop.","title":"1. What should you have on your laptop?"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#2-what-should-you-set-up-on-the-server","text":"Decide which shell you're using. You have bash by default, but some people may prefer zsh (if you're new to working at the linux command line, stikcing with bash is a reasonable thing to do). Optionally, set up dotfiles (these are configuration files that start with a . and allow you to specify shortcuts and change the behavior of your environment and various tools). you can clone this repo with Adolfo's dotfiles as a starting point to work from. Danger You should never blindly copy lines to your dotfiles that you don't understand. Check the files in dotfiles repository and adapt/adopt what suits your needs and tastes Configure git Decide on your editor (vim or GNU/Emacs). Note that this is the editor you can use to edit files directly on the server. Some local text editors, such as VSCode, will allow you to edit files remotely on the server but through a GUI interface on your laptop. For vim users Get a good .vimrc file to make life easier for yourself if you choose vim. See for example this If you prefer GNU/Emacs There are several options and depends in your taste, but Emacs prelude is a good start Create configuration files with your database credentials: .pg_service.conf and .pgpass files, which should live in your home directory and have 600 permissions (e.g., chmod 600 .pgpass && chmod 600 .pg_service.conf ) so only you can read/write it. Learn about pyenv and virtual environments and set one up (if it hasn't been set up for you). Learn how to install new python packages through pip install","title":"2. What should you set up on the server?"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#3-workflow-how-should-you-work-day-to-day-with-your-laptop-and-the-remote-server","text":"screen / tmux : When you log in to your remote machine, run screen (note: it will already be installed, so you can ignore those details; also here's a quick video intro ) or tmux and work from a screen/tmux session (Optional) When using the database for any reason from your laptop (to connect with tableau or dbeaver or for any other application), open an ssh tunnel from your local machine to the remote server : ssh -N -L localhost:8888:localhost:8888 username@[projectname].dssg.io Note that many GUI tools like dbeaver or dbvisualizer have a built-in interface for establishing an SSH tunnel that you can use as well. Writing and Running Code Because your data needs to stay in the secure environment we've set up for it, you'll only be able to run your code on the server. As such, you have three options for how/where you want to write code: [Reccomended (especially if you're new to remote workflows)] Using an GUI editor on your laptop (such as VSCode) that allows you to remotely edit files stored on the server over SSH. Using another editor on your laptop (sublime, atom, etc) to edit code stored locally, then use git to commit and push to the repo and then do a git pull on the server to get your code there. Editing code on the server directly, using a text-based editor such as vim or GNU/Emacs in a terminal window. git commit often. Every time you finish a chunk of work, do a git commit . git push when you've tested it and it is doing what you intended for it to do. Do not push code to master if it breaks. You will annoy your teammates :) Later in the summer, we'll talk more about how to create git branches. Every time you resume working , do a git pull to get be sure you're starting from the latest version of the code. If you need to copy files from your laptop to server, use scp . Danger Other way around, i.e. from the server to your laptop , DON'T! All the data needs to stay on the remote server. If you're writing (or running) your code in jupyter notebooks, then you should: create a no-browser jupyter session on the server jupyter lab --no-browser --port=8889 You may need to chage the port number to avoid conflicts with other teammates using the same port. On your local machine, create an SSH tunnel that forwards the port for Jupyter Lab ( 8889 in the above command) on the remote machine to a port on the local machine (also 8888 above) so that we can access it using our local browser. ssh -N -L localhost:8888:localhost:8889 username@projectname.dssg.io Access the remote jupyter server via your local browser. Open your browser and go to http://0.0.0.0:8888 you may need to copy and paste the longer URL with a token that is generated when you run the command in step 1) that looks like http://localhost:8889/?token=343vdfvdfggdfgfdt345&token=fdsfdf345353vc See More detailed instructions","title":"3. Workflow: How should you work day to day with your laptop and the remote server?"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#4-other-workflow-considerations","text":"When should you use Jupyter lab, versus when you should use .py files to write code When to use psql versus DBeaver When to use SQL versus when to use Python and/or Pandas","title":"4. Other Workflow Considerations"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#5-other-tips","text":"Tunneling to the DB for Tableau (or another app like QGIS): ssh -L 5433:databaseservername:5432 username@projectservername","title":"5. Other Tips"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#below-copied-from-old-setup_session_guidemd","text":"","title":"BELOW COPIED FROM OLD setup_session_guide.md"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#reaching-the-database-server","text":"The database server runs PostgreSQL. MacOS Make sure you have the psql client installed; on Mac, this would be $ brew tap-pin dbcli/tap $ brew install pgcli Note, we are installing pgcli instead of psql , but apparently there is no way of install just the client without installing the whole database server. If you still want to give it a shot: $ brew postgres GNU/Linux On Debian based distros: sudo apt install postgresql-client libpq-dev Once you have the postgres client installed, you can access the training database with it. However, the database server only allows access from the training server. Thus, you need to set up an SSH tunnel through the training server to the Postgres server: $ ssh -NL localhost:8888:POSTGRESURL:5432 ec2username@EC2URL where you need to substitute POSTGRESURL , ec2username , and EC2URL with the postgres server's URL, your username on the training server, and the training server's URL respectively. Also, you should substitute 8888 with a random number in the 8000-65000 range of your choice (port 8888 might be in use already). This command forwards your laptop's port 8888 through your account on the EC2 (EC2URL) to the Postgres server port 5432. So if you access your local port 8888 in the next step, you get forwarded to the Postgres server's port 5432 - but from the Postgres server's view, the traffic is now coming from the training server (instead of your laptop), and the training server is the only IP address that is allowed to access the postgres server. Figure. A graphical representation of a ssh tunnel. Not quite our situation -they are using a MySQL db who knows why-, but it is close enough. Courtesy from this Medium post . Connect to the Postgres database on the forwarded port $ psql -h localhost -p 8888 -U USERNAME -d DBNAME where you need to replace USERNAME with the postgres [!] username, DBNAME with the name of your database, and the 8888 with the number you chose in the previous step. You then get prompted for a password. This is now the postgres server asking, so you need to reply with the corresponding password! This should drop you into a SQL shell on the database server. Note In some configurations, you'll need to explicitly assume a role to do anything beyond connecting to the database. To make changes to the training database, use the training_write role. Let's test it by creating and dropping a schema: set role training_write; create schema jsmith; drop schema jsmith; PRO tip You could save a lot of keystrokes if you setup a .pgservice.conf file and a .pgpass file in your $HOME folder. Then you could simply type $ psql service=mydb # mydb is the name of the dbservice I really prefer a GUI if you want a graphical interface to databases - you might want to use DBeaver .","title":"Reaching the Database Server"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/","text":"Pipelines and Project Workflow # Motivation # Most data-science projects have the same set of tasks: ETL : extracting data from its source, transforming it, then loading it into a database. Remember, ETL stands for Extract , Transform and Load . Pre-process data : This might include imputing missing values and choosing the training and testing sets. Create features : Recombine and enrich the data to create features aiding the modelling work. Train the model(s) : You can try different algorithms, features, and so on. Assess performance on the test set: Using an appropriate metric (e.g. Precision@k Precision@k , Recall Recall , AUC AUC ), examine the performance of your model \"out of sample.\" Think of new things to try. Repeat steps 1 through 4 as appropriate. If the code base is not structured and named well, you might be struggling to remember the details of each step once you have built a few models. What features did you use for each? What training and testing split? What hyperparameters? Your code might be getting messy too. Did you overwrite the code for the previous model? Maybe you copied, pasted, and edited code from an earlier model. Can you still read what's there? It can quickly become a hodgepodge that requires heroics to decipher. In this session, we will introduce a workflow that can avoid (or at least reduce) these problems. Data pipelines # It is helpful to structure the data into multiple layers. In data bases, a layer is expressed as schema. In most other formats, they are expressed through a directory structure. Raw The data we receive from the partners and external sources is the raw data. Raw data is immutable. Quoting from the popular workflow package Data Science Cookiecutter : Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG), but anyone should be able to reproduce the final products with only the code in src and the data in data/raw. Intermediate If the raw data is messy, it is advisable to create an intermediate layer that consists of tidy copies of the raw data. Typical situations where this is useful are: - Data is received in multiple different file types - Data fields are not typed (e.g. csv files, excel) or poorly typed (dates as strings, inconsistent date formats) - Column names are unclear, have spaces, special characters, or there are no column names The transformations from raw to intermediate should be limited to fix the issues mentioned above. We should not combine different data sets or create calculated fields. This is reserved for the next layer. Typical storage formats for the intermediate layer are a data base (e.g. postgres ) or parquet files. Processed To perform the modelling work, the input data needs to be combined and enriched, for example by creating features. The data sets that are created in this process are stored in the processed layer. Sometimes it can be useful to split this layer out into a domain data model, a feature layer and a master layer but the exact layering will depend on the project context. Models The processed data is used to train predictive models, explanatory models, recommender engines and optimisation algorithms. The trained models are stored in the model layer. In contrast to the previous layers, models are usually stored in pickle because they are not in tabular format. Model output Model performance metrics, model selection information and predictions are kept in the model output layer. Reporting Reporting can be performed across the pipeline. For example, there might be data quality reports on the inputs, distribution analysis on the processed data, predictions, explanations, recommendations that are provided to the user, and performance evaluation and tracking. If a front-end is constructed, it will access the reporting layer to display information to the users and developers. For example, a Tableau dashboard, power BI, a jupyter notebook or an excel output will read from the reporting layer. Accordingly, the format of the data in the reporting layer will be adjusted to the front end of choice. Code setup # The code repository will mirror the data pipeline by creating the corresponding folder structure for the python files. In addition, there are multiple other files that need to be stored and managed. The community has arrived at a standard setup of the project directories that we will also follow. Directory structure: \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 conf \u2502 \u251c\u2500\u2500 base <- Space for shared configurations like parameters \u2502 \u2514\u2500\u2500 local <- Space for local configurations, usually credentials \u2502 \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 01_raw <- Imutable input data \u2502 \u251c\u2500\u2500 02_intermediate<- Cleaned version of raw \u2502 \u251c\u2500\u2500 03_processed <- The data used for modelling \u2502 \u251c\u2500\u2500 04_models <- trained models \u2502 \u251c\u2500\u2500 05_model_output<- model output \u2502 \u2514\u2500\u2500 06_reporting <- Reports and input to frontend \u2502 \u251c\u2500\u2500 docs <- Space for Sphinx documentation \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is date YYYYMMDD (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `20190601-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 results <- Intermediate analysis as HTML, PDF, LaTeX, etc. \u2502 \u251c\u2500\u2500 requirements.txt <- The requirements file for reproducing the analysis environment, e.g. \u2502 generated with `pip freeze > requirements.txt` \u2502 \u251c\u2500\u2500 .gitignore <- Avoids uploading data, credentials, outputs, system files etc \u2502 \u2514\u2500\u2500 src <- Source code for use in this project. \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u251c\u2500\u2500 d00_utils <- Functions used across the project \u2502 \u2514\u2500\u2500 remove_accents.py \u2502 \u251c\u2500\u2500 d01_data <- Scripts to reading and writing data etc \u2502 \u2514\u2500\u2500 load_data.py \u2502 \u251c\u2500\u2500 d02_intermediate<- Scripts to transform data from raw to intermediate \u2502 \u2514\u2500\u2500 create_int_payment_data.py \u2502 \u251c\u2500\u2500 d03_processing <- Scripts to turn intermediate data into modelling input \u2502 \u2514\u2500\u2500 create_master_table.py \u2502 \u251c\u2500\u2500 d04_modelling <- Scripts to train models and then use trained models to make \u2502 \u2502 predictions \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u251c\u2500\u2500 d05_model_evaluation<- Scripts that analyse model performance and model selection \u2502 \u2514\u2500\u2500 calculate_performance_metrics.py \u2502 \u251c\u2500\u2500 d06_reporting <- Scripts to produce reporting tables \u2502 \u2514\u2500\u2500 create_rpt_payment_summary.py \u2502 \u2514\u2500\u2500 d06_visualisation<- Scripts to create frequently used plots \u2514\u2500\u2500 visualise_patient_journey.py Workflow # The typical workflow to develop code is the following: Prototype code in a jupyter notebook Move code into a function that takes data and parameters as inputs and returns the processed data or trained model as output. Test the function in the jupyter notebook Move the function into the src folder Import the function in the jupyter notebook Test the function is working Functions can be imported into a notebook as follows. First we tell the notebook where the functions are import os import sys src_dir = os.path.join(os.getcwd(), '..', 'src') sys.path.append(src_dir) Then we state which functions to import from d00_utils.my_fun import my_fun Try it! Code pipeline # The code that produces the different layers of the data pipeline should be abstracted into functions. A code pipeline is a set of code that handles all the computational tasks your project needs from beginning to end. The simplest pipeline is a set of functions strung together. For example, int_data = create_int_data(raw_data) pro_drug_features = create_pro_drug_features(int_data) pro_patient_features = create_pro_patient_features(int_data) pro_master_table = create_pro_master_table(pro_drug_features, pro_patient_features) model = train_model(pro_master_table) rpt_report = produce_report(model) This is a very schematic example. Typically, each step is broken down into a number of subsets creating pipelines for each layer of the data pipeline. The end-to-end pipeline is then the concatenation of the sub-pipelines. Examples # Example 1 Here's a simple example of a pipeline using scikit-learn 's boston dataset: This pipeline has two steps. The first, called \"preprocessing,\" prepares the data for modeling by creating training and testing splits. The second, which called \"models, predictions, and metrics,\" uses the preprocessed data to train models, make predictions, and print R^2 R^2 on the test set. The pipeline takes inputs (e.g. data, training/testing proportions, and model types) at one end and produces outputs ( accuracy ) at the other end. Obviously, this analysis is incomplete, but the pipeline is a good start. Because we use the same code and data, we can run the pipeline from beginning to end and get the same results. And because we split the pipeline into functions, we can identify where the pipeline goes wrong and improve the pipeline one function at a time. (Each function just needs to use the same inputs and outputs as before.) Also note the function and loops in the second part of the pipeline. We're somewhat agnostic about the methods we use. If it works, great! This structure lets us loop through many types of models using the same preprocessed data and the same predictions and metrics. It makes adding new methods and comparing the results easier, and it helps us focus on other parts of the pipeline, such as feature generation. Example 2 The police pipeline , started at DSSG 2015 , is an example of a relatively well developed pipeline. It lets us specify the pipeline options we want in a yaml file, from preprocessing on. (The code in this repository does not include ETL.) It gives us many modeling options, and it makes comparisons easy. Resources # Our lead pipeline , started at DSSG 2014 Our Cincinnati pipeline , started at DSSG 2015 Triage (a generalized DSSG pipeline)","title":"Pipelines and Project Workflow"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#pipelines-and-project-workflow","text":"","title":"Pipelines and Project Workflow"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#motivation","text":"Most data-science projects have the same set of tasks: ETL : extracting data from its source, transforming it, then loading it into a database. Remember, ETL stands for Extract , Transform and Load . Pre-process data : This might include imputing missing values and choosing the training and testing sets. Create features : Recombine and enrich the data to create features aiding the modelling work. Train the model(s) : You can try different algorithms, features, and so on. Assess performance on the test set: Using an appropriate metric (e.g. Precision@k Precision@k , Recall Recall , AUC AUC ), examine the performance of your model \"out of sample.\" Think of new things to try. Repeat steps 1 through 4 as appropriate. If the code base is not structured and named well, you might be struggling to remember the details of each step once you have built a few models. What features did you use for each? What training and testing split? What hyperparameters? Your code might be getting messy too. Did you overwrite the code for the previous model? Maybe you copied, pasted, and edited code from an earlier model. Can you still read what's there? It can quickly become a hodgepodge that requires heroics to decipher. In this session, we will introduce a workflow that can avoid (or at least reduce) these problems.","title":"Motivation"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#data-pipelines","text":"It is helpful to structure the data into multiple layers. In data bases, a layer is expressed as schema. In most other formats, they are expressed through a directory structure.","title":"Data pipelines"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#code-setup","text":"The code repository will mirror the data pipeline by creating the corresponding folder structure for the python files. In addition, there are multiple other files that need to be stored and managed. The community has arrived at a standard setup of the project directories that we will also follow. Directory structure: \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 conf \u2502 \u251c\u2500\u2500 base <- Space for shared configurations like parameters \u2502 \u2514\u2500\u2500 local <- Space for local configurations, usually credentials \u2502 \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 01_raw <- Imutable input data \u2502 \u251c\u2500\u2500 02_intermediate<- Cleaned version of raw \u2502 \u251c\u2500\u2500 03_processed <- The data used for modelling \u2502 \u251c\u2500\u2500 04_models <- trained models \u2502 \u251c\u2500\u2500 05_model_output<- model output \u2502 \u2514\u2500\u2500 06_reporting <- Reports and input to frontend \u2502 \u251c\u2500\u2500 docs <- Space for Sphinx documentation \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is date YYYYMMDD (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `20190601-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 results <- Intermediate analysis as HTML, PDF, LaTeX, etc. \u2502 \u251c\u2500\u2500 requirements.txt <- The requirements file for reproducing the analysis environment, e.g. \u2502 generated with `pip freeze > requirements.txt` \u2502 \u251c\u2500\u2500 .gitignore <- Avoids uploading data, credentials, outputs, system files etc \u2502 \u2514\u2500\u2500 src <- Source code for use in this project. \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u251c\u2500\u2500 d00_utils <- Functions used across the project \u2502 \u2514\u2500\u2500 remove_accents.py \u2502 \u251c\u2500\u2500 d01_data <- Scripts to reading and writing data etc \u2502 \u2514\u2500\u2500 load_data.py \u2502 \u251c\u2500\u2500 d02_intermediate<- Scripts to transform data from raw to intermediate \u2502 \u2514\u2500\u2500 create_int_payment_data.py \u2502 \u251c\u2500\u2500 d03_processing <- Scripts to turn intermediate data into modelling input \u2502 \u2514\u2500\u2500 create_master_table.py \u2502 \u251c\u2500\u2500 d04_modelling <- Scripts to train models and then use trained models to make \u2502 \u2502 predictions \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u251c\u2500\u2500 d05_model_evaluation<- Scripts that analyse model performance and model selection \u2502 \u2514\u2500\u2500 calculate_performance_metrics.py \u2502 \u251c\u2500\u2500 d06_reporting <- Scripts to produce reporting tables \u2502 \u2514\u2500\u2500 create_rpt_payment_summary.py \u2502 \u2514\u2500\u2500 d06_visualisation<- Scripts to create frequently used plots \u2514\u2500\u2500 visualise_patient_journey.py","title":"Code setup"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#workflow","text":"The typical workflow to develop code is the following: Prototype code in a jupyter notebook Move code into a function that takes data and parameters as inputs and returns the processed data or trained model as output. Test the function in the jupyter notebook Move the function into the src folder Import the function in the jupyter notebook Test the function is working Functions can be imported into a notebook as follows. First we tell the notebook where the functions are import os import sys src_dir = os.path.join(os.getcwd(), '..', 'src') sys.path.append(src_dir) Then we state which functions to import from d00_utils.my_fun import my_fun Try it!","title":"Workflow"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#code-pipeline","text":"The code that produces the different layers of the data pipeline should be abstracted into functions. A code pipeline is a set of code that handles all the computational tasks your project needs from beginning to end. The simplest pipeline is a set of functions strung together. For example, int_data = create_int_data(raw_data) pro_drug_features = create_pro_drug_features(int_data) pro_patient_features = create_pro_patient_features(int_data) pro_master_table = create_pro_master_table(pro_drug_features, pro_patient_features) model = train_model(pro_master_table) rpt_report = produce_report(model) This is a very schematic example. Typically, each step is broken down into a number of subsets creating pipelines for each layer of the data pipeline. The end-to-end pipeline is then the concatenation of the sub-pipelines.","title":"Code pipeline"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#examples","text":"","title":"Examples"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#resources","text":"Our lead pipeline , started at DSSG 2014 Our Cincinnati pipeline , started at DSSG 2015 Triage (a generalized DSSG pipeline)","title":"Resources"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2016_welcome_email/","text":"Hey DSSG 2016 Fellows! Jane Zanzig, Benedict Kuester, and Eduardo Blancas Reyes reporting. We\u2019re SUPER stoked that DSSG 2016 is less than a month away. A lot of you have expressed both excitement about the fellowship and trepidation about being prepared. We are here to help! Here is a guide of everything you need to install before showing up in Chicago, on your machine as well as in your brain. We expect everyone should at least be familiar with Python (or R) to an intermediate level, and have some knowledge of data analysis, computers, and stats. However, everyone comes from different backgrounds. Make sure you have everything on the Things to Install list installed, look through the Explanations & Tutorials section if you don't know why/how to use any of these tools, and check out the Background Reading section to brush up on quantitative social science or machine learning (or both!) and you\u2019ll be ready to roll. Cry out for help on Slack if you are having trouble! And get used to it, you\u2019ll be doing a lot of it over the summer (the Slacking, not the crying). Things to Install # SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pip + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter R DBeaver (to query databases) Tableau (get the free license for students) Sublime Text (if you need a decent editor) If you are an OS X user, we highly recommend to install Homebrew to make software installation easier. For more information on requirements, see this . A guide to install pre-requirements on OS X is available here . Explanations & Tutorials # Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). - General command line navigation - Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . - grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at the Fellowship. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). - Python Package & Environment Management: To properly create a Python environment, we recommend you use Anaconda or Miniconda . If you feel adventurous, feel free to use pip + virtualenv. - Python Programming Resources: - Writing efficient Python - Tips for Idiomatic Python - Introduction to Python debugging tools - Jupyter (formerly known as IPython) - Example of how IPython notebooks are used in data science - Tutorial for setting up and opening IPython notebook - Amazing examples of IPython notebooks R R is an open-source programming language for statistical analysis, with lots of great packages for modeling and visualization. - RStudio (IDE made for R) - Shiny - Shiny is a web framework for R, so that you can build interactive visualizations and web widgets. You can use this to prototype tools for project partners to visualize and understand your analyses. Databases We typically use postgres for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. - Installing Git - Complete Beginner\u2019s Guide to Git and Github - 10-minute Hello World Tutorial to using Git and Github - Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: Unless you prefer to program using vim/emacs, we suggest you install a general purpose text editor, such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license. Background Reading # General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science","title":"2016 welcome email"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2016_welcome_email/#things-to-install","text":"SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pip + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter R DBeaver (to query databases) Tableau (get the free license for students) Sublime Text (if you need a decent editor) If you are an OS X user, we highly recommend to install Homebrew to make software installation easier. For more information on requirements, see this . A guide to install pre-requirements on OS X is available here .","title":"Things to Install"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2016_welcome_email/#explanations-tutorials","text":"Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). - General command line navigation - Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . - grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at the Fellowship. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). - Python Package & Environment Management: To properly create a Python environment, we recommend you use Anaconda or Miniconda . If you feel adventurous, feel free to use pip + virtualenv. - Python Programming Resources: - Writing efficient Python - Tips for Idiomatic Python - Introduction to Python debugging tools - Jupyter (formerly known as IPython) - Example of how IPython notebooks are used in data science - Tutorial for setting up and opening IPython notebook - Amazing examples of IPython notebooks R R is an open-source programming language for statistical analysis, with lots of great packages for modeling and visualization. - RStudio (IDE made for R) - Shiny - Shiny is a web framework for R, so that you can build interactive visualizations and web widgets. You can use this to prototype tools for project partners to visualize and understand your analyses. Databases We typically use postgres for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. - Installing Git - Complete Beginner\u2019s Guide to Git and Github - 10-minute Hello World Tutorial to using Git and Github - Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: Unless you prefer to program using vim/emacs, we suggest you install a general purpose text editor, such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license.","title":"Explanations &amp; Tutorials"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2016_welcome_email/#background-reading","text":"General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science","title":"Background Reading"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2019_welcome_email/","text":"Hey DSSG 2019 Fellows! We\u2019re excited that DSSG 2019 is comning up soon! A lot of you have expressed both excitement about the fellowship and trepidation about being prepared. We are here to help! Here is a guide of everything you need to install before showing up in Chicago, on your machine as well as in your brain. We expect everyone should at least be familiar with Python to an intermediate level, and have some knowledge of data analysis, computers, and stats. However, everyone comes from different backgrounds. Make sure you have everything on the Things to Install list installed, look through the Explanations & Tutorials section if you don't know why/how to use any of these tools, and check out the Background Reading section to brush up on quantitative social science or machine learning (or both!) and you\u2019ll be ready to roll. Cry out for help on Slack if you are having trouble! And get used to it, you\u2019ll be doing a lot of it over the summer (the Slacking, not the crying). Things to Install # SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pyenv + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter Seaborn R (you won't need it but it may make some of you feel better) DBeaver (to query databases) Tableau (get the free license for students) GNU/Emacs, VIM or \u2026 Sublime Text (if you need a decent editor) If you are an OS X user, we recommend to install Homebrew to make software installation easier. A guide to install pre-requirements on OS X is available here . Explanations & Tutorials # Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). General command line navigation Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at DSSG. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). Python Package & Environment Management: To properly create a Python environment, we recommend you use [Anaconda] 1 or [Miniconda] 2 . If you feel adventurous, feel free to use pip + virtualenv. Python Programming Resources: [Writing efficient Python] 3 [Tips for Idiomatic Python] 4 [Introduction to Python debugging tools] 5 Jupyter Notebooks and Jupyter Lab [Beginner's Guide to Jupyter Lab] 9 [Example of how IPython notebooks are used in data science] 6 [Tutorial for setting up and opening IPython notebook] 7 [Amazing examples of IPython notebooks] 8 Databases We typically use Postgres (full name PostgreSQL) for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. Installing Git Complete Beginner\u2019s Guide to Git and Github 10-minute Hello World Tutorial to using Git and Github Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: If for some weird reason 10 you don\u2019t use GNU/Emacs or VIM , we suggest you install text editor such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license. Background Reading # General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Machine Learning book chapter from Big Data and Social Science Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science https://www.continuum.io/downloads \u21a9 http://conda.pydata.org/miniconda.html \u21a9 https://www.memonic.com/user/pneff/folder/python/id/1bufp \u21a9 https://web.archive.org/web/20180411011411/http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html \u21a9 https://web.archive.org/web/20141209082719/https://blog.safaribooksonline.com/2014/11/18/intro-python-debugger/ \u21a9 http://nbviewer.ipython.org/github/jvns/talks/blob/master/pydatanyc2013/PyData%20NYC%202013%20tutorial.ipynb \u21a9 http://opentechschool.github.io/python-data-intro/core/notebook.html \u21a9 https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-and-IPython-Notebooks \u21a9 https://medium.com/@brianray_7981/jupyterlab-first-impressions-e6d70d8a175d \u21a9 Why do you do that to yourself? \u21a9","title":"2019 welcome email"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2019_welcome_email/#things-to-install","text":"SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pyenv + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter Seaborn R (you won't need it but it may make some of you feel better) DBeaver (to query databases) Tableau (get the free license for students) GNU/Emacs, VIM or \u2026 Sublime Text (if you need a decent editor) If you are an OS X user, we recommend to install Homebrew to make software installation easier. A guide to install pre-requirements on OS X is available here .","title":"Things to Install"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2019_welcome_email/#explanations-tutorials","text":"Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). General command line navigation Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at DSSG. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). Python Package & Environment Management: To properly create a Python environment, we recommend you use [Anaconda] 1 or [Miniconda] 2 . If you feel adventurous, feel free to use pip + virtualenv. Python Programming Resources: [Writing efficient Python] 3 [Tips for Idiomatic Python] 4 [Introduction to Python debugging tools] 5 Jupyter Notebooks and Jupyter Lab [Beginner's Guide to Jupyter Lab] 9 [Example of how IPython notebooks are used in data science] 6 [Tutorial for setting up and opening IPython notebook] 7 [Amazing examples of IPython notebooks] 8 Databases We typically use Postgres (full name PostgreSQL) for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. Installing Git Complete Beginner\u2019s Guide to Git and Github 10-minute Hello World Tutorial to using Git and Github Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: If for some weird reason 10 you don\u2019t use GNU/Emacs or VIM , we suggest you install text editor such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license.","title":"Explanations &amp; Tutorials"},{"location":"curriculum/0_before_you_start/prerequisites/old_welcome_emails/2019_welcome_email/#background-reading","text":"General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Machine Learning book chapter from Big Data and Social Science Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science https://www.continuum.io/downloads \u21a9 http://conda.pydata.org/miniconda.html \u21a9 https://www.memonic.com/user/pneff/folder/python/id/1bufp \u21a9 https://web.archive.org/web/20180411011411/http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html \u21a9 https://web.archive.org/web/20141209082719/https://blog.safaribooksonline.com/2014/11/18/intro-python-debugger/ \u21a9 http://nbviewer.ipython.org/github/jvns/talks/blob/master/pydatanyc2013/PyData%20NYC%202013%20tutorial.ipynb \u21a9 http://opentechschool.github.io/python-data-intro/core/notebook.html \u21a9 https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-and-IPython-Notebooks \u21a9 https://medium.com/@brianray_7981/jupyterlab-first-impressions-e6d70d8a175d \u21a9 Why do you do that to yourself? \u21a9","title":"Background Reading"},{"location":"curriculum/1_getting_and_keeping_data/","text":"Getting and Keeping Data # Data comes in many forms, from many sources - you may get a database dump directly from a project partner, or you may need to scrape data from the web (see Basic Web Scraping ). Either way, once you've got your hands on some data, you'll need to bring it into a database , and start formatting it in such a way that you can use it for analysis. Command Line Tools will start to come in handy here. If your data is in a format that resembles CSV this instructions will be helpful . You'll definitely want to keep track of the steps you took to go from raw data to model-ready data ( Reproducible ETL ). Often data science for social good projects will involve sensitive data, so it's important to be aware of some basic principles of data security: Data Security Primer .","title":"Intro"},{"location":"curriculum/1_getting_and_keeping_data/#getting-and-keeping-data","text":"Data comes in many forms, from many sources - you may get a database dump directly from a project partner, or you may need to scrape data from the web (see Basic Web Scraping ). Either way, once you've got your hands on some data, you'll need to bring it into a database , and start formatting it in such a way that you can use it for analysis. Command Line Tools will start to come in handy here. If your data is in a format that resembles CSV this instructions will be helpful . You'll definitely want to keep track of the steps you took to go from raw data to model-ready data ( Reproducible ETL ). Often data science for social good projects will involve sensitive data, so it's important to be aware of some basic principles of data security: Data Security Primer .","title":"Getting and Keeping Data"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/","text":"Web Scraping 101 # Background and Motivation # With the vast amount of data on the internet, an important question is how to to access it. Normally, we just use a web browser to go to a website and view it manually. Other times a site may have a data portal we can use to download curated data or they an API. In the case where there is useful data stored in web pages we can create programs for the automatic and systematic gathering and parsing of data from the web. Go to the notebook Further Resources # Stanford Tutorial Web Scraping with Python","title":"APIs and scrapping"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/#web-scraping-101","text":"","title":"Web Scraping 101"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/#background-and-motivation","text":"With the vast amount of data on the internet, an important question is how to to access it. Normally, we just use a web browser to go to a website and view it manually. Other times a site may have a data portal we can use to download curated data or they an API. In the case where there is useful data stored in web pages we can create programs for the automatic and systematic gathering and parsing of data from the web. Go to the notebook","title":"Background and Motivation"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/#further-resources","text":"Stanford Tutorial Web Scraping with Python","title":"Further Resources"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/","text":"CSVs to the Database # Motivation # This summer, you will use a database to store and analyze data. Databases have several advantages over using text files such as CSVs: Databases can store information about relationships between tables. We're collecting more and more data -- often too much to fit in memory. Most databases can handle this. Databases can provide integrity checks and guarantees. If you have a column of numbers in a spreadsheet, Excel will let you change a random cell to text. In contrast, you can tell your database to only accept input that meets your conditions (e.g. type, uniqueness). This is especially important for ongoing projects, where you have new data coming in. Databases allow you to store data in one place. That makes updates easy and reliable. Databases are more secure. You can more carefully control who has which types of access to what data better in a database than with a CSV. Databases can handle multiple users. Concurrent edits to a CSV can get messy. Some file systems won't even let multiple users access a CSV at the same time. Databases are designed to help you do analysis. SQL will probably become your best friend. You'll likely have to load CSVs into your database (e.g. from the open data portal), even if your partner gave a database dump ( which is ideal ). This session builds on what you learned last week in the pipeline and command line sessions. We will focus on ETL. Tools # psql (command line) dBeaver csvkit Notice that we're not using pandas. DO NOT COPY DATA INTO THE DATABASE USING PANDAS. We strongly recommend using psql , which is orders of magnitude faster. Basic Database Lingo # Database server or host : the computer on which the database is running. We will use Amazon RDS. Database : a self-contained set of tables and schema. A server can run many databases. This summer, we will operate databases for almost all projects from the same Amazon server. Schema : similar to a folder. A database can contain many schema, each containing many tables. Tables : tables are like spreadsheets. They have rows and columns and values in each cell. Views : views are virtual tables created by a query but only instantiated when the query is run. They can be used as tables but are generated \"on-demand\" when they're used. An advantage is that they always contain the most current data but take time to compute. Queries : Queries are analysis that you run on a database, often in SQL. Let's Rock Some Data! # Connecting to the database # Some unique aspects of the setup at DSSG: You cannot access the database server directly; you have to connect to the University's secure network and tunnel go through one of the EC2 instances. The data are far safer that way: you have to access the University's secure network, then one of our EC2s, and then the database. There are two ways to connect to the database (once you're on the University network): Connect from your laptop : Use an SSH tunnel to pass data between your laptop and the database. You have a database program running locally. If you're using dBeaver, you're connecting from your laptop. Connect from the EC2 : SSH into the EC2 and run everything from there. Your laptop only sends your commands to the EC2; the EC2 does the work. You don't use an SSH tunnel because everything stays on the EC2. You can use option 1 (especially dBeaver) to explore the data, but you should use option 2 to load the data. First, downloading the datasets to your laptop may violate our contracts. Second, the internet connections will be better. The connections within Amazon are pretty fast; the connections from our office to Amazon might not be. Option 2 keeps the heavy transfers on Amazon's infrastructure. Getting data into a Database # There are three steps to get a CSV into an existing database: 1. Create table : This involves figuring out the structure of the table (how many fields, what should they be called, and what data types they are). Once you figure out the structure, you can create a sql \"CREATE TABLE\" statement and run that to generate an empty table* 2. Copy CSV to the table : Every database has a \"bulk\" copy command that is much more efficient than using pandas. Please do not use pandas to copy large csvs to a database. Postgres has a COPY command that can now copy your csv to the table you just created. 3. Check if it copied successfully : You want to check if your table now has the same number of rows and columns as the CSV (as well as other consistency checks). If it did not copy successfully, you may need to modify the table structure, clean the csv to remove characters, replace nulls, and try steps 1 and 2 again. Step 1: Let's get the structure of the data # In this session, we will put the City of Chicago's food-inspection data into the DSSG training database. Start by SSHing into the training server: ssh your_username@the_training_EC2_address Create a folder for yourself in the EC2 training directory and download the data: cd /mnt/data/projects/training/ mkdir jwalsh cd jwalsh wget -O inspections.csv https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD This gives you a file called inspections.csv . You can explore the data using head , tail , csvlook , and other command-line tools you've learned in previous sessions. Here's the output from csvlook : csvsql generates create table statements for you. Because it uses Python, it will load all the data and then do its thing. That can be really inefficient for large datasets: you have to wait to read the entire dataset, and you need lots of memory to do it. To limit the resources csvsql needs, I'll only use the first 1000 rows. We're using a PostgreSQL (\"Postgres\") database: head -n1000 inspections.csv | csvsql -i postgresql Here's the output: CREATE TABLE stdin ( \"Inspection ID\" DECIMAL NOT NULL, \"DBA Name\" VARCHAR NOT NULL, \"AKA Name\" VARCHAR, \"License #\" DECIMAL NOT NULL, \"Facility Type\" VARCHAR, \"Risk\" VARCHAR, \"Address\" VARCHAR NOT NULL, \"City\" VARCHAR NOT NULL, \"State\" VARCHAR, \"Zip\" DECIMAL NOT NULL, \"Inspection Date\" DATE NOT NULL, \"Inspection Type\" VARCHAR NOT NULL, \"Results\" VARCHAR NOT NULL, \"Violations\" VARCHAR, \"Latitude\" DECIMAL, \"Longitude\" DECIMAL, \"Location\" VARCHAR ); A few things to note: * Inspection ID, DBA Name, AKA Name, etc. are column names. * VARCHAR and INTEGER are column types. VARCHAR(11) means variable character length column up to 11 characters. If you try to give a character column a number, an integer column a decimal, and so on, Postgres will prevent the entire transfer. * NOT NULL means you have to provide a value for that column. * Postgres hates uppercase and spaces in column names. If you have either, you need to wrap the column name in quotation marks. Yuck. * We need to replace stdin with the table name ( jwalsh.jwalsh ). Let's give it another shot: head -n 1000 inspections.csv | tr [:upper:] [:lower:] | tr ' ' '_' | sed 's/#/num/' | csvsql -i postgresql --db-schema jwalsh --tables jwalsh tr [:upper:] [:lower:] converts all uppercase to all lowercase. tr ' ' '_' converts all spaces to underscores. sed 's/#/num/' replaces the pound sign with \"num\". csvsql -i postgresql --db-schema jwalsh --tables jwalsh generates the postgres create table statement. Here's the output: CREATE TABLE jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); csvsql ain't perfect. We could still make changes if we wanted, e.g. changing the license_num column type. But DECIMAL is good enough for this exercise. Let's create the schema and table # Remember, the schema is like a folder. You can use schema to categorize your tables. Let's use a script, which I'll call inspections.sql , to create the schema and table. Here's what it looks like: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh; CREATE TABLE IF NOT EXISTS jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); A few things to note: * The first row sets the role. You need to assume a role that has write permissions to create schemas and tables and to copy data. * I added IF NOT EXISTS conditions for create schema and create table . You don't need those if you run the script once, but if you run the script multiple times, you'll get errors if those already exist. * The create table statement is from above. Step 2: Let's copy the data # All you've given the database to this point is a schema and an empty table. Use psql's \\copy command to get data into the table: \\copy [db table] from '[source CSV]' with csv header . I'll add it to the inspections.sql script: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; To run the script securely, follow these data security guidelines by storing the database credentials in a file. Postgres looks for four environment variables: PGHOST, PGUSER, PGPASSWORD, and PGDATABASE. To set the environment variables using default_profile (copy and modify from default_profile.example ): eval $(cat default_profile) Then psql -f inspections.sql Uh oh, we got an error: Password for user jwalsh: SET psql:copy_example.sql:2: ERROR: null value in column \"city\" violates not-null constraint DETAIL: Failing row contains (2145008, INTERURBAN, INTERURBAN, 2492070, Restaurant, Risk 1 (High), 1438 W CORTLAND ST , null, null, 60642, 2018-02-15, License, Pass, null, 41.916996072966775, -87.6645967198223, (41.916996072966775, -87.6645967198223)). CONTEXT: COPY jwalsh_table, line 7960: \"2145008,INTERURBAN,INTERURBAN,2492070,Restaurant,Risk 1 (High),1438 W CORTLAND ST ,,,60642,02/15/201...\" With Postgres copy , either the entire copy is successful or none of it is. Check your table: nothing is there. I'll modify inspections.sql to allow missing values and try again: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; DROP TABLE IF EXISTS jwalsh_schema.jwalsh_table; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL, dba_name VARCHAR, aka_name VARCHAR, license_num DECIMAL, facility_type VARCHAR, risk VARCHAR, address VARCHAR, city VARCHAR, state VARCHAR, zip DECIMAL, inspection_date DATE, inspection_type VARCHAR, results VARCHAR, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; Run the script: psql -f inspections.sql Step 3: Let's look at the data and make sure everything is there # Check if the data are there. Here's what it looks like in dBeaver: Further Resources # Software Carpentry: Databases and SQL Discussion Notes #","title":"Flat files"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#csvs-to-the-database","text":"","title":"CSVs to the Database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#motivation","text":"This summer, you will use a database to store and analyze data. Databases have several advantages over using text files such as CSVs: Databases can store information about relationships between tables. We're collecting more and more data -- often too much to fit in memory. Most databases can handle this. Databases can provide integrity checks and guarantees. If you have a column of numbers in a spreadsheet, Excel will let you change a random cell to text. In contrast, you can tell your database to only accept input that meets your conditions (e.g. type, uniqueness). This is especially important for ongoing projects, where you have new data coming in. Databases allow you to store data in one place. That makes updates easy and reliable. Databases are more secure. You can more carefully control who has which types of access to what data better in a database than with a CSV. Databases can handle multiple users. Concurrent edits to a CSV can get messy. Some file systems won't even let multiple users access a CSV at the same time. Databases are designed to help you do analysis. SQL will probably become your best friend. You'll likely have to load CSVs into your database (e.g. from the open data portal), even if your partner gave a database dump ( which is ideal ). This session builds on what you learned last week in the pipeline and command line sessions. We will focus on ETL.","title":"Motivation"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#tools","text":"psql (command line) dBeaver csvkit Notice that we're not using pandas. DO NOT COPY DATA INTO THE DATABASE USING PANDAS. We strongly recommend using psql , which is orders of magnitude faster.","title":"Tools"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#basic-database-lingo","text":"Database server or host : the computer on which the database is running. We will use Amazon RDS. Database : a self-contained set of tables and schema. A server can run many databases. This summer, we will operate databases for almost all projects from the same Amazon server. Schema : similar to a folder. A database can contain many schema, each containing many tables. Tables : tables are like spreadsheets. They have rows and columns and values in each cell. Views : views are virtual tables created by a query but only instantiated when the query is run. They can be used as tables but are generated \"on-demand\" when they're used. An advantage is that they always contain the most current data but take time to compute. Queries : Queries are analysis that you run on a database, often in SQL.","title":"Basic Database Lingo"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#lets-rock-some-data","text":"","title":"Let's Rock Some Data!"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#connecting-to-the-database","text":"Some unique aspects of the setup at DSSG: You cannot access the database server directly; you have to connect to the University's secure network and tunnel go through one of the EC2 instances. The data are far safer that way: you have to access the University's secure network, then one of our EC2s, and then the database. There are two ways to connect to the database (once you're on the University network): Connect from your laptop : Use an SSH tunnel to pass data between your laptop and the database. You have a database program running locally. If you're using dBeaver, you're connecting from your laptop. Connect from the EC2 : SSH into the EC2 and run everything from there. Your laptop only sends your commands to the EC2; the EC2 does the work. You don't use an SSH tunnel because everything stays on the EC2. You can use option 1 (especially dBeaver) to explore the data, but you should use option 2 to load the data. First, downloading the datasets to your laptop may violate our contracts. Second, the internet connections will be better. The connections within Amazon are pretty fast; the connections from our office to Amazon might not be. Option 2 keeps the heavy transfers on Amazon's infrastructure.","title":"Connecting to the database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#getting-data-into-a-database","text":"There are three steps to get a CSV into an existing database: 1. Create table : This involves figuring out the structure of the table (how many fields, what should they be called, and what data types they are). Once you figure out the structure, you can create a sql \"CREATE TABLE\" statement and run that to generate an empty table* 2. Copy CSV to the table : Every database has a \"bulk\" copy command that is much more efficient than using pandas. Please do not use pandas to copy large csvs to a database. Postgres has a COPY command that can now copy your csv to the table you just created. 3. Check if it copied successfully : You want to check if your table now has the same number of rows and columns as the CSV (as well as other consistency checks). If it did not copy successfully, you may need to modify the table structure, clean the csv to remove characters, replace nulls, and try steps 1 and 2 again.","title":"Getting data into a Database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#step-1-lets-get-the-structure-of-the-data","text":"In this session, we will put the City of Chicago's food-inspection data into the DSSG training database. Start by SSHing into the training server: ssh your_username@the_training_EC2_address Create a folder for yourself in the EC2 training directory and download the data: cd /mnt/data/projects/training/ mkdir jwalsh cd jwalsh wget -O inspections.csv https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD This gives you a file called inspections.csv . You can explore the data using head , tail , csvlook , and other command-line tools you've learned in previous sessions. Here's the output from csvlook : csvsql generates create table statements for you. Because it uses Python, it will load all the data and then do its thing. That can be really inefficient for large datasets: you have to wait to read the entire dataset, and you need lots of memory to do it. To limit the resources csvsql needs, I'll only use the first 1000 rows. We're using a PostgreSQL (\"Postgres\") database: head -n1000 inspections.csv | csvsql -i postgresql Here's the output: CREATE TABLE stdin ( \"Inspection ID\" DECIMAL NOT NULL, \"DBA Name\" VARCHAR NOT NULL, \"AKA Name\" VARCHAR, \"License #\" DECIMAL NOT NULL, \"Facility Type\" VARCHAR, \"Risk\" VARCHAR, \"Address\" VARCHAR NOT NULL, \"City\" VARCHAR NOT NULL, \"State\" VARCHAR, \"Zip\" DECIMAL NOT NULL, \"Inspection Date\" DATE NOT NULL, \"Inspection Type\" VARCHAR NOT NULL, \"Results\" VARCHAR NOT NULL, \"Violations\" VARCHAR, \"Latitude\" DECIMAL, \"Longitude\" DECIMAL, \"Location\" VARCHAR ); A few things to note: * Inspection ID, DBA Name, AKA Name, etc. are column names. * VARCHAR and INTEGER are column types. VARCHAR(11) means variable character length column up to 11 characters. If you try to give a character column a number, an integer column a decimal, and so on, Postgres will prevent the entire transfer. * NOT NULL means you have to provide a value for that column. * Postgres hates uppercase and spaces in column names. If you have either, you need to wrap the column name in quotation marks. Yuck. * We need to replace stdin with the table name ( jwalsh.jwalsh ). Let's give it another shot: head -n 1000 inspections.csv | tr [:upper:] [:lower:] | tr ' ' '_' | sed 's/#/num/' | csvsql -i postgresql --db-schema jwalsh --tables jwalsh tr [:upper:] [:lower:] converts all uppercase to all lowercase. tr ' ' '_' converts all spaces to underscores. sed 's/#/num/' replaces the pound sign with \"num\". csvsql -i postgresql --db-schema jwalsh --tables jwalsh generates the postgres create table statement. Here's the output: CREATE TABLE jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); csvsql ain't perfect. We could still make changes if we wanted, e.g. changing the license_num column type. But DECIMAL is good enough for this exercise.","title":"Step 1: Let's get the structure of the data"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#lets-create-the-schema-and-table","text":"Remember, the schema is like a folder. You can use schema to categorize your tables. Let's use a script, which I'll call inspections.sql , to create the schema and table. Here's what it looks like: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh; CREATE TABLE IF NOT EXISTS jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); A few things to note: * The first row sets the role. You need to assume a role that has write permissions to create schemas and tables and to copy data. * I added IF NOT EXISTS conditions for create schema and create table . You don't need those if you run the script once, but if you run the script multiple times, you'll get errors if those already exist. * The create table statement is from above.","title":"Let's create the schema and table"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#step-2-lets-copy-the-data","text":"All you've given the database to this point is a schema and an empty table. Use psql's \\copy command to get data into the table: \\copy [db table] from '[source CSV]' with csv header . I'll add it to the inspections.sql script: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; To run the script securely, follow these data security guidelines by storing the database credentials in a file. Postgres looks for four environment variables: PGHOST, PGUSER, PGPASSWORD, and PGDATABASE. To set the environment variables using default_profile (copy and modify from default_profile.example ): eval $(cat default_profile) Then psql -f inspections.sql Uh oh, we got an error: Password for user jwalsh: SET psql:copy_example.sql:2: ERROR: null value in column \"city\" violates not-null constraint DETAIL: Failing row contains (2145008, INTERURBAN, INTERURBAN, 2492070, Restaurant, Risk 1 (High), 1438 W CORTLAND ST , null, null, 60642, 2018-02-15, License, Pass, null, 41.916996072966775, -87.6645967198223, (41.916996072966775, -87.6645967198223)). CONTEXT: COPY jwalsh_table, line 7960: \"2145008,INTERURBAN,INTERURBAN,2492070,Restaurant,Risk 1 (High),1438 W CORTLAND ST ,,,60642,02/15/201...\" With Postgres copy , either the entire copy is successful or none of it is. Check your table: nothing is there. I'll modify inspections.sql to allow missing values and try again: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; DROP TABLE IF EXISTS jwalsh_schema.jwalsh_table; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL, dba_name VARCHAR, aka_name VARCHAR, license_num DECIMAL, facility_type VARCHAR, risk VARCHAR, address VARCHAR, city VARCHAR, state VARCHAR, zip DECIMAL, inspection_date DATE, inspection_type VARCHAR, results VARCHAR, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; Run the script: psql -f inspections.sql","title":"Step 2: Let's copy the data"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#step-3-lets-look-at-the-data-and-make-sure-everything-is-there","text":"Check if the data are there. Here's what it looks like in dBeaver:","title":"Step 3: Let's look at the data and make sure everything is there"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#further-resources","text":"Software Carpentry: Databases and SQL","title":"Further Resources"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#discussion-notes","text":"","title":"Discussion Notes"},{"location":"curriculum/1_getting_and_keeping_data/databases/","text":"Databases 101 # Background and Motivation # In the case of small data (you can load it all into memory), simple analysis (maps well to your statistical package of choice), and no plans for you or anyone else to repeat your analysis (nor receive updated data), then keeping your data in text files, and using a scripting language like Python or R to work with it, is fine. In the case that you have a large amount of diverse data (cannot all be loaded into memory) that may be updated, or if you want to share your data with others and let others easily reproduce your analysis, then use a DBMS ( Database Management System ). DBMS are important for storing, organizing, managing and analyzing data. They mitigate the scaling and complexity problem of increasing data in volume and diversity. DBMS facilitate a data model that allows data to be stored, queried, and updated efficiently and concurrently by multiple users. In general, as a data scientist your toolkit will involve using SQL (Structured Query Language) with a database and something else-- python , R , SAS, Stata, SPSS. This tutorial covers the basics of relational databases and NoSQL databases, the pros and cons of each type of database, and when to use which one. Materials # Slides","title":"Databases 101"},{"location":"curriculum/1_getting_and_keeping_data/databases/#databases-101","text":"","title":"Databases 101"},{"location":"curriculum/1_getting_and_keeping_data/databases/#background-and-motivation","text":"In the case of small data (you can load it all into memory), simple analysis (maps well to your statistical package of choice), and no plans for you or anyone else to repeat your analysis (nor receive updated data), then keeping your data in text files, and using a scripting language like Python or R to work with it, is fine. In the case that you have a large amount of diverse data (cannot all be loaded into memory) that may be updated, or if you want to share your data with others and let others easily reproduce your analysis, then use a DBMS ( Database Management System ). DBMS are important for storing, organizing, managing and analyzing data. They mitigate the scaling and complexity problem of increasing data in volume and diversity. DBMS facilitate a data model that allows data to be stored, queried, and updated efficiently and concurrently by multiple users. In general, as a data scientist your toolkit will involve using SQL (Structured Query Language) with a database and something else-- python , R , SAS, Stata, SPSS. This tutorial covers the basics of relational databases and NoSQL databases, the pros and cons of each type of database, and when to use which one.","title":"Background and Motivation"},{"location":"curriculum/1_getting_and_keeping_data/databases/#materials","text":"Slides","title":"Materials"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/","text":"Reproducible ETL # Motivation # Understanding what you did : Save all the steps you took so you can tell how you got here. Using what you did : Use and re-use code for this project or for others. Fix errors easily. Import new data with confidence. GET IT IMPLEMENTED! This session builds on what you learned last week in the CSV to DB session. Potential Teachouts # Proprietary-database transfers (e.g. SQL Server, Oracle) Concepts # Many people associate data science with fancy machine-learning algorithms, but ETL is arguably more important. ETL: Extract : Get data from the source, e.g. a CSV the partner gave you. Transform : Get the data into the format you want/need, e.g. standardize missing values. Load : Get the data into the database. There are two reasons why ETL matters so much: The rest of your analysis depends on your ETL. For example, you might ignore some of the most important cases if you drop rows with missing values . Better data can help more than better methods. So you should do ETL well: Reliably Understandably Preferably automatically Tools: Code is typically better than GUIs. Code can be automated. All else being equal, command-line tools are good choices. They are time tested and efficient. make (command-line tool written to compile software efficiently) If you can't save the code, save the notes, e.g. record how you used Pentaho to transfer an Oracle database to PostgreSQL. Examples # Hitchhiker's Guide Weather Example # Remember the weather example ? Let's make sure it's reproducible. I stored the code in two files: jwalsh_table.sql drops jwalsh_schema.jwalsh_table if it exists, creates the table using our statement from the CSV-to-DB session, then copies the data. To run it, make sure you specify the PostgresQL environment variables in default_profile , then type drake while in this directory. I've run this code many times without error, and I feel pretty confident that it will continue to run without error for a while. Because we wrote some decent ETL code, we don't need to start from scratch. We can borrow the code for this project. (Some of this code originated with the lead project .) Let's say NOAA changes the format of the weather file. This code will throw an error when we try to run it. We don't need to start from scratch. We can simply modify jwalsh_table.sql to match the new format, re-run the code without error, and enjoy the up-to-date data. ETL Tests # You run the risk of losing or corrupting data with each step. To ensure that you extracted, transformed, and loaded the data correctly, you can run simple checks. Here are a few ways: If you're copying a file, check the hash before and after. They should match. Count rows. If they should match before and after, do they? Check aggregate statistics, such as the sum of a column. If they should match before and after, do they? If you receive a database dump, count the number of schemas/tables/sequences/etc in the origin and destination databases. Do they match? For example, we request partners who use Oracle to run the count_rows_oracle.sql script, which gives the number of rows and columns in the origin database. It's one (necessary but not sufficient) way to check that we got all the data. Lead Project # Well-developed ETL in input/ . Sanergy Project # input/Drakefile calls an R script. The repository is here . What If You Have to Use Points and Clicks? # See last year's police team Oracle directions . Discussion #","title":"ETL - cleaning, loading"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#reproducible-etl","text":"","title":"Reproducible ETL"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#motivation","text":"Understanding what you did : Save all the steps you took so you can tell how you got here. Using what you did : Use and re-use code for this project or for others. Fix errors easily. Import new data with confidence. GET IT IMPLEMENTED! This session builds on what you learned last week in the CSV to DB session.","title":"Motivation"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#potential-teachouts","text":"Proprietary-database transfers (e.g. SQL Server, Oracle)","title":"Potential Teachouts"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#concepts","text":"Many people associate data science with fancy machine-learning algorithms, but ETL is arguably more important. ETL: Extract : Get data from the source, e.g. a CSV the partner gave you. Transform : Get the data into the format you want/need, e.g. standardize missing values. Load : Get the data into the database. There are two reasons why ETL matters so much: The rest of your analysis depends on your ETL. For example, you might ignore some of the most important cases if you drop rows with missing values . Better data can help more than better methods. So you should do ETL well: Reliably Understandably Preferably automatically Tools: Code is typically better than GUIs. Code can be automated. All else being equal, command-line tools are good choices. They are time tested and efficient. make (command-line tool written to compile software efficiently) If you can't save the code, save the notes, e.g. record how you used Pentaho to transfer an Oracle database to PostgreSQL.","title":"Concepts"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#examples","text":"","title":"Examples"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#hitchhikers-guide-weather-example","text":"Remember the weather example ? Let's make sure it's reproducible. I stored the code in two files: jwalsh_table.sql drops jwalsh_schema.jwalsh_table if it exists, creates the table using our statement from the CSV-to-DB session, then copies the data. To run it, make sure you specify the PostgresQL environment variables in default_profile , then type drake while in this directory. I've run this code many times without error, and I feel pretty confident that it will continue to run without error for a while. Because we wrote some decent ETL code, we don't need to start from scratch. We can borrow the code for this project. (Some of this code originated with the lead project .) Let's say NOAA changes the format of the weather file. This code will throw an error when we try to run it. We don't need to start from scratch. We can simply modify jwalsh_table.sql to match the new format, re-run the code without error, and enjoy the up-to-date data.","title":"Hitchhiker's Guide Weather Example"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#etl-tests","text":"You run the risk of losing or corrupting data with each step. To ensure that you extracted, transformed, and loaded the data correctly, you can run simple checks. Here are a few ways: If you're copying a file, check the hash before and after. They should match. Count rows. If they should match before and after, do they? Check aggregate statistics, such as the sum of a column. If they should match before and after, do they? If you receive a database dump, count the number of schemas/tables/sequences/etc in the origin and destination databases. Do they match? For example, we request partners who use Oracle to run the count_rows_oracle.sql script, which gives the number of rows and columns in the origin database. It's one (necessary but not sufficient) way to check that we got all the data.","title":"ETL Tests"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#lead-project","text":"Well-developed ETL in input/ .","title":"Lead Project"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#sanergy-project","text":"input/Drakefile calls an R script. The repository is here .","title":"Sanergy Project"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#what-if-you-have-to-use-points-and-clicks","text":"See last year's police team Oracle directions .","title":"What If You Have to Use Points and Clicks?"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#discussion","text":"","title":"Discussion"},{"location":"curriculum/2_data_exploration_and_analysis/","text":"Data Exploration and Analysis # Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data . Data Exploration Tips # Here are some things you want to do during data exploration: distributions of different variables - historgrams, boxplots distinct values for a categorical variable correlations between variables - you can do a correlation matrix and turn it into a heatmap changes and trends over time - how does the data and the entities in the data change over time. Distributions over time. missing values: are there lots of missing values? is there any pattern there? looking at outliers - this can be done using clustering but also using other methods by plotting distributions. cross-tabs (if you're looking at multiple classes/labels), describing how the positive and negative classes are different without doing any machine learning.","title":"Introduction to EDA"},{"location":"curriculum/2_data_exploration_and_analysis/#data-exploration-and-analysis","text":"Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data .","title":"Data Exploration and Analysis"},{"location":"curriculum/2_data_exploration_and_analysis/#data-exploration-tips","text":"Here are some things you want to do during data exploration: distributions of different variables - historgrams, boxplots distinct values for a categorical variable correlations between variables - you can do a correlation matrix and turn it into a heatmap changes and trends over time - how does the data and the entities in the data change over time. Distributions over time. missing values: are there lots of missing values? is there any pattern there? looking at outliers - this can be done using clustering but also using other methods by plotting distributions. cross-tabs (if you're looking at multiple classes/labels), describing how the positive and negative classes are different without doing any machine learning.","title":"Data Exploration Tips"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/","text":"Advanced SQL for data analysis # Test your connection # If you are using psql use the following command to connect: psql postgresql://your_host:your_password@db_host:db_port/training If you are using a graphical client: host: db_host port: db_port username: your_username password: your_password db: training NOTE It is very likely that you are using a different database, please adjust the previous connection parameters. The punchline # Databases are not only for storage they are for manipulating in an efficient way your data: Try to do the data manipulation near to where the data is located. The food inspections data set # The data represents the inspections made in different facilities in the area of Chicago. There are different types of inspections, different types of facilities and different results (or outcomes) of that inspections. Also the data contains the types of violations and text descriptions in free form about the violations. Obviously, we have spatio-temporal data (i.e. the inspections happen in a given time at some place). Some basic tasks in a data analysis project # Cleaning the data Manipulating the data Create new FeatureS Create new views of the data Answering analytical questions Cleaning and manipulating the data # We already prepared a partial \"cleaning\" of the data. That data is located in the schema cleaned . Hands-on Expected time: 2 minutes Feel the data: How many tables are there? Which are the respective columns? How many rows per table? Any idea about how to join them? Look at the inspection 2078651 , How many violations does it had? Let's move on, for most of the analytical purposes (related to data science) we need a consolidated view of the entities in our data, i.e. we need to denormalize the data. We will call to this new table the semantic view of the data. If we do a simple join between this two tables, we will get many rows per inspection. And that will complicate our future analysis. So we need a way of collapse those rows, without losing data. Manipulating the data: JSON # PostgreSQL supports collapsing several rows using arrays or JSON . We will transform the rows of the cleaned.violations table into json and we will aggregate those into a json array . We will do this together using the functions row_to_json and json_agg. select json_agg ( row_to_json ( v . * ) ) as violations from cleaned . violations as v where inspection = '2078651' We could improve the output (make it more pretty) using the function json_build_object , and a simple group by select v . inspection , v . license_num , v . date , json_agg ( json_build_object ( 'code' , v . code , 'description' , v . description , 'comment' , v . comment ) ) as violations from cleaned . violations as v where inspection = '2078651' group by v . inspection , v . license_num , v . date ; -- We need a group by since we are using an aggregator function Hands-on Estimated time: 1 minute Manipulate the previous query statement and try to join it with the inspections (You should get only one row) Cleaning your code and (maybe) gaining a little speed: CTEs # It is very probable that you use a sub-query in you previous hands-on. There is a better way of doing it, and is using Common Table Expressions (CTEs) also know as WITH queries . This will improve your readability (be nice wih the future you!) and in some cases speed improvements -- You first define your subquery and assign a name to it -- This will work as a \"common table\" with violations as ( select v . inspection , v . license_num , v . date , json_agg ( json_build_object ( 'code' , v . code , 'description' , v . description , 'comment' , v . comment ) ) as violations from cleaned . violations as v group by v . inspection , v . license_num , v . date ) -- Then you can use it select i . * , v . violations from cleaned . inspections as i left join violations as v -- Here we are using the \"common table\" using ( inspection ); -- we can use this, since both tables have the same column name You can use several CTEs, just remove all except the first with and separate them by colons. We will show you more examples later in this workshop. Querying unstructured data # We created for you the table semantic.events , and is very similar to the results of your last hands-on. For querying json unstructured data, PostgreSQL provides you with the operator ->> . This operator extracts the value of the key in the json. We first need to transform the array of json objects ( unnest it) into rows (using jsonb_array_elements , and then use the operator ->> for retrieving the value of the specified key. with violations as ( select event_id , jsonb_array_elements ( violations ) as violations -- This returns several rows from semantic . events where event_id = '104246' ) select event_id , violations ->> 'code' as violation_code , -- We want the value of the key 'code' count ( * ) from violations group by event_id , violation_code ; Hands-on Estimated time: 2 minutes Modify this query to get the facility (using license_num ) in which the inspectors found the biggest number of violation code 40. \"Datawarehousing\" # Generate data for a BI dashboard, that shows all total number of inspections, and their results, per city, facility type, month, year including totals and subtotals Hands-on Estimated time: 2 minutes How to solve this using basic sql? Datawarehousing functions # PostgreSQL overloaded the operator GROUP BY , so besides their normal use, now you can produce reports of aggregation metrics by sets ( GROUPING SETS ), hierarchy ( ROLLUP ) and combinations ( CUBE ) in a simple query. -- This doesn't give you the subtotals and totals select extract ( month from date ) as month , extract ( year from date ) as year , facility_type , result , count ( * ) as number_of_inspections from semantic . events where extract ( year from date ) = 2017 and extract ( month from date ) = 1 group by month , year , facility_type , result --group by GROUPING SETS (month, year, facility_type, result, ()) --group by ROLLUP (month, year, facility_type, result) --group by CUBE (month, year, facility_type, result) NOTE Instead of the function extract(...) you could use date_trunc(...) Hands-on Estimated time: 5 minutes Play with the different commented lines in the example query, if you only one the subtotal per facility_type and city , Which one you should use? Analytical Questions: Looking through the window # How do each facility' number of inspections compares to others in their facility type? Total of inspections? Average of inspections? Distance to the top? Distance from the average? How percentage of inspections where used in a particular facility? Hands-on: Estimated time: 5 minutes Try to solve this by yourself using only SELECT , GROUP BY , HAVING , WHERE Analytical Questions: Looking through the window # Window functions # They are similar to aggregate functions, but instead of operating on groups of rows to produce a single row, they act on rows related to the current row to produce the same amount of rows. There are several window functions like row_number , rank , ntile , lag , lead , first_value , last_value , nth_value . And you can use any aggregation functions: sum , count , avg , json_agg , array_agg , etc Those functions are used in window function calls . with failures_per_facility as ( select entity_id , facility_type , extract ( year from date ) as year , count ( * ) as inspections from semantic . events where extract ( year from date ) = 2015 and facility_type is not null group by entity_id , facility_type , year ) select year , entity_id , facility_type , inspections , sum ( inspections ) over w1 as \"total inspections per type\" , 100 * ( inspections :: decimal / sum ( inspections ) over w1 ):: numeric ( 18 , 1 ) as \"% of inspections\" , ( avg ( inspections ) over w1 ):: numeric ( 18 , 3 ) as \"avg inspections per type\" , inspections - avg ( inspections ) over w1 as \"distance from avg\" , first_value ( inspections ) over w2 as \"max inspections per type\" , inspections - first_value ( inspections ) over w2 as \"distance from top 1\" , dense_rank () over w2 as rank , ( nth_value ( inspections , 1 ) over w3 / inspections :: decimal ):: numeric ( 18 , 1 ) as \"rate to top 1\" , ntile ( 5 ) over w2 as ntile from failures_per_facility where facility_type = 'wholesale' window w1 as ( partition by facility_type , year ), w2 as ( partition by facility_type , year order by inspections desc ), w3 as ( partition by facility_type , year order by inspections desc rows between unbounded preceding and unbounded following ) limit 10 ; Hands-on Estimated time: 5 minutes Change the previous query to show the number of 'Fail' results instead the number of inspections. Hint: Instead of using sum( case results when 'Fail' then 1 else 0 end ) as failures you can use count(*) filter (where results = 'Fail') Analytical Questions: Using the previous row # At a given date, number of days since the last inspection? select entity_id , date as inspection_date , lag ( date , 1 ) over w1 as previous_inspection , age ( date , lag ( date , 1 ) over w1 ) as time_since_last_inspection from semantic . events where facility_type = 'wholesale' window w1 as ( partition by entity_id order by date asc ) order by entity_id , date asc ; Analytical Questions: Using some other rows # Number of violations in the last 3 inspections with violations as ( select event_id , entity_id , date , jsonb_array_elements ( violations ) as violations from semantic . events ), number_of_violations as ( select event_id , entity_id , date , count ( * ) as num_of_violations from violations group by event_id , entity_id , date ) select entity_id , date , num_of_violations , sum ( num_of_violations ) over w as running_total , array_agg ( num_of_violations ) over w as previous_violations from number_of_violations where entity_id = 11326 window w as ( partition by entity_id order by date asc rows between 3 preceding and 1 preceding ) Hands-on Estimated time: 5 minutes Which are the facilities with more changes in the risk column (i.e. lower -> medium, medium -> high, high -> medium)? Could you count how to many changes where \"up\" and how many where \"down\"? with risks as ( select date , entity_id , risk , lag ( risk , 1 ) over w as previous_risk from semantic . events window w as ( partition by entity_id order by date asc ) ) select extract ( year from date ) as year , entity_id , count ( case when risk = 'high' and previous_risk = 'medium' then 1 when risk = 'medium' and previous_risk = 'low' then 1 end ) as up , count ( case when risk = 'medium' and previous_risk = 'high' then 1 when risk = 'low' and previous_risk = 'medium' then 1 end ) as down from risks group by entity_id , extract ( year from date ) order by year , up desc , down desc limit 10 SQL \"for loops\" # Let's try to solve the following: For each facility still active, with less than 1 year, In which failed inspection they got the most violations inspected? One way to solve this is using LATERAL joins select entity_id as facility , start_time , date as inspected_at , event_id as inpection , number_of_violations from ( select entity_id , start_time , date_part ( 'year' , age ( now (), start_time )) as years_in_service from semantic . entities where end_time is null -- Still active group by entity_id , start_time having date_part ( 'year' , age ( now (), start_time )) < 1 ) as facilities , lateral ( --subquery select event_id , date , jsonb_array_length ( violations ) as number_of_violations from semantic . events where entity_id = facilities . entity_id and result = 'fail' group by event_id , date , violations order by 3 desc limit 1 ) inspections order by number_of_violations desc ; The equivalent pseudo-code is results = [] for entity_row in semantic.entities: for inspection_row in subquery: results.append( (entity_row, inspection_row) ) # We are doing more complicated things, but is just an example Meaning in text # Which are the most common words descriptions of the violations? Full Text Search # PostgreSQL has a lot of capabilities for working with text data ( fuzzy search , n-grams , etc) that you can use for searching inside the text. But the same techniques allows you to do some text analysis. The first steps of it are: removing stop words, stemming, calculating frequencies and then vectorization . See the following example: select comment , replace ( plainto_tsquery ( comment ):: text , ' & ' , ' ' ) as cleaned_comment , to_tsvector ( comment ) as vectorized_comment from cleaned . violations limit 1 ; Let's create a word count (from here you can create a word cloud, if you like it). We will use the table text_analysis.comments select regexp_split_to_table ( cleaned_comment , '\\s+' ) as word , count ( 1 ) as word_count from text_analysis . comments group by word order by word_count desc limit 50 ; Spatial awareness # Which restaurants with high risk which had an inspection are located near to public schools? select distinct on ( entity_id , s . school_nm ) entity_id , s . school_nm as \"school\" from gis . public_schools as s join semantic . events as i on ST_DWithin ( geography ( s . wkb_geometry ), geography ( i . location ), 200 ) -- This is the distance in meters where facility_type = 'restaurant' and risk = 'high' ; Spatial queries # PostgresSQL has an extension called PosGIS , that allows you to do Spatial Joins , i.e. use geographical data to answer questions as What is near? What is inside this area? What intersects or connect with this? Hands-on Estimated time: 5 min There is another table: gis.boundaries , use the function ST_Contains to calculate the number of facilities per zip code? Compare that with the count using zip_code column in the semantic.events Hint : Use a CTE\u2026 Hands-on Estimated time: 10min Generate a list with the top 5 facilities with the higher number of violations which are near to public schools Appendix # Creating the database # First the raw.inspections table create schema if not exists raw ; create table raw . inspections ( inspection varchar not null , DBA_Name varchar , AKA_Name varchar , license_Num decimal , facility_type varchar , risk varchar , address varchar , city varchar , state varchar , zip varchar , date date , type varchar , results varchar , violations varchar , latitude decimal , longitude decimal , location varchar ); Then we fill that table with data \\copy raw.inspections from program 'curl \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\"' HEADER CSV After that, we created a more \"clean\" version of the data create schema if not exists cleaned ; drop table if exists cleaned . inspections cascade ; create table cleaned . inspections as ( with cleaned as ( select inspection :: integer , btrim ( lower ( results )) as result , license_num :: integer , btrim ( lower ( dba_name )) as facility , btrim ( lower ( aka_name )) as facility_aka , case when facility_type is null then 'unknown' else btrim ( lower ( facility_type )) end as facility_type , lower ( substring ( risk from '\\((.+)\\)' )) as risk , btrim ( lower ( address )) as address , zip as zip_code , substring ( btrim ( lower ( regexp_replace ( type , 'liquor' , 'task force' , 'gi' ))) from 'canvass|task force|complaint|food poisoning|consultation|license|tag removal' ) as type , date , ST_SetSRID ( ST_MakePoint ( longitude , latitude ), 4326 ):: geography as location -- We use geography so the measurements are in meters from raw . inspections where zip is not null -- removing NULL zip codes ) select * from cleaned where type is not null ); drop table if exists cleaned . violations cascade ; create table cleaned . violations as ( select inspection :: integer , license_num :: integer , date :: date , btrim ( tuple [ 1 ]) as code , btrim ( tuple [ 2 ]) as description , btrim ( tuple [ 3 ]) as comment , ( case when btrim ( tuple [ 1 ]) = '' then NULL when btrim ( tuple [ 1 ]):: int between 1 and 14 then 'critical' -- From the documentation when btrim ( tuple [ 1 ]):: int between 15 and 29 then 'serious' else 'minor' end ) as severity from ( select inspection , license_num , date , regexp_split_to_array ( -- Create an array we will split the code, description, comment regexp_split_to_table ( -- Create a row per each comment we split by | coalesce ( -- If there isn't a violation add '- Comments:' regexp_replace ( violations , '[\\n\\r]+' , '' , 'g' ) -- Remove line breaks , '- Comments:' ) , '\\|' ) -- Split the violations , '(?<=\\d+)\\.\\s*|\\s*-\\s*Comments:' ) -- Split each violation in three -- , '\\.\\s*|\\s*-\\s*Comments:') -- Split each violation in three (Use this if your postgresql is kind off old as tuple from raw . inspections where results in ( 'Fail' , 'Pass' , 'Pass w/ Conditions' ) and license_num is not null ) as t ); The semantic.entities table create schema if not exists semantic ; drop table if exists semantic . entities cascade ; create table semantic . entities as ( with entities_date as ( select license_num , facility , facility_aka , facility_type , address , zip_code , location , min ( date ) over ( partition by license_num , facility , facility_aka , address ) as start_time , max ( case when result in ( 'out of business' , 'business not located' ) then date else NULL end ) over ( partition by license_num , facility , facility_aka , address ) as end_time from cleaned . inspections ) select distinct dense_rank () over ( w ) as entity_id , license_num , facility , facility_aka , facility_type , address , zip_code , location , start_time , end_time from entities_date window w as ( order by license_num , facility , facility_aka , facility_type , address ) -- This kinda defines an unique facility ); -- Adding some indices create index entities_ix on semantic . entities ( entity_id ); create index entities_license_num_ix on semantic . entities ( license_num ); create index entities_facility_ix on semantic . entities ( facility ); create index entities_facility_type_ix on semantic . entities ( facility_type ); create index entities_zip_code_ix on semantic . entities ( zip_code ); -- Spatial index create index entities_location_gix on semantic . entities using gist ( location ); create index entities_full_key_ix on semantic . entities ( license_num , facility , facility_aka , facility_type , address ); The semantics.events : drop table if exists semantic . events cascade ; create table semantic . events as ( with entities as ( select * from semantic . entities ), inspections as ( select i . inspection , i . type , i . date , i . risk , i . result , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , jsonb_agg ( jsonb_build_object ( 'code' , v . code , 'severity' , v . severity , 'description' , v . description , 'comment' , v . comment ) order by code ) as violations from cleaned . inspections as i inner join cleaned . violations as v on i . inspection = v . inspection group by i . inspection , i . type , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , i . date , i . risk , i . result ) select i . inspection as event_id , e . entity_id , i . type , i . date , i . risk , i . result , e . facility_type , e . zip_code , e . location , i . violations from entities as e inner join inspections as i using ( license_num , facility , facility_aka , facility_type , address , zip_code ) ); -- Add some indices create index events_entity_ix on semantic . events ( entity_id ); create index events_event_ix on semantic . events ( event_id ); create index events_type_ix on semantic . events ( type ); create index events_date_ix on semantic . events ( date desc nulls last ); create index events_facility_type_ix on semantic . events ( facility_type ); create index events_zip_code_ix on semantic . events ( zip_code ); -- Spatial index create index events_location_gix on semantic . events using gist ( location ); -- JSONB indices create index events_violations on semantic . events using gin ( violations ); create index events_violations_json_path on semantic . events using gin ( violations jsonb_path_ops ); create index events_event_entity_zip_code_date on semantic . events ( event_id desc nulls last , entity_id , zip_code , date desc nulls last ); Next we will create the table for text analytics: create schema text_analysis ; drop table if exists text_analysis . comments ; create table text_analysis . comments as ( with violations as ( select event_id , entity_id , jsonb_array_elements ( violations ) as violations from semantic . events ), cleaned as ( select event_id , entity_id , violations ->> 'comment' as original_comment , replace ( plainto_tsquery ( violations ->> 'comment' ):: text , ' & ' , ' ' ) as cleaned_comment , to_tsvector ( violations ->> 'comment' ) as vectorized_comment from violations where btrim ( violations ->> 'comment' ) <> '' ) select * from cleaned ); And finally the tables for the spatial analysis. The data was downloaded from the Chicago Data Portal . In particular we are using the location of the schools ( Chicago Public Schools - School Locations SY1415 ) and the Chicago ZIP codes boundaries ( Boundaries - ZIP Codes ). Both data sources use the WSG84 projection, by the way. You can check that you have everything setup to upload the data with the following command (I recommend to use ogr2ogr ) ogrinfo -ro PG:'host=0.0.0.0 port=5434 user=your_user password=some_password dbname=food' Again, adjust the connection string accordingly. Then unzip the files that you downloaded, first the boundaries unzip \"Boundaries - ZIP Codes.zip\" (This will create 4 files in your directory, all of them with the prefix geo_export_c0962a58-51c1-4ea4-af11-acb7ed233465 , the extensions will be shp (the spatial data), dbf (the data in tabular form), prj (specifies the projection) and shx (index information) And now the schools: unzip 'Chicago Public Schools - School Locations SY1415.zip' Be careful, now there are 4 new files, all of them with the prefix: geo_export_ebe2869c-9bf6-4a04-ae14-a01062f8fa2a , and the same extensions as before. Finally, ogr2ogr -f \"PostgreSQL\" \\ PG:'host=0.0.0.0 port=8888 user=your_user password=some_password dbname=training' \\ geo_export_c0962a58-51c1-4ea4-af11-acb7ed233465.shp \\ -nln gis.boundaries -nlt PROMOTE_TO_MULTI -lco precision=NO NOTE : I added the -nlt PROMOTE_TO_MULTI because the data source had mixed spatial types (Multipolygon and polygon), see https://trac.osgeo.org/gdal/ticket/4939 for details. NOTE : I added the -lco precision=NO due to a precision error with the data: See this Stackoverflow question for details: https://gis.stackexchange.com/questions/254671/ogr2ogr-error-importing-shapefile-into-postgis-numeric-field-overflow ogr2ogr -f \"PostgreSQL\" \\ PG:'host=0.0.0.0 port=8888 user=your_user password=some_password dbname=training' \\ geo_export_ebe2869c-9bf6-4a04-ae14-a01062f8fa2a.shp \\ -nln gis.public_schools","title":"SQL"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#advanced-sql-for-data-analysis","text":"","title":"Advanced SQL for data analysis"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#test-your-connection","text":"If you are using psql use the following command to connect: psql postgresql://your_host:your_password@db_host:db_port/training If you are using a graphical client: host: db_host port: db_port username: your_username password: your_password db: training NOTE It is very likely that you are using a different database, please adjust the previous connection parameters.","title":"Test your connection"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#the-punchline","text":"Databases are not only for storage they are for manipulating in an efficient way your data: Try to do the data manipulation near to where the data is located.","title":"The punchline"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#the-food-inspections-data-set","text":"The data represents the inspections made in different facilities in the area of Chicago. There are different types of inspections, different types of facilities and different results (or outcomes) of that inspections. Also the data contains the types of violations and text descriptions in free form about the violations. Obviously, we have spatio-temporal data (i.e. the inspections happen in a given time at some place).","title":"The food inspections data set"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#some-basic-tasks-in-a-data-analysis-project","text":"Cleaning the data Manipulating the data Create new FeatureS Create new views of the data Answering analytical questions","title":"Some basic tasks in a data analysis project"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#cleaning-and-manipulating-the-data","text":"We already prepared a partial \"cleaning\" of the data. That data is located in the schema cleaned .","title":"Cleaning and manipulating the data"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#manipulating-the-data-json","text":"PostgreSQL supports collapsing several rows using arrays or JSON . We will transform the rows of the cleaned.violations table into json and we will aggregate those into a json array . We will do this together using the functions row_to_json and json_agg. select json_agg ( row_to_json ( v . * ) ) as violations from cleaned . violations as v where inspection = '2078651' We could improve the output (make it more pretty) using the function json_build_object , and a simple group by select v . inspection , v . license_num , v . date , json_agg ( json_build_object ( 'code' , v . code , 'description' , v . description , 'comment' , v . comment ) ) as violations from cleaned . violations as v where inspection = '2078651' group by v . inspection , v . license_num , v . date ; -- We need a group by since we are using an aggregator function","title":"Manipulating the data: JSON"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#cleaning-your-code-and-maybe-gaining-a-little-speed-ctes","text":"It is very probable that you use a sub-query in you previous hands-on. There is a better way of doing it, and is using Common Table Expressions (CTEs) also know as WITH queries . This will improve your readability (be nice wih the future you!) and in some cases speed improvements -- You first define your subquery and assign a name to it -- This will work as a \"common table\" with violations as ( select v . inspection , v . license_num , v . date , json_agg ( json_build_object ( 'code' , v . code , 'description' , v . description , 'comment' , v . comment ) ) as violations from cleaned . violations as v group by v . inspection , v . license_num , v . date ) -- Then you can use it select i . * , v . violations from cleaned . inspections as i left join violations as v -- Here we are using the \"common table\" using ( inspection ); -- we can use this, since both tables have the same column name You can use several CTEs, just remove all except the first with and separate them by colons. We will show you more examples later in this workshop.","title":"Cleaning your code and (maybe) gaining a little speed: CTEs"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#querying-unstructured-data","text":"We created for you the table semantic.events , and is very similar to the results of your last hands-on. For querying json unstructured data, PostgreSQL provides you with the operator ->> . This operator extracts the value of the key in the json. We first need to transform the array of json objects ( unnest it) into rows (using jsonb_array_elements , and then use the operator ->> for retrieving the value of the specified key. with violations as ( select event_id , jsonb_array_elements ( violations ) as violations -- This returns several rows from semantic . events where event_id = '104246' ) select event_id , violations ->> 'code' as violation_code , -- We want the value of the key 'code' count ( * ) from violations group by event_id , violation_code ;","title":"Querying unstructured data"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#datawarehousing","text":"Generate data for a BI dashboard, that shows all total number of inspections, and their results, per city, facility type, month, year including totals and subtotals","title":"\"Datawarehousing\""},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#datawarehousing-functions","text":"PostgreSQL overloaded the operator GROUP BY , so besides their normal use, now you can produce reports of aggregation metrics by sets ( GROUPING SETS ), hierarchy ( ROLLUP ) and combinations ( CUBE ) in a simple query. -- This doesn't give you the subtotals and totals select extract ( month from date ) as month , extract ( year from date ) as year , facility_type , result , count ( * ) as number_of_inspections from semantic . events where extract ( year from date ) = 2017 and extract ( month from date ) = 1 group by month , year , facility_type , result --group by GROUPING SETS (month, year, facility_type, result, ()) --group by ROLLUP (month, year, facility_type, result) --group by CUBE (month, year, facility_type, result) NOTE Instead of the function extract(...) you could use date_trunc(...)","title":"Datawarehousing functions"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#analytical-questions-looking-through-the-window","text":"How do each facility' number of inspections compares to others in their facility type? Total of inspections? Average of inspections? Distance to the top? Distance from the average? How percentage of inspections where used in a particular facility?","title":"Analytical Questions: Looking through the window"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#analytical-questions-looking-through-the-window_1","text":"","title":"Analytical Questions: Looking through the window"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#window-functions","text":"They are similar to aggregate functions, but instead of operating on groups of rows to produce a single row, they act on rows related to the current row to produce the same amount of rows. There are several window functions like row_number , rank , ntile , lag , lead , first_value , last_value , nth_value . And you can use any aggregation functions: sum , count , avg , json_agg , array_agg , etc Those functions are used in window function calls . with failures_per_facility as ( select entity_id , facility_type , extract ( year from date ) as year , count ( * ) as inspections from semantic . events where extract ( year from date ) = 2015 and facility_type is not null group by entity_id , facility_type , year ) select year , entity_id , facility_type , inspections , sum ( inspections ) over w1 as \"total inspections per type\" , 100 * ( inspections :: decimal / sum ( inspections ) over w1 ):: numeric ( 18 , 1 ) as \"% of inspections\" , ( avg ( inspections ) over w1 ):: numeric ( 18 , 3 ) as \"avg inspections per type\" , inspections - avg ( inspections ) over w1 as \"distance from avg\" , first_value ( inspections ) over w2 as \"max inspections per type\" , inspections - first_value ( inspections ) over w2 as \"distance from top 1\" , dense_rank () over w2 as rank , ( nth_value ( inspections , 1 ) over w3 / inspections :: decimal ):: numeric ( 18 , 1 ) as \"rate to top 1\" , ntile ( 5 ) over w2 as ntile from failures_per_facility where facility_type = 'wholesale' window w1 as ( partition by facility_type , year ), w2 as ( partition by facility_type , year order by inspections desc ), w3 as ( partition by facility_type , year order by inspections desc rows between unbounded preceding and unbounded following ) limit 10 ;","title":"Window functions"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#analytical-questions-using-the-previous-row","text":"At a given date, number of days since the last inspection? select entity_id , date as inspection_date , lag ( date , 1 ) over w1 as previous_inspection , age ( date , lag ( date , 1 ) over w1 ) as time_since_last_inspection from semantic . events where facility_type = 'wholesale' window w1 as ( partition by entity_id order by date asc ) order by entity_id , date asc ;","title":"Analytical Questions: Using the previous row"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#analytical-questions-using-some-other-rows","text":"Number of violations in the last 3 inspections with violations as ( select event_id , entity_id , date , jsonb_array_elements ( violations ) as violations from semantic . events ), number_of_violations as ( select event_id , entity_id , date , count ( * ) as num_of_violations from violations group by event_id , entity_id , date ) select entity_id , date , num_of_violations , sum ( num_of_violations ) over w as running_total , array_agg ( num_of_violations ) over w as previous_violations from number_of_violations where entity_id = 11326 window w as ( partition by entity_id order by date asc rows between 3 preceding and 1 preceding )","title":"Analytical Questions: Using some other rows"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#sql-for-loops","text":"Let's try to solve the following: For each facility still active, with less than 1 year, In which failed inspection they got the most violations inspected? One way to solve this is using LATERAL joins select entity_id as facility , start_time , date as inspected_at , event_id as inpection , number_of_violations from ( select entity_id , start_time , date_part ( 'year' , age ( now (), start_time )) as years_in_service from semantic . entities where end_time is null -- Still active group by entity_id , start_time having date_part ( 'year' , age ( now (), start_time )) < 1 ) as facilities , lateral ( --subquery select event_id , date , jsonb_array_length ( violations ) as number_of_violations from semantic . events where entity_id = facilities . entity_id and result = 'fail' group by event_id , date , violations order by 3 desc limit 1 ) inspections order by number_of_violations desc ; The equivalent pseudo-code is results = [] for entity_row in semantic.entities: for inspection_row in subquery: results.append( (entity_row, inspection_row) ) # We are doing more complicated things, but is just an example","title":"SQL \"for loops\""},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#meaning-in-text","text":"Which are the most common words descriptions of the violations?","title":"Meaning in text"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#full-text-search","text":"PostgreSQL has a lot of capabilities for working with text data ( fuzzy search , n-grams , etc) that you can use for searching inside the text. But the same techniques allows you to do some text analysis. The first steps of it are: removing stop words, stemming, calculating frequencies and then vectorization . See the following example: select comment , replace ( plainto_tsquery ( comment ):: text , ' & ' , ' ' ) as cleaned_comment , to_tsvector ( comment ) as vectorized_comment from cleaned . violations limit 1 ; Let's create a word count (from here you can create a word cloud, if you like it). We will use the table text_analysis.comments select regexp_split_to_table ( cleaned_comment , '\\s+' ) as word , count ( 1 ) as word_count from text_analysis . comments group by word order by word_count desc limit 50 ;","title":"Full Text Search"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#spatial-awareness","text":"Which restaurants with high risk which had an inspection are located near to public schools? select distinct on ( entity_id , s . school_nm ) entity_id , s . school_nm as \"school\" from gis . public_schools as s join semantic . events as i on ST_DWithin ( geography ( s . wkb_geometry ), geography ( i . location ), 200 ) -- This is the distance in meters where facility_type = 'restaurant' and risk = 'high' ;","title":"Spatial awareness"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#spatial-queries","text":"PostgresSQL has an extension called PosGIS , that allows you to do Spatial Joins , i.e. use geographical data to answer questions as What is near? What is inside this area? What intersects or connect with this?","title":"Spatial queries"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#appendix","text":"","title":"Appendix"},{"location":"curriculum/2_data_exploration_and_analysis/advanced_sql/#creating-the-database","text":"First the raw.inspections table create schema if not exists raw ; create table raw . inspections ( inspection varchar not null , DBA_Name varchar , AKA_Name varchar , license_Num decimal , facility_type varchar , risk varchar , address varchar , city varchar , state varchar , zip varchar , date date , type varchar , results varchar , violations varchar , latitude decimal , longitude decimal , location varchar ); Then we fill that table with data \\copy raw.inspections from program 'curl \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\"' HEADER CSV After that, we created a more \"clean\" version of the data create schema if not exists cleaned ; drop table if exists cleaned . inspections cascade ; create table cleaned . inspections as ( with cleaned as ( select inspection :: integer , btrim ( lower ( results )) as result , license_num :: integer , btrim ( lower ( dba_name )) as facility , btrim ( lower ( aka_name )) as facility_aka , case when facility_type is null then 'unknown' else btrim ( lower ( facility_type )) end as facility_type , lower ( substring ( risk from '\\((.+)\\)' )) as risk , btrim ( lower ( address )) as address , zip as zip_code , substring ( btrim ( lower ( regexp_replace ( type , 'liquor' , 'task force' , 'gi' ))) from 'canvass|task force|complaint|food poisoning|consultation|license|tag removal' ) as type , date , ST_SetSRID ( ST_MakePoint ( longitude , latitude ), 4326 ):: geography as location -- We use geography so the measurements are in meters from raw . inspections where zip is not null -- removing NULL zip codes ) select * from cleaned where type is not null ); drop table if exists cleaned . violations cascade ; create table cleaned . violations as ( select inspection :: integer , license_num :: integer , date :: date , btrim ( tuple [ 1 ]) as code , btrim ( tuple [ 2 ]) as description , btrim ( tuple [ 3 ]) as comment , ( case when btrim ( tuple [ 1 ]) = '' then NULL when btrim ( tuple [ 1 ]):: int between 1 and 14 then 'critical' -- From the documentation when btrim ( tuple [ 1 ]):: int between 15 and 29 then 'serious' else 'minor' end ) as severity from ( select inspection , license_num , date , regexp_split_to_array ( -- Create an array we will split the code, description, comment regexp_split_to_table ( -- Create a row per each comment we split by | coalesce ( -- If there isn't a violation add '- Comments:' regexp_replace ( violations , '[\\n\\r]+' , '' , 'g' ) -- Remove line breaks , '- Comments:' ) , '\\|' ) -- Split the violations , '(?<=\\d+)\\.\\s*|\\s*-\\s*Comments:' ) -- Split each violation in three -- , '\\.\\s*|\\s*-\\s*Comments:') -- Split each violation in three (Use this if your postgresql is kind off old as tuple from raw . inspections where results in ( 'Fail' , 'Pass' , 'Pass w/ Conditions' ) and license_num is not null ) as t ); The semantic.entities table create schema if not exists semantic ; drop table if exists semantic . entities cascade ; create table semantic . entities as ( with entities_date as ( select license_num , facility , facility_aka , facility_type , address , zip_code , location , min ( date ) over ( partition by license_num , facility , facility_aka , address ) as start_time , max ( case when result in ( 'out of business' , 'business not located' ) then date else NULL end ) over ( partition by license_num , facility , facility_aka , address ) as end_time from cleaned . inspections ) select distinct dense_rank () over ( w ) as entity_id , license_num , facility , facility_aka , facility_type , address , zip_code , location , start_time , end_time from entities_date window w as ( order by license_num , facility , facility_aka , facility_type , address ) -- This kinda defines an unique facility ); -- Adding some indices create index entities_ix on semantic . entities ( entity_id ); create index entities_license_num_ix on semantic . entities ( license_num ); create index entities_facility_ix on semantic . entities ( facility ); create index entities_facility_type_ix on semantic . entities ( facility_type ); create index entities_zip_code_ix on semantic . entities ( zip_code ); -- Spatial index create index entities_location_gix on semantic . entities using gist ( location ); create index entities_full_key_ix on semantic . entities ( license_num , facility , facility_aka , facility_type , address ); The semantics.events : drop table if exists semantic . events cascade ; create table semantic . events as ( with entities as ( select * from semantic . entities ), inspections as ( select i . inspection , i . type , i . date , i . risk , i . result , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , jsonb_agg ( jsonb_build_object ( 'code' , v . code , 'severity' , v . severity , 'description' , v . description , 'comment' , v . comment ) order by code ) as violations from cleaned . inspections as i inner join cleaned . violations as v on i . inspection = v . inspection group by i . inspection , i . type , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , i . date , i . risk , i . result ) select i . inspection as event_id , e . entity_id , i . type , i . date , i . risk , i . result , e . facility_type , e . zip_code , e . location , i . violations from entities as e inner join inspections as i using ( license_num , facility , facility_aka , facility_type , address , zip_code ) ); -- Add some indices create index events_entity_ix on semantic . events ( entity_id ); create index events_event_ix on semantic . events ( event_id ); create index events_type_ix on semantic . events ( type ); create index events_date_ix on semantic . events ( date desc nulls last ); create index events_facility_type_ix on semantic . events ( facility_type ); create index events_zip_code_ix on semantic . events ( zip_code ); -- Spatial index create index events_location_gix on semantic . events using gist ( location ); -- JSONB indices create index events_violations on semantic . events using gin ( violations ); create index events_violations_json_path on semantic . events using gin ( violations jsonb_path_ops ); create index events_event_entity_zip_code_date on semantic . events ( event_id desc nulls last , entity_id , zip_code , date desc nulls last ); Next we will create the table for text analytics: create schema text_analysis ; drop table if exists text_analysis . comments ; create table text_analysis . comments as ( with violations as ( select event_id , entity_id , jsonb_array_elements ( violations ) as violations from semantic . events ), cleaned as ( select event_id , entity_id , violations ->> 'comment' as original_comment , replace ( plainto_tsquery ( violations ->> 'comment' ):: text , ' & ' , ' ' ) as cleaned_comment , to_tsvector ( violations ->> 'comment' ) as vectorized_comment from violations where btrim ( violations ->> 'comment' ) <> '' ) select * from cleaned ); And finally the tables for the spatial analysis. The data was downloaded from the Chicago Data Portal . In particular we are using the location of the schools ( Chicago Public Schools - School Locations SY1415 ) and the Chicago ZIP codes boundaries ( Boundaries - ZIP Codes ). Both data sources use the WSG84 projection, by the way. You can check that you have everything setup to upload the data with the following command (I recommend to use ogr2ogr ) ogrinfo -ro PG:'host=0.0.0.0 port=5434 user=your_user password=some_password dbname=food' Again, adjust the connection string accordingly. Then unzip the files that you downloaded, first the boundaries unzip \"Boundaries - ZIP Codes.zip\" (This will create 4 files in your directory, all of them with the prefix geo_export_c0962a58-51c1-4ea4-af11-acb7ed233465 , the extensions will be shp (the spatial data), dbf (the data in tabular form), prj (specifies the projection) and shx (index information) And now the schools: unzip 'Chicago Public Schools - School Locations SY1415.zip' Be careful, now there are 4 new files, all of them with the prefix: geo_export_ebe2869c-9bf6-4a04-ae14-a01062f8fa2a , and the same extensions as before. Finally, ogr2ogr -f \"PostgreSQL\" \\ PG:'host=0.0.0.0 port=8888 user=your_user password=some_password dbname=training' \\ geo_export_c0962a58-51c1-4ea4-af11-acb7ed233465.shp \\ -nln gis.boundaries -nlt PROMOTE_TO_MULTI -lco precision=NO NOTE : I added the -nlt PROMOTE_TO_MULTI because the data source had mixed spatial types (Multipolygon and polygon), see https://trac.osgeo.org/gdal/ticket/4939 for details. NOTE : I added the -lco precision=NO due to a precision error with the data: See this Stackoverflow question for details: https://gis.stackexchange.com/questions/254671/ogr2ogr-error-importing-shapefile-into-postgis-numeric-field-overflow ogr2ogr -f \"PostgreSQL\" \\ PG:'host=0.0.0.0 port=8888 user=your_user password=some_password dbname=training' \\ geo_export_ebe2869c-9bf6-4a04-ae14-a01062f8fa2a.shp \\ -nln gis.public_schools","title":"Creating the database"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/","text":"Introduction to Data Analysis in Python # Background and Motivation # Python is a high-level interpreted general purpose programming language named after a British Comedy Troupe, created by Guido van Rossum (Python's benevolent dictator for life), and maintained by a international group of python enthusiasts. In a python interpreter type import this to read Python's guiding principles. As of the time of this writing (10/2016) python is currently the fifth most popular programming language. It is popular for data science due to being powerful, fast, playing well with others, runs everywhere, easy to learn, highly-readable, open-source, and its fast development time compared to other languages. Because of its general-purpose and ability to call compiled languages like FORTRAN or C it can be used for full-stack development. There is a growing and every improving list of open-source libraries for scientific programming, data manipulation, and data analysis (e.g. NumPy, SciPy, Pandas, Scikit-Learn, Statsmodels Matplotlib, Seaborn, PyTables, etc.) IPython is an enhanced, interactive python interpreter that started as a grad school project by Fernando Perez. The IPython project then evolved into the IPython notebook that would allow users to archive their code, figures, and analysis in a single document, making doing reproducible computational research and sharing said research much easier. The creators of the IPython notebook quickly realized that the \"notebook\" aspects were agnostic to a specific programming language and ported the notebook to other languages, including but not limited to Julia, Python and R. This then led to a rebranding known as the Jupyter Project. The Pandas library, created by Wes McKinney, introduced the R-like dataframe object in Python, making working with data in Python much easier. This tutorial will go over over the basics of Data Analysis in Python using the PyData stack. Getting started with Jupyter Notebooks # To start up a Jupyter notebook server, simply navigate to the directory where you want the notebooks to be saved and run the command jupyter notebook A browser should open with a notebook navigator. Click the \"New\" button and select \"Python 3\". A beautiful blank notebook should open in a new tab Name the notebook by clicking on \"Untitled\" at the top of the page. Notebooks are sequences of cells. Cells can be markdown, code, or raw text. Change the first cell to markdown and briefly describe what you are going to do in the notebook. Running a remote Jupyter Server # You can also run a Jupyter server from a remote machine (e.g, EC2 Instance) and forward the notebook to your local machine. This is preferable if you need to access a large amount of data on a database. Your work will also be saved on a server that is periodically backed-up which will make your work immune to computer-crashes/someone-stealing-your-computer/(some)acts-of-god. Basic port forwarding: ssh -L 44444:localhost:44444 <username>@<instance-name>.dssg.io Now go to the location in the file system you would like to launch a Jupyter server and type jupyter notebook --no-browser --port 44444 You may have to also specifiy the ip address as well jupyter notebook --no-browser --port 44444 --ip=127.0.0.1 You should now be able to go to your browser and go to the address localhost:44444 . Important The port 44444 is an arbitrary port. If another user is running a server with the same port forwarding then you will not be able to run a Jupyter Server using that port. You will then need to switch to another port number. Anything between 30000-70000 should work The notebooks # The problem notebook The complete solution notebook","title":"Python/Pandas"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#introduction-to-data-analysis-in-python","text":"","title":"Introduction to Data Analysis in Python"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#background-and-motivation","text":"Python is a high-level interpreted general purpose programming language named after a British Comedy Troupe, created by Guido van Rossum (Python's benevolent dictator for life), and maintained by a international group of python enthusiasts. In a python interpreter type import this to read Python's guiding principles. As of the time of this writing (10/2016) python is currently the fifth most popular programming language. It is popular for data science due to being powerful, fast, playing well with others, runs everywhere, easy to learn, highly-readable, open-source, and its fast development time compared to other languages. Because of its general-purpose and ability to call compiled languages like FORTRAN or C it can be used for full-stack development. There is a growing and every improving list of open-source libraries for scientific programming, data manipulation, and data analysis (e.g. NumPy, SciPy, Pandas, Scikit-Learn, Statsmodels Matplotlib, Seaborn, PyTables, etc.) IPython is an enhanced, interactive python interpreter that started as a grad school project by Fernando Perez. The IPython project then evolved into the IPython notebook that would allow users to archive their code, figures, and analysis in a single document, making doing reproducible computational research and sharing said research much easier. The creators of the IPython notebook quickly realized that the \"notebook\" aspects were agnostic to a specific programming language and ported the notebook to other languages, including but not limited to Julia, Python and R. This then led to a rebranding known as the Jupyter Project. The Pandas library, created by Wes McKinney, introduced the R-like dataframe object in Python, making working with data in Python much easier. This tutorial will go over over the basics of Data Analysis in Python using the PyData stack.","title":"Background and Motivation"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#getting-started-with-jupyter-notebooks","text":"To start up a Jupyter notebook server, simply navigate to the directory where you want the notebooks to be saved and run the command jupyter notebook A browser should open with a notebook navigator. Click the \"New\" button and select \"Python 3\". A beautiful blank notebook should open in a new tab Name the notebook by clicking on \"Untitled\" at the top of the page. Notebooks are sequences of cells. Cells can be markdown, code, or raw text. Change the first cell to markdown and briefly describe what you are going to do in the notebook.","title":"Getting started with Jupyter Notebooks"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#running-a-remote-jupyter-server","text":"You can also run a Jupyter server from a remote machine (e.g, EC2 Instance) and forward the notebook to your local machine. This is preferable if you need to access a large amount of data on a database. Your work will also be saved on a server that is periodically backed-up which will make your work immune to computer-crashes/someone-stealing-your-computer/(some)acts-of-god. Basic port forwarding: ssh -L 44444:localhost:44444 <username>@<instance-name>.dssg.io Now go to the location in the file system you would like to launch a Jupyter server and type jupyter notebook --no-browser --port 44444 You may have to also specifiy the ip address as well jupyter notebook --no-browser --port 44444 --ip=127.0.0.1 You should now be able to go to your browser and go to the address localhost:44444 . Important The port 44444 is an arbitrary port. If another user is running a server with the same port forwarding then you will not be able to run a Jupyter Server using that port. You will then need to switch to another port number. Anything between 30000-70000 should work","title":"Running a remote Jupyter Server"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#the-notebooks","text":"The problem notebook The complete solution notebook","title":"The notebooks"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/","text":"Spatial analytics: PostGIS Workshop # Slides # Tutorial slides (2016) A little bit of theory # Spatial database # Storage of spatial data Analysis of geographic data Manipulation of spatial objects just like the other database objects Creation of subsets of data Fixing geographic data \\ldots \\ldots Database backend for apps Spatial data # Data which describes or represents either a location or a shape Points, lines, polygons Besides the geometrical properties, the spatial data has attributes. Examples: Geocodable address Crime patterns EMS / patient location Weather information City planning Hazard detection Relationships # Proximity Adjacency (touching, connectivity) Containment Operations # Area Length Intersection Union Buffer Why a db instead of a file? # Spatial data is usually related to other types of data. How load data to the db? # shp2pgsql imports standard esri shapefiles and dbf ogr2ogr imports 20 different vector and flat files The spatial data that is not spatial data # longitude latitude disease date 26.870436 -31.909519 mumps 13/12/2008 26.868682 -31.909259 mumps 24/12/2008 26.867707 -31.910494 mumps 22/01/2009 26.854908 -31.920759 measles 11/01/2009 26.855817 -31.921929 measles 26/01/2009 26.852764 -31.921929 measles 10/02/2009 26.854778 -31.925112 measles 22/02/2009 26.869072 -31.911988 mumps 02/02/2009 (the disease and date columns are the attributes of this data) shape files # Stored in files on the computer The most common one is probably the 'shape file' It consists of at least three different files that work together to store vector data extension description `.shp` the geometry file `.dbf` the attributes file `.shx` index file Vector data # Is stored as a series of x,y coordinate pairs inside the computer's memory. Vector data is used to represent points (1 vertex) , lines (polyline) (2 or more vertices, but the first and the last one are different) and areas (polygons). A vector feature has its shape represented using geometry . The geometry is made up of one or more interconnected vertices. A vertex describes a position in space using an x, y and optionally z axis. The x and y values will depend on the coordinate reference system ( CRS ) being used. Problems with vector data Image from A gentle introduction to gis Sutton T., Dassau O., Sutton M. 2009 \"Image from A gentle introduction to gis Sutton T., Dassau O., Sutton M. 2009 Raster data # Stored as a grid of values Each cell or pixel represents a geographical region, and the value of the pixel represents some attribute of the region Use it when you want to represent a continuous information across an area Multi-band images, each band contains different information Image from A gentle introduction to gis Sutton T., Dassau O., Sutton M. 2009 Problems with raster data High resolution raster data requires a huge amount of computer storage. Exercise: Cleaning and exploring spatial data # Connect to the db host: gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com port: 5432 username: dssg_gis password: dssg-gis db name:gis_tutorial SSH Tunneling ssh -fNT -L \\ 8889:gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com:5432 \\ -i ~/.ssh/your-dssh-key ec2-instance.dssg.io ## ssh tunneling Command line client psql -h localhost -p 8889 -U dssg_gis gis_tutorial Setup # create an schema using your github account (mine is nanounanue ) create schema your - github - username ; Upload the first shapefiles There are several shapefiles in the data directory First, we can see some information from the files ogrinfo -al roads.shp Observe that the projection is ... projcs[\"nad83_massachusetts_mainland\", geogcs[\"gcs_north_american_1983\", datum[\"north_american_datum_1983\", spheroid[\"grs_1980\",6378137,298.257222101]], primem[\"greenwich\",0], unit[\"degree\",0.017453292519943295]], projection[\"lambert_conformal_conic_2sp\"], parameter[\"standard_parallel_1\",42.68333333333333], parameter[\"standard_parallel_2\",41.71666666666667], parameter[\"latitude_of_origin\",41], parameter[\"central_meridian\",-71.5], parameter[\"false_easting\",200000], parameter[\"false_northing\",750000], unit[\"meter\",1]] ... This projection measures the area in meters. but Using shp2psql tool upload the following files: roads , land , hydrology shp2psql --host=localhost --port=8889 --username=dssg_gis \\ -f roads.shp gis your-github-username.roads \\ | psql -h localhost -p 8889 -u dssg_gis gis_tutorial ## if you want to change the projection to wgs 1984 (the one used in google maps) ## you need to add the flag -s 26986:4326 before the name of the database (gis) If you open QGIS you should see something like the following: Figure: land (purple), hydrology (red) and roads (blue) after their insertion in the database and after some customization: Figure: After adjusting the style in QGIS : land (one color per type), hydrology (blue) and roads (yellow) note that we have lands over the roads and over the water . Spatial predicates for cleaning # We will use st_intersects() and st_dwithin() for removing the land which is touch with roads and water, and if it is too far of roads and water, respectively See the file for the sql statements. NOTE: For use of the EXISTS(subquery) look here and here Figure: After removing the land objects which intersects roads or water or where too far from those St_intersects(a,b) returns true if exists at least one point in common between the geometrical objects a and b . St_dwithin(a,b,distance) returns true if the geometries a and b are within the specified distance of one another. Other functions: st_equals , st_disjoint , st_touches , st_crosses , st_overlaps , st_contains . Add more data: buildings and residents # Upload to the database the shapefiles buildings and residents . ## This time I will use ogr2ogr, but this is for demostration purpose only ## It is easier use shp2pgsql ogr2ogr -f \"PostgreSQL\" \\ PG:\"host=localhost user=dssg_gis dbname=gis_tutorial password=dssg-gis port=8889\" \\ buildings.shp -nln your-github-username.buildings Spatial joins: creating new views # As you can see, is not a spatial data. It is a regular psv file. But it contains the pid of the land in which the resident lives. csvhead -d '|' ./data/my_town/residents.psv | head How can I convert this data in spatial data? select r . * -- All the attributes of resident , st_centroid ( l . the_geom ) -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r . pid = l . pid ; Ok, very well. But, How can I see this new \"data\" in QGIS ? You need to create a view create or replace view residents_loc as select row_number () over () as rl_id -- We need an unique identifier , r . * -- All the attributes of resident , st_centroid ( l . the_geom ) as the_geom -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r . pid = l . pid ; Figure: After the creation of the view residents_loc (red star) Spatial operations: Legal issues in our town # How much real state area do we have? select sum ( st_area ( the_geom )) / 1000000 as total_sq_km , st_area ( st_union ( the_geom )) / 1000000 as no_overlap_total_sq_km -- st_union dissolves the overlaps! from land ; Oh, oh. And buildings? select sum ( st_area ( the_geom )) / 1000000 as total_sq_km , st_area ( st_union ( the_geom )) / 1000000 as no_overlap_total_sq_km from buildings ; - We have buildings inside buildings, and some lands overlaps with other lands :( Other operations: st_intersection(a,b) , st_difference(a,b) , st_symdifference(a,b) , st_buffer(c) , st_convexhull(c) Spatial joins: Which lands intersects? # select p . pid -- the land , count ( o . pid ) as total_intersections -- qty of intersections , array_agg ( o . pid ) as intersected_parcels -- the other lands from land as p inner join land as o on ( p . pid <> o . pid and st_intersects ( p . the_geom , o . the_geom )) group by p . pid order by p . pid ; -- First row returned: pid IN ('000000225', '000027745','000092727','000057051') Which type of overlap? select count ( o . pid ) as total_intersections -- Overlaps? , count ( case when st_overlaps ( o . the_geom , p . the_geom ) then 1 else null end ) as o_overlaps_p -- It is the same? , count ( case when st_equals ( o . the_geom , p . the_geom ) then 1 else null end ) as o_equals_p from land as p inner join land as o on ( p . pid <> o . pid and st_intersects ( p . the_geom , o . the_geom )); st_overlaps(a,b) returns true if the geometries share some but not all the points, and the intersection has the same dimension as a , b Cleaning the mess: Reassigning residents # update residents set pid = a . newpid from ( select p . pid , min ( o . pid ) as newpid from land as p inner join land as o on ( p . pid = o . pid or st_equals ( p . the_geom , o . the_geom )) group by p . pid having p . pid <> min ( o . pid )) as a where residents . pid = a . pid returning * -- Return all the updated residents -- so you can see what you just do -- (or you can store it in a another table using CTAS) Cleaning the mess: Deleting the dupe land # -- Add a new column for storing the house types alter table land add column land_type_other varchar []; -- Copy the types to the first parcel update land set land_type_other = a . dupe_types from ( select p . pid , min ( o . pid ) as newpid , array_agg ( distinct o . land_type ) as dupe_types from land as p inner join land as o on ( st_equals ( p . the_geom , o . the_geom )) group by p . pid having count ( p . pid ) > 1 and p . pid = min ( o . pid ) ) as a where land . pid = a . pid returning * ; -- Delete the parcels delete from land where pid in ( select p . pid from land as p inner join land as o on ( st_equals ( p . the_geom , o . the_geom )) group by p . pid having count ( p . pid ) > 1 and p . pid <> min ( o . pid )) ; Spatial analytics: Questions # How many kinds under 12 are further than a km of an elementary school? select sum ( num_children_b12 ) * 100 . 00 / ( select sum ( num_children_b12 ) from residents ) from residents as r inner join land as l on r . pid = l . pid left join ( select pid , the_geom from land where land_type = 'elementary school' or 'elementary school' = any ( land_type_other ) ) as eschools on st_dwithin ( l . the_geom , eschools . the_geom , 1000 ) where eschools . pid is null ; How much area are in empty lands? select st_area ( st_union ( the_geom )) / 1000000 from land where land_type = 'vacant land' ; Which are the 10 nearest houses to the lakes? select h . hyd_name , array ( select bldg_name from buildings b where b . bldg_type like '%family' order by h . the_geom <#> b . the_geom limit 5 ) from hydrology h where h . hyd_name in ( 'lake 1' , 'elephantine youth' ); Note the <#> (bounding box), <-> (centroids) are distance operators, see here and here . Exercise: Mapping civilizations # Intro # Recently this article was published: Spatializing 6,000 years of global urbanization from 3700 BC to AD 2000 Reba, M., Reitsma, F. and Seto, C. , 2016 The article describes all the cities since 3700 BC, including name, population and the position (latitude, longitude). We will use a subset ( chandlerV2 ) of the data for transforming it to a table, and then generating a geojson . Uploading the data # Run the following inside ./data/Historical Urban Population Growth Data cvslook chandlerV2.csv It will fail due some encoding issues iconv -f iso-8859-1 -t utf-8 chandlerV2.csv > chandler_utf8.csv csvsql --db postgresql://dssg_gis:dssg-gis@localhost:8889/gis_tutorial \\ --insert chandlerV2_utf8.csv --table chandler --db-schema nanounanue SQL stuff # select count ( * ) from chandler ; -- How many cities do we have? Let\u2019s create a new table for easier manipulation create table cities as -- CTAS select \"City\" as city , \"Country\" as country , \"Latitude\" as y_lat , \"Longitude\" as x_lon from chandler ; Adding a geometry column and transform to Point alter table cities add column geom geometry ( Point , 4326 ); -- Transforming Lon/Lat to Points update cities set geom = ST_SetSRID ( ST_MakePoint ( x_lon , y_lat ), 4326 ); Converting to GeoJSON \\ copy ( select row_to_json ( fc ) from ( select 'featurecollection' as type , array_to_json ( array_agg ( f )) as features from ( select 'feature' as type , st_asgeojson ( cities . geom ):: json as geometry , row_to_json ( ( select c from ( select city , country ) as c ) ) as properties from cities ) as f ) as fc ) to '~/cities.geojson' ; This type of file could be used with d3.js for making interactive plots. For better performance you could use topojson More resources! # PostGIS: More about spatial queries Exploratory spatial data analysis with Python and PostGIS","title":"GIS"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-analytics-postgis-workshop","text":"","title":"Spatial analytics: PostGIS Workshop"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#slides","text":"Tutorial slides (2016)","title":"Slides"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#a-little-bit-of-theory","text":"","title":"A little bit of theory"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-database","text":"Storage of spatial data Analysis of geographic data Manipulation of spatial objects just like the other database objects Creation of subsets of data Fixing geographic data \\ldots \\ldots Database backend for apps","title":"Spatial database"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-data","text":"Data which describes or represents either a location or a shape Points, lines, polygons Besides the geometrical properties, the spatial data has attributes. Examples: Geocodable address Crime patterns EMS / patient location Weather information City planning Hazard detection","title":"Spatial data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#relationships","text":"Proximity Adjacency (touching, connectivity) Containment","title":"Relationships"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#operations","text":"Area Length Intersection Union Buffer","title":"Operations"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#why-a-db-instead-of-a-file","text":"Spatial data is usually related to other types of data.","title":"Why a db instead of a file?"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#how-load-data-to-the-db","text":"shp2pgsql imports standard esri shapefiles and dbf ogr2ogr imports 20 different vector and flat files","title":"How load data to the db?"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#the-spatial-data-that-is-not-spatial-data","text":"longitude latitude disease date 26.870436 -31.909519 mumps 13/12/2008 26.868682 -31.909259 mumps 24/12/2008 26.867707 -31.910494 mumps 22/01/2009 26.854908 -31.920759 measles 11/01/2009 26.855817 -31.921929 measles 26/01/2009 26.852764 -31.921929 measles 10/02/2009 26.854778 -31.925112 measles 22/02/2009 26.869072 -31.911988 mumps 02/02/2009 (the disease and date columns are the attributes of this data)","title":"The spatial data that is not spatial data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#shape-files","text":"Stored in files on the computer The most common one is probably the 'shape file' It consists of at least three different files that work together to store vector data extension description `.shp` the geometry file `.dbf` the attributes file `.shx` index file","title":"shape files"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#vector-data","text":"Is stored as a series of x,y coordinate pairs inside the computer's memory. Vector data is used to represent points (1 vertex) , lines (polyline) (2 or more vertices, but the first and the last one are different) and areas (polygons). A vector feature has its shape represented using geometry . The geometry is made up of one or more interconnected vertices. A vertex describes a position in space using an x, y and optionally z axis. The x and y values will depend on the coordinate reference system ( CRS ) being used.","title":"Vector data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#raster-data","text":"Stored as a grid of values Each cell or pixel represents a geographical region, and the value of the pixel represents some attribute of the region Use it when you want to represent a continuous information across an area Multi-band images, each band contains different information Image from A gentle introduction to gis Sutton T., Dassau O., Sutton M. 2009","title":"Raster data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#exercise-cleaning-and-exploring-spatial-data","text":"Connect to the db host: gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com port: 5432 username: dssg_gis password: dssg-gis db name:gis_tutorial SSH Tunneling ssh -fNT -L \\ 8889:gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com:5432 \\ -i ~/.ssh/your-dssh-key ec2-instance.dssg.io ## ssh tunneling Command line client psql -h localhost -p 8889 -U dssg_gis gis_tutorial","title":"Exercise: Cleaning and exploring spatial data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#setup","text":"create an schema using your github account (mine is nanounanue ) create schema your - github - username ;","title":"Setup"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-predicates-for-cleaning","text":"We will use st_intersects() and st_dwithin() for removing the land which is touch with roads and water, and if it is too far of roads and water, respectively See the file for the sql statements. NOTE: For use of the EXISTS(subquery) look here and here Figure: After removing the land objects which intersects roads or water or where too far from those St_intersects(a,b) returns true if exists at least one point in common between the geometrical objects a and b . St_dwithin(a,b,distance) returns true if the geometries a and b are within the specified distance of one another. Other functions: st_equals , st_disjoint , st_touches , st_crosses , st_overlaps , st_contains .","title":"Spatial predicates for cleaning"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#add-more-data-buildings-and-residents","text":"Upload to the database the shapefiles buildings and residents . ## This time I will use ogr2ogr, but this is for demostration purpose only ## It is easier use shp2pgsql ogr2ogr -f \"PostgreSQL\" \\ PG:\"host=localhost user=dssg_gis dbname=gis_tutorial password=dssg-gis port=8889\" \\ buildings.shp -nln your-github-username.buildings","title":"Add more data: buildings and residents"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-joins-creating-new-views","text":"As you can see, is not a spatial data. It is a regular psv file. But it contains the pid of the land in which the resident lives. csvhead -d '|' ./data/my_town/residents.psv | head How can I convert this data in spatial data? select r . * -- All the attributes of resident , st_centroid ( l . the_geom ) -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r . pid = l . pid ; Ok, very well. But, How can I see this new \"data\" in QGIS ? You need to create a view create or replace view residents_loc as select row_number () over () as rl_id -- We need an unique identifier , r . * -- All the attributes of resident , st_centroid ( l . the_geom ) as the_geom -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r . pid = l . pid ; Figure: After the creation of the view residents_loc (red star)","title":"Spatial joins: creating new views"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-operations-legal-issues-in-our-town","text":"How much real state area do we have? select sum ( st_area ( the_geom )) / 1000000 as total_sq_km , st_area ( st_union ( the_geom )) / 1000000 as no_overlap_total_sq_km -- st_union dissolves the overlaps! from land ; Oh, oh. And buildings? select sum ( st_area ( the_geom )) / 1000000 as total_sq_km , st_area ( st_union ( the_geom )) / 1000000 as no_overlap_total_sq_km from buildings ; - We have buildings inside buildings, and some lands overlaps with other lands :( Other operations: st_intersection(a,b) , st_difference(a,b) , st_symdifference(a,b) , st_buffer(c) , st_convexhull(c)","title":"Spatial operations: Legal issues in our town"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-joins-which-lands-intersects","text":"select p . pid -- the land , count ( o . pid ) as total_intersections -- qty of intersections , array_agg ( o . pid ) as intersected_parcels -- the other lands from land as p inner join land as o on ( p . pid <> o . pid and st_intersects ( p . the_geom , o . the_geom )) group by p . pid order by p . pid ; -- First row returned: pid IN ('000000225', '000027745','000092727','000057051') Which type of overlap? select count ( o . pid ) as total_intersections -- Overlaps? , count ( case when st_overlaps ( o . the_geom , p . the_geom ) then 1 else null end ) as o_overlaps_p -- It is the same? , count ( case when st_equals ( o . the_geom , p . the_geom ) then 1 else null end ) as o_equals_p from land as p inner join land as o on ( p . pid <> o . pid and st_intersects ( p . the_geom , o . the_geom )); st_overlaps(a,b) returns true if the geometries share some but not all the points, and the intersection has the same dimension as a , b","title":"Spatial joins: Which lands intersects?"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#cleaning-the-mess-reassigning-residents","text":"update residents set pid = a . newpid from ( select p . pid , min ( o . pid ) as newpid from land as p inner join land as o on ( p . pid = o . pid or st_equals ( p . the_geom , o . the_geom )) group by p . pid having p . pid <> min ( o . pid )) as a where residents . pid = a . pid returning * -- Return all the updated residents -- so you can see what you just do -- (or you can store it in a another table using CTAS)","title":"Cleaning the mess: Reassigning residents"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#cleaning-the-mess-deleting-the-dupe-land","text":"-- Add a new column for storing the house types alter table land add column land_type_other varchar []; -- Copy the types to the first parcel update land set land_type_other = a . dupe_types from ( select p . pid , min ( o . pid ) as newpid , array_agg ( distinct o . land_type ) as dupe_types from land as p inner join land as o on ( st_equals ( p . the_geom , o . the_geom )) group by p . pid having count ( p . pid ) > 1 and p . pid = min ( o . pid ) ) as a where land . pid = a . pid returning * ; -- Delete the parcels delete from land where pid in ( select p . pid from land as p inner join land as o on ( st_equals ( p . the_geom , o . the_geom )) group by p . pid having count ( p . pid ) > 1 and p . pid <> min ( o . pid )) ;","title":"Cleaning the mess: Deleting the dupe land"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#spatial-analytics-questions","text":"How many kinds under 12 are further than a km of an elementary school? select sum ( num_children_b12 ) * 100 . 00 / ( select sum ( num_children_b12 ) from residents ) from residents as r inner join land as l on r . pid = l . pid left join ( select pid , the_geom from land where land_type = 'elementary school' or 'elementary school' = any ( land_type_other ) ) as eschools on st_dwithin ( l . the_geom , eschools . the_geom , 1000 ) where eschools . pid is null ; How much area are in empty lands? select st_area ( st_union ( the_geom )) / 1000000 from land where land_type = 'vacant land' ; Which are the 10 nearest houses to the lakes? select h . hyd_name , array ( select bldg_name from buildings b where b . bldg_type like '%family' order by h . the_geom <#> b . the_geom limit 5 ) from hydrology h where h . hyd_name in ( 'lake 1' , 'elephantine youth' ); Note the <#> (bounding box), <-> (centroids) are distance operators, see here and here .","title":"Spatial analytics: Questions"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#exercise-mapping-civilizations","text":"","title":"Exercise: Mapping civilizations"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#intro","text":"Recently this article was published: Spatializing 6,000 years of global urbanization from 3700 BC to AD 2000 Reba, M., Reitsma, F. and Seto, C. , 2016 The article describes all the cities since 3700 BC, including name, population and the position (latitude, longitude). We will use a subset ( chandlerV2 ) of the data for transforming it to a table, and then generating a geojson .","title":"Intro"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#uploading-the-data","text":"Run the following inside ./data/Historical Urban Population Growth Data cvslook chandlerV2.csv It will fail due some encoding issues iconv -f iso-8859-1 -t utf-8 chandlerV2.csv > chandler_utf8.csv csvsql --db postgresql://dssg_gis:dssg-gis@localhost:8889/gis_tutorial \\ --insert chandlerV2_utf8.csv --table chandler --db-schema nanounanue","title":"Uploading the data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#sql-stuff","text":"select count ( * ) from chandler ; -- How many cities do we have? Let\u2019s create a new table for easier manipulation create table cities as -- CTAS select \"City\" as city , \"Country\" as country , \"Latitude\" as y_lat , \"Longitude\" as x_lon from chandler ; Adding a geometry column and transform to Point alter table cities add column geom geometry ( Point , 4326 ); -- Transforming Lon/Lat to Points update cities set geom = ST_SetSRID ( ST_MakePoint ( x_lon , y_lat ), 4326 ); Converting to GeoJSON \\ copy ( select row_to_json ( fc ) from ( select 'featurecollection' as type , array_to_json ( array_agg ( f )) as features from ( select 'feature' as type , st_asgeojson ( cities . geom ):: json as geometry , row_to_json ( ( select c from ( select city , country ) as c ) ) as properties from cities ) as f ) as fc ) to '~/cities.geojson' ; This type of file could be used with d3.js for making interactive plots. For better performance you could use topojson","title":"SQL stuff"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/#more-resources","text":"PostGIS: More about spatial queries Exploratory spatial data analysis with Python and PostGIS","title":"More resources!"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/data/Historical Urban Population Growth Data/","text":"Data from the article: Spatializing 6,000 years of global urbanization from 3700 BC to AD 2000 Located: http://www.nature.com/articles/sdata201634#data-records Last consulted: June 29, 2016","title":"Home"},{"location":"curriculum/2_data_exploration_and_analysis/intro-to-git-and-python/","text":"","title":"Home"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/","text":"Networks # Background and Motivation # Networks are a measurable representation of patterns of relationships connecting entities in an abstract or actual space. The constituent parts of a network are nodes which are connected by ties. Networks have been used to model airplane traffic from airports, supply chains, friendship networks, and even amorphous materials like window glass, cells, and proteins. The importance of networks analysis is that it captures the effect of where and how an individual actor is positioned among others. In this tutorial, we are going to examine the friendship network of 34 students in a karate class. A political rivalry arose in the class and divided the class into two factions, eventually leading to a fissure and two separate karate classes. The club held periodic meetings to vote on policy decisions. When one of the faction heads, individuals 1 and 34, called a meeting, they would communicate the information only to members firmly in their faction, in order to ensure that the majority of members attending the meeting were in their faction, thereby guaranteeing that their policies would pass. Meeting times were not publicly announced, but spread from friend to friend in the social network. In this tutorial we will explore graphical representations of this network, degree metrics, centrality metrics, how to calculate the shortest path between nodes, and community detection. We will be using the NetworkX Python Library developed at Los Alamos National Laboratory (LANL). Installation # To install the virtual environment and dependencies run the following command ./install_network_env.sh && source network-venv/bin/activate","title":"Network"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/#networks","text":"","title":"Networks"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/#background-and-motivation","text":"Networks are a measurable representation of patterns of relationships connecting entities in an abstract or actual space. The constituent parts of a network are nodes which are connected by ties. Networks have been used to model airplane traffic from airports, supply chains, friendship networks, and even amorphous materials like window glass, cells, and proteins. The importance of networks analysis is that it captures the effect of where and how an individual actor is positioned among others. In this tutorial, we are going to examine the friendship network of 34 students in a karate class. A political rivalry arose in the class and divided the class into two factions, eventually leading to a fissure and two separate karate classes. The club held periodic meetings to vote on policy decisions. When one of the faction heads, individuals 1 and 34, called a meeting, they would communicate the information only to members firmly in their faction, in order to ensure that the majority of members attending the meeting were in their faction, thereby guaranteeing that their policies would pass. Meeting times were not publicly announced, but spread from friend to friend in the social network. In this tutorial we will explore graphical representations of this network, degree metrics, centrality metrics, how to calculate the shortest path between nodes, and community detection. We will be using the NetworkX Python Library developed at Los Alamos National Laboratory (LANL).","title":"Background and Motivation"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/#installation","text":"To install the virtual environment and dependencies run the following command ./install_network_env.sh && source network-venv/bin/activate","title":"Installation"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/","text":"Record Linkage # It is often necessary to combine data from multiple sources to get a complete picture of entities we're anslyzing. As data scientists, in addition to just linking data, we are also concerned about issues of missing links, duplicative links, and erroneous links. Record linkage methods range from traditional rule-based and probabilistic approaches, to more modern approaches using machine learning. Background and Motivation # The goal of record linkage is to determine if pairs of records describe the same entity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkages also goes by the terms -- data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora -- in various fields. There are several approaches to record linkage that includes exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not. Resources # The notebook External resources # DSaPP created a webapp for doing matching","title":"Record linkage"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#record-linkage","text":"It is often necessary to combine data from multiple sources to get a complete picture of entities we're anslyzing. As data scientists, in addition to just linking data, we are also concerned about issues of missing links, duplicative links, and erroneous links. Record linkage methods range from traditional rule-based and probabilistic approaches, to more modern approaches using machine learning.","title":"Record Linkage"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#background-and-motivation","text":"The goal of record linkage is to determine if pairs of records describe the same entity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkages also goes by the terms -- data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora -- in various fields. There are several approaches to record linkage that includes exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not.","title":"Background and Motivation"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#resources","text":"The notebook","title":"Resources"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#external-resources","text":"DSaPP created a webapp for doing matching","title":"External resources"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/","text":"Text Analysis # Motivation and Background # This provides an overview of how we can make use of text data using computational data analysis methods. We cover the types of analysis that can be done with text data (search, topic detection, classification, etc.) and give an overview of how to do these analysis, tasks that they\u2019re useful for, and how to evaluate the results. We provide a set of tools that are commonly used for doing text analysis and provide. We often deal with text data that comes from a variety of sources - open ended survey responses, phone call transcriptions, social media data, notes from electronic health records, and news. A challenge we face when dealing with these types of data is how to efficiently analyze it just like we do structured (tabular) data. For example, when analyzing survey responses or electronic health records data, both of which contain narrative text (from the respondents and medical practitioners respectively), the text data often gets ignored or read by the analysts (manually) and used anecdotally. Text analysis techniques described here allow you to use all of the data available (structured and unstructured), and efficiently incorporate large amounts of text data in your analysis. Things you should learn after this: How is text data different than \u201cstructured\u201d data? What types of analysis can be done with text data? Use it by itself Combine it with structured data List the types of analysis and examples How do we do the analysis Processing Pipeline Tokenization Stemming Stopwords Linguistic analysis Turning text into a matrix Term weights TFIDF Analysis (what it is, how to do it, how to evaluate it, and applications/examples in social science) Finding similar documents Finding themes and topics (describe the methods, examples, and evaluation process) Clustering Topic models Classification (describe the methods, examples, and evaluation process) Deep Learning and Word Embeddings Tools Summary Text Analysis is used for summarizing or getting useful information out of a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g., surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible, scaleable, adaptive and cost-effective way. Text Analysis can help with the following tasks: Searches and information retrieval : Help find relevant information in large databases such a systematic literature review. Clustering and text categorization : Techniques like topic modeling modeling can summarize a large corpus of text by finding the most important phrases. Text Summarizing : Create category-sensitive text summaries of a large corpus of text. Machine Translation : Translate from one language to another. Slides # Text analytics Tutorials # Topic modeling: Social services analysis In this tutorial, we are going to analyze social services descriptions using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. DoJobs data analysis In this tutorial, we are going to analyze job advertisements from 2010-2015 using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. First we will go over how to transform our data into a matrix that can be read in by an algorithm. Reddit analysis In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post originated from and also do topic modeling to categorize posts. Data # The data for the first two tutorials is located in data The data for the Reddit tutorial can be downloaded . To unzip the data, run gunzip ./data/RC_2015-05.json.gz Further Resources # Natural Language Processing with Python Getting Started with NLP","title":"Text"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#text-analysis","text":"","title":"Text Analysis"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#motivation-and-background","text":"This provides an overview of how we can make use of text data using computational data analysis methods. We cover the types of analysis that can be done with text data (search, topic detection, classification, etc.) and give an overview of how to do these analysis, tasks that they\u2019re useful for, and how to evaluate the results. We provide a set of tools that are commonly used for doing text analysis and provide. We often deal with text data that comes from a variety of sources - open ended survey responses, phone call transcriptions, social media data, notes from electronic health records, and news. A challenge we face when dealing with these types of data is how to efficiently analyze it just like we do structured (tabular) data. For example, when analyzing survey responses or electronic health records data, both of which contain narrative text (from the respondents and medical practitioners respectively), the text data often gets ignored or read by the analysts (manually) and used anecdotally. Text analysis techniques described here allow you to use all of the data available (structured and unstructured), and efficiently incorporate large amounts of text data in your analysis. Things you should learn after this: How is text data different than \u201cstructured\u201d data? What types of analysis can be done with text data? Use it by itself Combine it with structured data List the types of analysis and examples How do we do the analysis Processing Pipeline Tokenization Stemming Stopwords Linguistic analysis Turning text into a matrix Term weights TFIDF Analysis (what it is, how to do it, how to evaluate it, and applications/examples in social science) Finding similar documents Finding themes and topics (describe the methods, examples, and evaluation process) Clustering Topic models Classification (describe the methods, examples, and evaluation process) Deep Learning and Word Embeddings Tools Summary Text Analysis is used for summarizing or getting useful information out of a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g., surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible, scaleable, adaptive and cost-effective way. Text Analysis can help with the following tasks: Searches and information retrieval : Help find relevant information in large databases such a systematic literature review. Clustering and text categorization : Techniques like topic modeling modeling can summarize a large corpus of text by finding the most important phrases. Text Summarizing : Create category-sensitive text summaries of a large corpus of text. Machine Translation : Translate from one language to another.","title":"Motivation and Background"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#slides","text":"Text analytics","title":"Slides"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#tutorials","text":"Topic modeling: Social services analysis In this tutorial, we are going to analyze social services descriptions using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. DoJobs data analysis In this tutorial, we are going to analyze job advertisements from 2010-2015 using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. First we will go over how to transform our data into a matrix that can be read in by an algorithm. Reddit analysis In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post originated from and also do topic modeling to categorize posts.","title":"Tutorials"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#data","text":"The data for the first two tutorials is located in data The data for the Reddit tutorial can be downloaded . To unzip the data, run gunzip ./data/RC_2015-05.json.gz","title":"Data"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#further-resources","text":"Natural Language Processing with Python Getting Started with NLP","title":"Further Resources"},{"location":"curriculum/3_modeling_and_machine_learning/","text":"Modeling and Machine Learning # Let's make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating your models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through.","title":"Intro"},{"location":"curriculum/3_modeling_and_machine_learning/#modeling-and-machine-learning","text":"Let's make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating your models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through.","title":"Modeling and Machine Learning"},{"location":"curriculum/3_modeling_and_machine_learning/causal-inference/","text":"","title":"Home"},{"location":"curriculum/3_modeling_and_machine_learning/machine-learning/","text":"Machine Learning # Motivation # You've probably heard about machine learning, artificial intelligence, etc. It's an integral part of data science. So we'll teach you the basics about ML: the process, formulating a problem in a machine learning setting, feature generation, model selection and evaluation, and how to interpret results before eventually deploying the models. For a more detailed coverage of machine learning for social good and policy, you can take a look at lectures for the Machine Learning and Public Policy class . We have a few overview topics in this guide: Intro to Machine Learning Slides Machine Learning Book Chapter from Big Data and Social Science Jupyer Notebook going over building ML models Ethics, Bias, Fairness in ML","title":"Machine Learning"},{"location":"curriculum/3_modeling_and_machine_learning/machine-learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"curriculum/3_modeling_and_machine_learning/machine-learning/#motivation","text":"You've probably heard about machine learning, artificial intelligence, etc. It's an integral part of data science. So we'll teach you the basics about ML: the process, formulating a problem in a machine learning setting, feature generation, model selection and evaluation, and how to interpret results before eventually deploying the models. For a more detailed coverage of machine learning for social good and policy, you can take a look at lectures for the Machine Learning and Public Policy class . We have a few overview topics in this guide: Intro to Machine Learning Slides Machine Learning Book Chapter from Big Data and Social Science Jupyer Notebook going over building ML models Ethics, Bias, Fairness in ML","title":"Motivation"},{"location":"curriculum/3_modeling_and_machine_learning/operations-research/","text":"","title":"Home"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/","text":"Pipelining frameworks and Workflow management # When working on a machine learning problem, often a lot of time is invested in basic steps like loading and saving data, keeping track of parameters, and organising the code to run the steps in the right order. Machine learning pipelines can become complex as time goes on and are often used by multiple people. Workflow management tools are made to help reduce the effort required for these steps. The most commonly used pipelining frameworks are Airflow - developed by AirBnB. Open-sourced in 2015. Luigi - developed by Spotify. Open-sourced in 2011 QuantumBlack recently open-sourced their framework called Kedro . Workflows are DAGs # Tasks performed in a machine learning project are depended on each other, but should not be circular. In other words, they form a directed acyclic graph - a DAG. Worflow management systems (WMS) take advantage of this fact and optimise the execution of each step in the graph. For example, Airflow interprets each task as a node in a graph and the dependency between tasks as edges. Tasks could be - Transformations / operations on data sets - Sensors that check for the state of a process or a data structure Luigi and Kedro see nodes as data sets and edges are the functions that transform parent data sets into a child data set. Luigi calls them Tasks and Targets. Code example # To see in in action, we'll look at a simple code example on Towards Data Science . Visualisations # Airflow offers a powerful UI where the user can monitor DAGs and task execution and can directly interact with them through the web UI. Luigi and Kedro offer a very minimal UI. There is no user interaction with running processes. References # Most of the content here is taken from Towards Data Science .","title":"Pipelining frameworks and Workflow management"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/#pipelining-frameworks-and-workflow-management","text":"When working on a machine learning problem, often a lot of time is invested in basic steps like loading and saving data, keeping track of parameters, and organising the code to run the steps in the right order. Machine learning pipelines can become complex as time goes on and are often used by multiple people. Workflow management tools are made to help reduce the effort required for these steps. The most commonly used pipelining frameworks are Airflow - developed by AirBnB. Open-sourced in 2015. Luigi - developed by Spotify. Open-sourced in 2011 QuantumBlack recently open-sourced their framework called Kedro .","title":"Pipelining frameworks and Workflow management"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/#workflows-are-dags","text":"Tasks performed in a machine learning project are depended on each other, but should not be circular. In other words, they form a directed acyclic graph - a DAG. Worflow management systems (WMS) take advantage of this fact and optimise the execution of each step in the graph. For example, Airflow interprets each task as a node in a graph and the dependency between tasks as edges. Tasks could be - Transformations / operations on data sets - Sensors that check for the state of a process or a data structure Luigi and Kedro see nodes as data sets and edges are the functions that transform parent data sets into a child data set. Luigi calls them Tasks and Targets.","title":"Workflows are DAGs"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/#code-example","text":"To see in in action, we'll look at a simple code example on Towards Data Science .","title":"Code example"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/#visualisations","text":"Airflow offers a powerful UI where the user can monitor DAGs and task execution and can directly interact with them through the web UI. Luigi and Kedro offer a very minimal UI. There is no user interaction with running processes.","title":"Visualisations"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/#references","text":"Most of the content here is taken from Towards Data Science .","title":"References"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/","text":"\"We're All Social Scientists Now\" # An Introduction to Quantitative Social Science # In this session, we learn some of the fundamental differences between machine learning and quantitative social sciences. We also identify some useful datasets and lessons learned for DSSG projects. Potential Teachouts # Experiments Quasi-experiments Observational studies Slides # You can access the slides for this session here . Additional Reading # \"We're All Social Scientists Now\" \"Causal Inference in Statistics: An Overview\" \"Causal Inference in Social Science: An Elementary Introduction\" \"Fuck Nuance\" Counterfactuals and Causal Inference Mostly Harmless Econometrics Prediction Policy Problems","title":"\"We're All Social Scientists Now\""},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#were-all-social-scientists-now","text":"","title":"\"We're All Social Scientists Now\""},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#an-introduction-to-quantitative-social-science","text":"In this session, we learn some of the fundamental differences between machine learning and quantitative social sciences. We also identify some useful datasets and lessons learned for DSSG projects.","title":"An Introduction to Quantitative Social Science"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#potential-teachouts","text":"Experiments Quasi-experiments Observational studies","title":"Potential Teachouts"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#slides","text":"You can access the slides for this session here .","title":"Slides"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#additional-reading","text":"\"We're All Social Scientists Now\" \"Causal Inference in Statistics: An Overview\" \"Causal Inference in Social Science: An Elementary Introduction\" \"Fuck Nuance\" Counterfactuals and Causal Inference Mostly Harmless Econometrics Prediction Policy Problems","title":"Additional Reading"},{"location":"curriculum/communication/","text":"Presentations and Communications # We need to remember that it's very difficult to have a social impact through doing data science for social good work if you cannot convince organizations to work with you and tackle a problem, if no one understands what you did, and if they can't be convinced to use what you've done. We need to be able to communicate effectively abouthe work and spend a lot of time at DSSG training fellows on how to do that. Here are some resources that you'll find useful: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable.","title":"Intro"},{"location":"curriculum/communication/#presentations-and-communications","text":"We need to remember that it's very difficult to have a social impact through doing data science for social good work if you cannot convince organizations to work with you and tackle a problem, if no one understands what you did, and if they can't be convinced to use what you've done. We need to be able to communicate effectively abouthe work and spend a lot of time at DSSG training fellows on how to do that. Here are some resources that you'll find useful: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable.","title":"Presentations and Communications"},{"location":"curriculum/communication/presentations/","text":"","title":"Presentations"},{"location":"curriculum/communication/user_interface/","text":"Usability and User Interface Design # Motivation # When you're working on data science for social good projects, you'll usually be partnering with (or working within) an organization, and there will be many stakeholders, or people who influence or will be influenced by your work. These stakeholders will be both within and outside the organization, including you, whoever is funding the work, the people who create and collect the data, and the people who are actually affected by your analysis and the decisions it drives. To make sure that your work is creating a useful and usable solution to a real problem (and not just wasting your and everyone else's time), we borrow some ideas from the school of agile development. Concepts # Agile development: iterative development, user stories Wireframes Resources # Slides Suggested reading: The Design of Everyday Things by Donald Norman, The Lean Startup by Eric Ries, User Stories Applied for Agile Software Development by Mike Cohn","title":"User interface"},{"location":"curriculum/communication/user_interface/#usability-and-user-interface-design","text":"","title":"Usability and User Interface Design"},{"location":"curriculum/communication/user_interface/#motivation","text":"When you're working on data science for social good projects, you'll usually be partnering with (or working within) an organization, and there will be many stakeholders, or people who influence or will be influenced by your work. These stakeholders will be both within and outside the organization, including you, whoever is funding the work, the people who create and collect the data, and the people who are actually affected by your analysis and the decisions it drives. To make sure that your work is creating a useful and usable solution to a real problem (and not just wasting your and everyone else's time), we borrow some ideas from the school of agile development.","title":"Motivation"},{"location":"curriculum/communication/user_interface/#concepts","text":"Agile development: iterative development, user stories Wireframes","title":"Concepts"},{"location":"curriculum/communication/user_interface/#resources","text":"Slides Suggested reading: The Design of Everyday Things by Donald Norman, The Lean Startup by Eric Ries, User Stories Applied for Agile Software Development by Mike Cohn","title":"Resources"},{"location":"curriculum/communication/writing/","text":"","title":"Writing reports"},{"location":"curriculum/dbs/getting_data_in/","text":"Getting data from CSVs in to a Database (Whatever you do , do not use pandas for this!!!)","title":"Getting data in"},{"location":"curriculum/dbs/getting_data_out/","text":"","title":"Getting data out"},{"location":"curriculum/dbs/other_types/","text":"","title":"Other types of DBs"},{"location":"curriculum/dbs/relational_design/","text":"","title":"Designing a DB"},{"location":"curriculum/dbs/why/","text":"Databases 101 # Background and Motivation # In the case of small data (you can load it all into memory), simple analysis (maps well to your statistical package of choice), and no plans for you or anyone else to repeat your analysis (nor receive updated data), then keeping your data in text files, and using a scripting language like Python or R to work with it, is fine. In the case that you have a large amount of diverse data (cannot all be loaded into memory) that may be updated, or if you want to share your data with others and let others easily reproduce your analysis, then use a DBMS ( Database Management System ). DBMS are important for storing, organizing, managing and analyzing data. They mitigate the scaling and complexity problem of increasing data in volume and diversity. DBMS facilitate a data model that allows data to be stored, queried, and updated efficiently and concurrently by multiple users. In general, as a data scientist your toolkit will involve using SQL (Structured Query Language) with a database and something else-- python , R , SAS, Stata, SPSS. This tutorial covers the basics of relational databases and NoSQL databases, the pros and cons of each type of database, and when to use which one. Materials # Slides","title":"Why a DB?"},{"location":"curriculum/dbs/why/#databases-101","text":"","title":"Databases 101"},{"location":"curriculum/dbs/why/#background-and-motivation","text":"In the case of small data (you can load it all into memory), simple analysis (maps well to your statistical package of choice), and no plans for you or anyone else to repeat your analysis (nor receive updated data), then keeping your data in text files, and using a scripting language like Python or R to work with it, is fine. In the case that you have a large amount of diverse data (cannot all be loaded into memory) that may be updated, or if you want to share your data with others and let others easily reproduce your analysis, then use a DBMS ( Database Management System ). DBMS are important for storing, organizing, managing and analyzing data. They mitigate the scaling and complexity problem of increasing data in volume and diversity. DBMS facilitate a data model that allows data to be stored, queried, and updated efficiently and concurrently by multiple users. In general, as a data scientist your toolkit will involve using SQL (Structured Query Language) with a database and something else-- python , R , SAS, Stata, SPSS. This tutorial covers the basics of relational databases and NoSQL databases, the pros and cons of each type of database, and when to use which one.","title":"Background and Motivation"},{"location":"curriculum/dbs/why/#materials","text":"Slides","title":"Materials"},{"location":"curriculum/deployment/advanced_pipelines/","text":"","title":"Advanced pipelines"},{"location":"curriculum/deployment/how_to/","text":"","title":"How to deploy"},{"location":"curriculum/deployment/monitor/","text":"Model Monitoring presentation Model Monitoring Repo","title":"Monitor"},{"location":"curriculum/deployment/update/","text":"","title":"Update"},{"location":"curriculum/eda/clustering/","text":"","title":"ML as a data exploration tool (Clustering)"},{"location":"curriculum/eda/data_stories/","text":"","title":"Data stories concept and code"},{"location":"curriculum/eda/tableau/","text":"","title":"Tableau"},{"location":"curriculum/eda/visualization/","text":"Useful jupyter notebooks: Data Visualization in Python Spatial Visualization","title":"Visualization"},{"location":"curriculum/experimental_design/case_studies/","text":"","title":"Case studies"},{"location":"curriculum/experimental_design/intro/","text":"","title":"Experiment design"},{"location":"curriculum/get_data/flat_files/","text":"","title":"Flat files"},{"location":"curriculum/get_data/images/","text":"","title":"Working with images"},{"location":"curriculum/get_data/scraping/","text":"","title":"Scraping"},{"location":"curriculum/get_data/security/","text":"","title":"Security"},{"location":"curriculum/get_data/text/","text":"","title":"Text"},{"location":"curriculum/get_data/data-security-primer/","text":"Data Security Primer # Why this is important # We have a lot of sensitive information Much of it is private data about individuals Legal agreements in place with partners to keep data safe Security 101 # No such thing as absolute security Consider your home Can a dedicated attacker break in to your home? Do you lock your door? Goal: Reduce risk of disclosure What We Care About # Confidentiality of project data Login credentials to the servers and databases (and places where these credentials are stored) Common DSSG Challenges # Avoid: Committing database credentials, API keys, SSH keys, etc. to Github repos Maintain awareness: IPython notebooks with exploratory data analysis with confidential data in them (talk with your team about this) Commit with Confidence! # Use git add filename to stage files individually Before you commit, git diff --cached to verify what you have staged is what you expect If you have files that you want to make sure that you do not commit, add them to your [.gitignore]{.title-ref} Authentication # Use unique, strong passwords Use a password manager e.g. KeePass, LastPass, 1Password Use two factor authentication when available (e.g. on Github) Database: Don't # Don't commit the following: from sqlalchemy import create_engine engine = create_engine ( 'postgresql://dbpro:ayylmao@dssg.example.com:5432/mydatabase' ) Database: Do # Store these credentials in a separate file dbcreds.py : host = 'dssg.example.com' user = 'dbpro' database = 'mydatabase' password = 'ayylmao' Add this file to your .gitignore to ensure that you don't commit it You can commit an example file to your repo dbcreds.example : host = '' user = '' database = '' password = '' Database: Do # import dbcreds engine = sqlalchemy . create_engine (( 'postgresql://{conf.user}:' '{conf.password}@{conf.host}:5432/{conf.database}' ) . format ( conf = dbcreds )) Database: Do # Commit an even simpler config file `dbcreds.py`: config = { 'sqlalchemy.url' : 'postgres://dbpro:ayylmao@dssg.example.com/mydatabase' } And then connect: import sqlalchemy from dbcreds import config engine = sqlalchemy . engine_from_config ( config ) Beyond Content # Consider whether your project partner would want the names of tables disclosed Example: https://github.com/dssg/police-eis/blob/master/example_officer_config.yaml Cleaning Repos # Search for passwords/data leaks in a folder: https://github.com/dssg/repo-scraper Instead of git-filter-branch to remove secret things from your git repository: https://github.com/rtyley/bfg-repo-cleaner Mistakes Happen # Avoid cleaning by not putting sensitive data in your repos Web Applications # If you end up creating a web application, be aware of security best practices: OWASP Secure Coding Practices: https://www.owasp.org/images/0/08/OWASP_SCP_Quick_Reference_Guide_v2.pdf","title":"Data security"},{"location":"curriculum/get_data/data-security-primer/#data-security-primer","text":"","title":"Data Security Primer"},{"location":"curriculum/get_data/data-security-primer/#why-this-is-important","text":"We have a lot of sensitive information Much of it is private data about individuals Legal agreements in place with partners to keep data safe","title":"Why this is important"},{"location":"curriculum/get_data/data-security-primer/#security-101","text":"No such thing as absolute security Consider your home Can a dedicated attacker break in to your home? Do you lock your door? Goal: Reduce risk of disclosure","title":"Security 101"},{"location":"curriculum/get_data/data-security-primer/#what-we-care-about","text":"Confidentiality of project data Login credentials to the servers and databases (and places where these credentials are stored)","title":"What We Care About"},{"location":"curriculum/get_data/data-security-primer/#common-dssg-challenges","text":"Avoid: Committing database credentials, API keys, SSH keys, etc. to Github repos Maintain awareness: IPython notebooks with exploratory data analysis with confidential data in them (talk with your team about this)","title":"Common DSSG Challenges"},{"location":"curriculum/get_data/data-security-primer/#commit-with-confidence","text":"Use git add filename to stage files individually Before you commit, git diff --cached to verify what you have staged is what you expect If you have files that you want to make sure that you do not commit, add them to your [.gitignore]{.title-ref}","title":"Commit with Confidence!"},{"location":"curriculum/get_data/data-security-primer/#authentication","text":"Use unique, strong passwords Use a password manager e.g. KeePass, LastPass, 1Password Use two factor authentication when available (e.g. on Github)","title":"Authentication"},{"location":"curriculum/get_data/data-security-primer/#database-dont","text":"Don't commit the following: from sqlalchemy import create_engine engine = create_engine ( 'postgresql://dbpro:ayylmao@dssg.example.com:5432/mydatabase' )","title":"Database: Don't"},{"location":"curriculum/get_data/data-security-primer/#database-do","text":"Store these credentials in a separate file dbcreds.py : host = 'dssg.example.com' user = 'dbpro' database = 'mydatabase' password = 'ayylmao' Add this file to your .gitignore to ensure that you don't commit it You can commit an example file to your repo dbcreds.example : host = '' user = '' database = '' password = ''","title":"Database: Do"},{"location":"curriculum/get_data/data-security-primer/#database-do_1","text":"import dbcreds engine = sqlalchemy . create_engine (( 'postgresql://{conf.user}:' '{conf.password}@{conf.host}:5432/{conf.database}' ) . format ( conf = dbcreds ))","title":"Database: Do"},{"location":"curriculum/get_data/data-security-primer/#database-do_2","text":"Commit an even simpler config file `dbcreds.py`: config = { 'sqlalchemy.url' : 'postgres://dbpro:ayylmao@dssg.example.com/mydatabase' } And then connect: import sqlalchemy from dbcreds import config engine = sqlalchemy . engine_from_config ( config )","title":"Database: Do"},{"location":"curriculum/get_data/data-security-primer/#beyond-content","text":"Consider whether your project partner would want the names of tables disclosed Example: https://github.com/dssg/police-eis/blob/master/example_officer_config.yaml","title":"Beyond Content"},{"location":"curriculum/get_data/data-security-primer/#cleaning-repos","text":"Search for passwords/data leaks in a folder: https://github.com/dssg/repo-scraper Instead of git-filter-branch to remove secret things from your git repository: https://github.com/rtyley/bfg-repo-cleaner","title":"Cleaning Repos"},{"location":"curriculum/get_data/data-security-primer/#mistakes-happen","text":"Avoid cleaning by not putting sensitive data in your repos","title":"Mistakes Happen"},{"location":"curriculum/get_data/data-security-primer/#web-applications","text":"If you end up creating a web application, be aware of security best practices: OWASP Secure Coding Practices: https://www.owasp.org/images/0/08/OWASP_SCP_Quick_Reference_Guide_v2.pdf","title":"Web Applications"},{"location":"curriculum/link_data/record_linkage/","text":"Record Linkage # It is often necessary to combine data from multiple sources to get a complete picture of entities we're analyzing. As data scientists, in addition to just linking data, we are also concerned about issues of missing links, duplicative links, and erroneous links. Record linkage methods range from traditional rule-based and probabilistic approaches, to more modern approaches using machine learning. Background and Motivation # The goal of record linkage is to determine if pairs of records describe the same entity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkages also goes by the terms -- data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora -- in various fields. There are several approaches to record linkage that includes exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not. Resources # The notebook External resources # DSaPP created a webapp for doing matching","title":"Record Linkage"},{"location":"curriculum/link_data/record_linkage/#record-linkage","text":"It is often necessary to combine data from multiple sources to get a complete picture of entities we're analyzing. As data scientists, in addition to just linking data, we are also concerned about issues of missing links, duplicative links, and erroneous links. Record linkage methods range from traditional rule-based and probabilistic approaches, to more modern approaches using machine learning.","title":"Record Linkage"},{"location":"curriculum/link_data/record_linkage/#background-and-motivation","text":"The goal of record linkage is to determine if pairs of records describe the same entity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkages also goes by the terms -- data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora -- in various fields. There are several approaches to record linkage that includes exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not.","title":"Background and Motivation"},{"location":"curriculum/link_data/record_linkage/#resources","text":"The notebook","title":"Resources"},{"location":"curriculum/link_data/record_linkage/#external-resources","text":"DSaPP created a webapp for doing matching","title":"External resources"},{"location":"curriculum/methods/causal_inference/","text":"Causal Inference Methods # Many problems and questions in social good and public policy require causal inferencew methods to be used. This often involve questions around effectiveness of interventions in most domains that we will deal with. The methods discussed here will cover two broad areas: 1. Experimental Methods 2. Causal inference methods using observational data. See two versions of tutorials we've done on this: - DSSG 2016 - DSSG 2018","title":"Causal inference methods"},{"location":"curriculum/methods/causal_inference/#causal-inference-methods","text":"Many problems and questions in social good and public policy require causal inferencew methods to be used. This often involve questions around effectiveness of interventions in most domains that we will deal with. The methods discussed here will cover two broad areas: 1. Experimental Methods 2. Causal inference methods using observational data. See two versions of tutorials we've done on this: - DSSG 2016 - DSSG 2018","title":"Causal Inference Methods"},{"location":"curriculum/methods/or/","text":"","title":"OR/optimization methods"},{"location":"curriculum/methods/social_science/","text":"","title":"Social science methods"},{"location":"curriculum/methods/statistical_analysis/","text":"","title":"Other statistical analysis methods"},{"location":"curriculum/ml/checklist/","text":"","title":"ML Checklist"},{"location":"curriculum/ml/methods/","text":"","title":"Machine learning methods"},{"location":"curriculum/ml/pipeline/","text":"Machine Learning Pipeline Take a look at Triage , the pipeline we use for most of our projects. Dirtyduck : A tutorial for triage.","title":"ML pipeline I"},{"location":"curriculum/ml/pipeline_ii/","text":"","title":"Pipeline ii"},{"location":"curriculum/ml/problem_formulation/","text":"","title":"ML problem formulation"},{"location":"curriculum/ml/templates/","text":"","title":"Templates of policy problems"},{"location":"curriculum/ml/tips/","text":"","title":"Practical tips on how to use them, parameters, etc."},{"location":"curriculum/ml/feature_engineering/case_studies/","text":"","title":"Case studies"},{"location":"curriculum/ml/feature_engineering/intro/","text":"Overview of Feature Generation/Engineering","title":"Feature engineering"},{"location":"curriculum/ml/feature_engineering/workshop/","text":"","title":"Workshop on feature engineering"},{"location":"curriculum/ml/labels/implications/","text":"","title":"Implications of a label"},{"location":"curriculum/ml/labels/one_or_many/","text":"","title":"One or many"},{"location":"curriculum/ml/metrics/examples/","text":"","title":"Examples"},{"location":"curriculum/ml/metrics/overview/","text":"","title":"Overview"},{"location":"curriculum/ml/postmodeling/bias_analysis/","text":"","title":"Bias analysis"},{"location":"curriculum/ml/postmodeling/error_analysis/","text":"","title":"Error analysis"},{"location":"curriculum/ml/postmodeling/feature_importance/","text":"","title":"Feature importance"},{"location":"curriculum/ml/postmodeling/list_comparison/","text":"","title":"Comparing lists"},{"location":"curriculum/ml/postmodeling/model_comparison/","text":"","title":"Comparing different models"},{"location":"curriculum/ml/postmodeling/understanding/","text":"","title":"Model understanding"},{"location":"curriculum/ml/selection/audition/","text":"","title":"Audition"},{"location":"curriculum/ml/selection/bias/","text":"","title":"Bias"},{"location":"curriculum/ml/selection/interpretability/","text":"","title":"Interpretability"},{"location":"curriculum/ml/selection/performance/","text":"","title":"Performance"},{"location":"curriculum/ml/selection/stability/","text":"","title":"Stability"},{"location":"curriculum/ml/validation/field_trials/","text":"","title":"Field trials"},{"location":"curriculum/ml/validation/kfold/","text":"","title":"K-fold cross-validation"},{"location":"curriculum/ml/validation/process_and_goal/","text":"","title":"Process and goal"},{"location":"curriculum/ml/validation/tcc/","text":"Temporal Cross Validation # Resources # Temporal Cross Validation - Erika Salomon","title":"Temporal cross-validation"},{"location":"curriculum/ml/validation/tcc/#temporal-cross-validation","text":"","title":"Temporal Cross Validation"},{"location":"curriculum/ml/validation/tcc/#resources","text":"Temporal Cross Validation - Erika Salomon","title":"Resources"},{"location":"curriculum/programming_best_practices/","text":"Programming Best Practices # As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in.","title":"Intro"},{"location":"curriculum/programming_best_practices/#programming-best-practices","text":"As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in.","title":"Programming Best Practices"},{"location":"curriculum/programming_best_practices/legible-good-code/","text":"Best Practices: Writing Legible, Good Code # Motivation # All fellows will have to write code that is usable and understandable by their peers and partners, and potentially other 3rd parties. What does that entail in practice? Many fellows are coming from academic backgrounds, have self-taught programming skills, and have never worked collaboratively on a software project. We want to help them establish good habits and avoid common mistakes. (Adapted from Tutorial by Kevin Wilson, 2016 DSSG Technical Mentor) For whom do we write code? # What you write (for people). # def fib ( n ): \"\"\" :param int n: The Fibonnaci index you want to return :return: The nth Fibonnaci number :rtype: int \"\"\" if n < 2 : return 1 else : return fib ( n - 1 ) + fib ( n - 2 ) Here is an example of a function. Why write this function? Well, literally, you give the function an integer n , and the function gives you back the n th Fibonnaci number. Writing code allows you to have the computer calculate the n th Fibonnaci number, which it can probably do much faster than you can, especially as n gets larger and larger. It also allows your human comrades to see for themselves how you rattle off arbitrary Fibonacci numbers so fast. But you don't really write code for the computer... What the computer sees (assembly language for the processor). # ... because if you did, it would look like this. 2 0 LOAD_FAST 0 (n) 3 LOAD_CONST 1 (2) 6 COMPARE_OP 0 (<) 9 POP_JUMP_IF_FALSE 16 3 12 LOAD_CONST 2 (1) 15 RETURN_VALUE 5 >> 16 LOAD_GLOBAL 0 (fib) 19 LOAD_FAST 0 (n) 22 LOAD_CONST 2 (1) 25 BINARY_SUBTRACT 26 CALL_FUNCTION 1 29 LOAD_GLOBAL 0 (fib) 32 LOAD_FAST 0 (n) 35 LOAD_CONST 1 (2) 38 BINARY_SUBTRACT 39 CALL_FUNCTION 1 42 BINARY_ADD 43 RETURN_VALUE 44 LOAD_CONST 0 (None) 47 RETURN_VALUE Seriously, the code is for you ... # ...one month from now when you've completely forgotten what the heck alpha was... ...or tomorrow when your teammate wonders why all those 3 s are in the database.... ...or when your system is on fire at 2 AM (the witching hour of production systems), you're tired, and your customers are calling wondering what the heck is going on over there (why are they awake, anyway?)... ...or when I'm doing code review for you... There is only one law of good coding: Code is for humans, not for computers . # This law has 4 consequences: Give things informative names. Document inputs and outputs. Don't repeat yourself. Reduce cognitive load: Use PEP-8. Give Things Informative Names # What does the function below do? import math def doit ( x ): output = [] y = 2 while x != 1 : if x % y == 0 : output . append ( y ) x //= y else : y += 1 continue return output def magic ( input ): filter = set () return sorted ( x for x in y for y in input if ( len ( x ) < 20 and x not in filter ) or filter . add ( x )) def pretty_pictures (): df = pd . read_csv ( '2016.csv.gz' , names = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ]) first = df [ weather_df . STATION == 'US1ILCK0010' ] first [ first . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == 'US1ILCK0014' ] second [ second . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return first [ first . TYPE == 'PRCP' ] . mean (), second [ second . TYPE == 'PRCP' ] . mean () Names should have meaning to the intended readers, e.g., You a week from now Your teammates tomorrow Your future teammates who have to deal with your code [ i , j , k ] for iterators are OK, alpha with a reference to a specific paper is not kwargs are a great place to name things CONSTANT_VALUES are too Do not fear long names; you have autocomplete. The TAB key is your friend. Document Inputs and Outputs # import math def factor ( x ): output = [] fact = 2 while x != 1 : if x % fact == 0 : output . append ( y ) x //= fact else : fact += 1 continue return output def unique_flatten ( the_input , max_length = 20 ): flattened_set = { val for val in row for row in the_input } filtered_list = [ val for val in flattened_set if len ( val ) < max_length ] filtered_list . sort () return filtered_list WEATHER_HEADERS = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ] PRECIPITATION_TYPE = 'PRCP' CHICAGO_STATION_NAMES = [ 'US1ILCK0010' , 'US1ILCK0014' ] def precipitation_in_chicago (): df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES ] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ] . mean (), second [ second . TYPE == PRECIPITATION_TYPE ] . mean ()) Every function should have a docstring The docstring should: Describe the function briefly Explicitly document the inputs with :param type name: description Explicitly document the return value with :returns: description Explicitly document the return type with :rtype: Note this follows the Sphinx/RST syntax guide . You can also follow the Numpy Format . Just be consistent. Don't Repeat Yourself # The WET (Write Everything Twice) Way # def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = {} for val in left : if val not in left_counts : left_counts [ val ] = 1 else : left_counts [ val ] += 1 right_counts = {} for val in right : if val not in right_counts : right_counts [ val ] = 1 else : right_counts [ val ] += 1 left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts In this case, the value counting logic is redundant and can be abstracted away into a function. The DRY Way: Use a function to avoid redundancy # def value_counts ( the_list ): \"\"\" :param list[object] the_list: The list whose values we'll count :rtype: dict[object, int] :return: A dict from the value to its count \"\"\" output = {} for val in the_list : if val not in output : output [ val ] = 1 else : output [ val ] += 1 return output def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = value_counts ( left ) right_counts = value_counts ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts Better... But, there is no need to reinvent the wheel with the value_count function if it has already been implemented for you. The DRY-er Way # from collections import Counter def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = Counter ( left ) right_counts = Counter ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts The Python Standard Library has a collections library that has a Counter object already built. The WET Way: Plotting # Here is a soggy example of a function that makes plots. Good: Docstring, somewhat informative name. Bad: This function does multiple things, and uses a hard path ('2016.csv.gz'). def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 1 ]] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ]. mean (), second [ second . TYPE == PRECIPITATION_TYPE ]. mean ()) The DRY Way: Plotting # In this example, we've made a helper function. In this example, the function has a better name, doesn't load the dataframe within the function, and has a docstring specifying what the inputs and outputs are as well as their types. def plot_precipitation ( df , station_id , output_file = 'out.png' ): \"\"\" Plot the precipitation at the passed weather station and return the mean precipitation among all values. :param pd.DataFrame df: NOAA data (see parsers.py for more info) :param str station_id: The station to plot :param str output_file: Where to store the output plot :return: The mean precipitation in the data frame :rtype: float \"\"\" res_df = df [ df . STATION == station_id ] res_df [ res_df . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( output_file ) return res_df [ 'VALUE' ]. mean () def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) return ( plot_precipitation ( df , CHICAGO_STATION_NAMES [ 0 ] , output_file = 'first_fig.png' ), plot_precipitation ( df , CHICAGO_STATION_NAMES [ 1 ] , output_file = 'second_fig.png' )) Use functions to not repeat yourself Good rule of thumb: If you've gone 20 lines without making a comment (e.g., the docstring of a function), you likely should add one. Reduce Cognitive Load. # Follow PEP-8 # How long does it take you to understand what this function is doing? def GCD ( a , b ): \"\"\"Return the GCD of a and b\"\"\" a , b = max ( a , b ), min ( a , b ) return b if a % b == 0 else GCD ( b , a % b ) What makes this one better? # def gcd ( a , b ): \"\"\"Return the GCD of a and b :param int a: The first number (positive) :param int b: The second number (positive) :return: The GCD of a and b :rtype: int \"\"\" if a < b : b , a = a , b if a % b : return gcd ( b , a % b ) return b PEP-8 is your friend! # 79 character lines (comes from the days of punchcards) Use parenthesis for lines that span multiple lines function_names are snake_case ClassNames are CamelCase CONSTANT_NAMES are BIG_CASE One statement per line Use spaces not tabs! You can use a checker to PEP-8 your code. # pip install autopep8 autopep8 --in-place mypythonfile.py # Summary: Code is for humans, not computers. # Give things informative names Document inputs and outputs Don't repeat yourself (or others!) PEP-8 is your friend Exercises # Fix this class! # class myclass ( object ): def __init__ ( self , R , I ): self . R = R self . I = I def Multiply ( self , other ): return myclass ( self . R * other . R - self . I * other . I , self . R * other . I + self . I * other . R ) # Fix this function! # def bang ( n ): return n == 1 or ( n * bang ( n )) Fix this function! # def read_data ( filename ): \"\"\" Return the precipitation field from the csv passed in \"\"\" with open ( filename , 'r' ) as f : return [ line . split ( ',' ) [ 2 ] for line in f ]","title":"Legible, good code"},{"location":"curriculum/programming_best_practices/legible-good-code/#best-practices-writing-legible-good-code","text":"","title":"Best Practices: Writing Legible, Good Code"},{"location":"curriculum/programming_best_practices/legible-good-code/#motivation","text":"All fellows will have to write code that is usable and understandable by their peers and partners, and potentially other 3rd parties. What does that entail in practice? Many fellows are coming from academic backgrounds, have self-taught programming skills, and have never worked collaboratively on a software project. We want to help them establish good habits and avoid common mistakes. (Adapted from Tutorial by Kevin Wilson, 2016 DSSG Technical Mentor)","title":"Motivation"},{"location":"curriculum/programming_best_practices/legible-good-code/#for-whom-do-we-write-code","text":"","title":"For whom do we write code?"},{"location":"curriculum/programming_best_practices/legible-good-code/#what-you-write-for-people","text":"def fib ( n ): \"\"\" :param int n: The Fibonnaci index you want to return :return: The nth Fibonnaci number :rtype: int \"\"\" if n < 2 : return 1 else : return fib ( n - 1 ) + fib ( n - 2 ) Here is an example of a function. Why write this function? Well, literally, you give the function an integer n , and the function gives you back the n th Fibonnaci number. Writing code allows you to have the computer calculate the n th Fibonnaci number, which it can probably do much faster than you can, especially as n gets larger and larger. It also allows your human comrades to see for themselves how you rattle off arbitrary Fibonacci numbers so fast. But you don't really write code for the computer...","title":"What you write (for people)."},{"location":"curriculum/programming_best_practices/legible-good-code/#what-the-computer-sees-assembly-language-for-the-processor","text":"... because if you did, it would look like this. 2 0 LOAD_FAST 0 (n) 3 LOAD_CONST 1 (2) 6 COMPARE_OP 0 (<) 9 POP_JUMP_IF_FALSE 16 3 12 LOAD_CONST 2 (1) 15 RETURN_VALUE 5 >> 16 LOAD_GLOBAL 0 (fib) 19 LOAD_FAST 0 (n) 22 LOAD_CONST 2 (1) 25 BINARY_SUBTRACT 26 CALL_FUNCTION 1 29 LOAD_GLOBAL 0 (fib) 32 LOAD_FAST 0 (n) 35 LOAD_CONST 1 (2) 38 BINARY_SUBTRACT 39 CALL_FUNCTION 1 42 BINARY_ADD 43 RETURN_VALUE 44 LOAD_CONST 0 (None) 47 RETURN_VALUE","title":"What the computer sees (assembly language for the processor)."},{"location":"curriculum/programming_best_practices/legible-good-code/#seriously-the-code-is-for-you","text":"...one month from now when you've completely forgotten what the heck alpha was... ...or tomorrow when your teammate wonders why all those 3 s are in the database.... ...or when your system is on fire at 2 AM (the witching hour of production systems), you're tired, and your customers are calling wondering what the heck is going on over there (why are they awake, anyway?)... ...or when I'm doing code review for you...","title":"Seriously, the code is for you..."},{"location":"curriculum/programming_best_practices/legible-good-code/#there-is-only-one-law-of-good-coding-code-is-for-humans-not-for-computers","text":"This law has 4 consequences: Give things informative names. Document inputs and outputs. Don't repeat yourself. Reduce cognitive load: Use PEP-8.","title":"There is only one law of good coding: Code is for humans, not for computers."},{"location":"curriculum/programming_best_practices/legible-good-code/#give-things-informative-names","text":"What does the function below do? import math def doit ( x ): output = [] y = 2 while x != 1 : if x % y == 0 : output . append ( y ) x //= y else : y += 1 continue return output def magic ( input ): filter = set () return sorted ( x for x in y for y in input if ( len ( x ) < 20 and x not in filter ) or filter . add ( x )) def pretty_pictures (): df = pd . read_csv ( '2016.csv.gz' , names = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ]) first = df [ weather_df . STATION == 'US1ILCK0010' ] first [ first . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == 'US1ILCK0014' ] second [ second . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return first [ first . TYPE == 'PRCP' ] . mean (), second [ second . TYPE == 'PRCP' ] . mean () Names should have meaning to the intended readers, e.g., You a week from now Your teammates tomorrow Your future teammates who have to deal with your code [ i , j , k ] for iterators are OK, alpha with a reference to a specific paper is not kwargs are a great place to name things CONSTANT_VALUES are too Do not fear long names; you have autocomplete. The TAB key is your friend.","title":"Give Things Informative Names"},{"location":"curriculum/programming_best_practices/legible-good-code/#document-inputs-and-outputs","text":"import math def factor ( x ): output = [] fact = 2 while x != 1 : if x % fact == 0 : output . append ( y ) x //= fact else : fact += 1 continue return output def unique_flatten ( the_input , max_length = 20 ): flattened_set = { val for val in row for row in the_input } filtered_list = [ val for val in flattened_set if len ( val ) < max_length ] filtered_list . sort () return filtered_list WEATHER_HEADERS = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ] PRECIPITATION_TYPE = 'PRCP' CHICAGO_STATION_NAMES = [ 'US1ILCK0010' , 'US1ILCK0014' ] def precipitation_in_chicago (): df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES ] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ] . mean (), second [ second . TYPE == PRECIPITATION_TYPE ] . mean ()) Every function should have a docstring The docstring should: Describe the function briefly Explicitly document the inputs with :param type name: description Explicitly document the return value with :returns: description Explicitly document the return type with :rtype: Note this follows the Sphinx/RST syntax guide . You can also follow the Numpy Format . Just be consistent.","title":"Document Inputs and Outputs"},{"location":"curriculum/programming_best_practices/legible-good-code/#dont-repeat-yourself","text":"","title":"Don't Repeat Yourself"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-wet-write-everything-twice-way","text":"def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = {} for val in left : if val not in left_counts : left_counts [ val ] = 1 else : left_counts [ val ] += 1 right_counts = {} for val in right : if val not in right_counts : right_counts [ val ] = 1 else : right_counts [ val ] += 1 left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts In this case, the value counting logic is redundant and can be abstracted away into a function.","title":"The WET (Write Everything Twice) Way"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-dry-way-use-a-function-to-avoid-redundancy","text":"def value_counts ( the_list ): \"\"\" :param list[object] the_list: The list whose values we'll count :rtype: dict[object, int] :return: A dict from the value to its count \"\"\" output = {} for val in the_list : if val not in output : output [ val ] = 1 else : output [ val ] += 1 return output def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = value_counts ( left ) right_counts = value_counts ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts Better... But, there is no need to reinvent the wheel with the value_count function if it has already been implemented for you.","title":"The DRY Way: Use a function to avoid redundancy"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-dry-er-way","text":"from collections import Counter def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = Counter ( left ) right_counts = Counter ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts The Python Standard Library has a collections library that has a Counter object already built.","title":"The DRY-er Way"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-wet-way-plotting","text":"Here is a soggy example of a function that makes plots. Good: Docstring, somewhat informative name. Bad: This function does multiple things, and uses a hard path ('2016.csv.gz'). def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 1 ]] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ]. mean (), second [ second . TYPE == PRECIPITATION_TYPE ]. mean ())","title":"The WET Way: Plotting"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-dry-way-plotting","text":"In this example, we've made a helper function. In this example, the function has a better name, doesn't load the dataframe within the function, and has a docstring specifying what the inputs and outputs are as well as their types. def plot_precipitation ( df , station_id , output_file = 'out.png' ): \"\"\" Plot the precipitation at the passed weather station and return the mean precipitation among all values. :param pd.DataFrame df: NOAA data (see parsers.py for more info) :param str station_id: The station to plot :param str output_file: Where to store the output plot :return: The mean precipitation in the data frame :rtype: float \"\"\" res_df = df [ df . STATION == station_id ] res_df [ res_df . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( output_file ) return res_df [ 'VALUE' ]. mean () def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) return ( plot_precipitation ( df , CHICAGO_STATION_NAMES [ 0 ] , output_file = 'first_fig.png' ), plot_precipitation ( df , CHICAGO_STATION_NAMES [ 1 ] , output_file = 'second_fig.png' )) Use functions to not repeat yourself Good rule of thumb: If you've gone 20 lines without making a comment (e.g., the docstring of a function), you likely should add one.","title":"The DRY Way: Plotting"},{"location":"curriculum/programming_best_practices/legible-good-code/#reduce-cognitive-load","text":"","title":"Reduce Cognitive Load."},{"location":"curriculum/programming_best_practices/legible-good-code/#follow-pep-8","text":"How long does it take you to understand what this function is doing? def GCD ( a , b ): \"\"\"Return the GCD of a and b\"\"\" a , b = max ( a , b ), min ( a , b ) return b if a % b == 0 else GCD ( b , a % b )","title":"Follow PEP-8"},{"location":"curriculum/programming_best_practices/legible-good-code/#what-makes-this-one-better","text":"def gcd ( a , b ): \"\"\"Return the GCD of a and b :param int a: The first number (positive) :param int b: The second number (positive) :return: The GCD of a and b :rtype: int \"\"\" if a < b : b , a = a , b if a % b : return gcd ( b , a % b ) return b","title":"What makes this one better?"},{"location":"curriculum/programming_best_practices/legible-good-code/#pep-8-is-your-friend","text":"79 character lines (comes from the days of punchcards) Use parenthesis for lines that span multiple lines function_names are snake_case ClassNames are CamelCase CONSTANT_NAMES are BIG_CASE One statement per line Use spaces not tabs!","title":"PEP-8 is your friend!"},{"location":"curriculum/programming_best_practices/legible-good-code/#you-can-use-a-checker-to-pep-8-your-code","text":"","title":"You can use a checker to PEP-8 your code."},{"location":"curriculum/programming_best_practices/legible-good-code/#pip-install-autopep8-autopep8-in-place-mypythonfilepy","text":"","title":"pip install autopep8"},{"location":"curriculum/programming_best_practices/legible-good-code/#summary-code-is-for-humans-not-computers","text":"Give things informative names Document inputs and outputs Don't repeat yourself (or others!) PEP-8 is your friend","title":"Summary: Code is for humans, not computers."},{"location":"curriculum/programming_best_practices/legible-good-code/#exercises","text":"","title":"Exercises"},{"location":"curriculum/programming_best_practices/legible-good-code/#fix-this-class","text":"","title":"Fix this class!"},{"location":"curriculum/programming_best_practices/legible-good-code/#class-myclassobject-def-__init__self-r-i-selfr-r-selfi-i-def-multiplyself-other-return-myclassselfr-otherr-selfi-otheri-selfr-otheri-selfi-otherr","text":"","title":"class myclass(object):"},{"location":"curriculum/programming_best_practices/legible-good-code/#fix-this-function","text":"def bang ( n ): return n == 1 or ( n * bang ( n ))","title":"Fix this function!"},{"location":"curriculum/programming_best_practices/legible-good-code/#fix-this-function_1","text":"def read_data ( filename ): \"\"\" Return the precipitation field from the csv passed in \"\"\" with open ( filename , 'r' ) as f : return [ line . split ( ',' ) [ 2 ] for line in f ]","title":"Fix this function!"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/","text":"Living in command land, or: how I learned to stop worrying and love the terminal # Where to start? # Navigating online (stackoverflow) and offline (--help & the man pages). # Terminal land is vast and may appear nebulous to the unseasoned scripter. Where do we start? Let search engines light the way, and manual pages hold your hand. Adding \"bash\" or \"in terminal\" to search terms like \"replace whitespaces\" or \"filesize\" will point you in the right direction. \"My internet is down!\" man pages are stored on your computer, so you can reference the documentation even when you are outside the internet! For example... Search: \"replace whitespaces in filename in bash\" Tip! The search engine DuckDuckGo returns StackOverflow answers as the first-hit-preview ;) find -name \"* *\" -type d | rename 's/ /_/g' find -name \"* *\" -type f | rename 's/ /_/g' So what else can we do with rename ? $ man rename > ... -n, -nono No action: print names of files to be renamed, but don't rename. This is helpful when you're not too confident about what your command will do. ... use /<keyword> to search for <keyword> in the manual. Command Line 101 # Mind the command # The first rule of command line is \"be careful what you wish for\". The computer will do exactly what you say, but human's may have trouble speaking the computer's language. This can be dangerous when you're running commands like rm (remove), or mv (move, also used for renaming files). You can \"echo\" your commands to just print the command text without actually running the command. This can save your files and sometimes even your jorb! (Tip! Don't delete all your data with a misplaced mv ) You can create dummy files to use for this tutorial sing the touch command, in case you don't want to operate on real files until you're comfortable with these commands. Let's start by creating a file with space bars in the name. touch space\\ bars\\ .txt Note the use of the escape character \\ to signal that we intend to use the space bar as a character in our filename string. Without the backslashes, the command is interepreted as touch with several separate arguments, so in fact... touch space bars .txt ...will create 3 files seperate files! space , bars , and .txt . Where am I? # pwd prints the name of the current working directory cd .. changes directory to one level/folder up cd ~/ goes to the home directory What's in my folder? # ls lists the contents in your current dictory. ls -l \"long listing\" format ( -l ) shows the filesize, date of last change, and file permissions tree lists the contents of the current directory and all sub-directories as a tree structure (great for peeking into folder structures!) tree -L 2 limits the tree expansion to 2 levels tree -hs shows file sizes ( -s ) in human-readable format ( -h ) What's in my file? # head -n10 $f shows the \"head\" of the file, in this case the top 10 lines tail -n10 $f shows the \"tail\" of the file tail -n10 $f | watch -n1 watches the tail of the file for any changes every second ( -n1 ) tail -f -n10 $f follows ( -f ) the tail of the file every time it changes, useful if you are checking the log of a running program wc $f counts words, lines and characters in a file (separate counts using -w or -l or -c ) Where is my file? # find -name \"<lost_file_name>\" -type f finds files by name find -name \"<lost_dir_name>\" -type d finds directories by name Renaming files # Rename files with rename . For example, to replace all space bars with underscores: rename 's/ /_/g' space\\ bars\\ .txt This command substitutes ( s ) space bars ( / / ) for underscores ( /_/ ) in the entire file name (globally, g ). (The 3 slashes can be replaced by any sequence of 3 characters, so 's# #_#g' would also work and can sometimes be more legible, for example when you need to escape a special character with a backslash.) You can replace multiple characters at a time by using a simple logical OR \"regular expression\" ( | ) such as [ |?] which will replace every space bar or question mark. rename 's/[ |?]/_/g' space\\ bars?.txt (The file will be renamed to space_bars_.txt ) Bonus points: rename 'y/A-Z/a-z/' renames files to all-lowercase rename 'y/a-z/A-Z/' renames files to all-uppercase Caveats for git users # Moving files around on your computer can confuse git. If you are git-tracking a file, make sure to use the following alternatives so git knows what's going on. git mv /source/path/$move_me /destination/path/$move_me git rm $remove_me Data structures # Variables are declared with a single \"=\" and no spaces. location=\"Lisbon\" Arrays are enclosed in brackets. array=(abc 123 doremi) If you echo the array, you will get the first element. $ echo $array > abc To echo the full array, expand the array with @: $ echo ${array[@]} > abc 123 doremi Control flow and logic # Every bash statement is separated by a semicolon. This allows us to write one-liners that would normally be spread out over multiple lines. So a for loop... for i in {a..z}; do echo $i; done ...can be written as a one-liner: for i in {a..z}; do echo $i; done Tricks # Brace expansion allows you to iterate over a range of possible variables. $ echo {0..9} > 0 1 2 3 4 5 6 7 8 9 $ echo {0..9..2} > 0 2 4 8 $ echo happy_birthday.{wav,mp3,flac} > happy_birthday.wav happy_birthday.mp3 happy_birthday.flac Functions # We can write functions in shell scripts as well! The syntax looks like this... function_name(args) { function_body } You can even define shell functions inside your ~/.bashrc profile when a simple alias just won't do... For example, run a jupyter notebook remotely through an SSH tunnel and forward the connection to your localhost: jupyter_local() { ssh -i ~/.ssh/<key>.pem -NfL \"$1\":localhost:\"$2\" <user>@<host>; } Then we can just write... jupyter_local 8888 8889 ...to run a jupyter server on <host> (@ port 8888) and view it on our local machine (@port 8889) Surfing the net # You can send HTTP requests to URLs from the command line. You can retrieve a page by sending a GET request: curl -iX GET https://duckduckgo.com Or just the response header: curl -I https://duckduckgo.com From which you can parse out the status code, which is useful to see if the page is responding (200 OK) or non-existinent (404 File Not Found), etc. curl -I https://duckduckgo.com 2>/dev/null | head -n 1 | cut -d$' ' -f2 where... 2>/dev/null redirects the stderr to oblivion head -n 1 reads the top line only cut -d$' ' -f2 separates the line by the divider (spacebar) and takes the 2nd field (which is the numerical HTTP response status code). Working remotely via SSH # When working via SSH, a connection interruption can terminate your running scripts, lose your environment varaibles and lose your command history! D: There's a way to avoid this. Actually there's two: screen and tmux are two programs that allow you to run a terminal session remotely on a remote server which you can interact with from your own machine via SSH. So if you ever lose connection to the server, your terminal session is still running - you just have to log back into it. You can also run multiple independent terminal sessions on the same server this way. tmux (aka: terminal multiplexer) # # \"Ping\" the server to check if it's reachable (it \"pongs\" back... get it?) ping <server> # ssh into the server ssh <user>@<server> # Open a tmux session tmux # List existing sessions tmux ls # Attach (a) to a target session (-t #) tmux a -t 1 # Rename the current window Ctrl+b+, # Kill the current pane Ctrl+b+x # Create a new pane Ctrl+b+c # Split windows horizontally into two Ctrl+b+\" # Split windows vertically into two Ctrl+b+% # Tohttps://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ggle between horizontal/vertical splits Ctrl+b+space Tmux can easily be configured by editing the tmux configuration file at ~/.tmux.conf If you search \"tmux cheatsheet\" on (DuckDuckGo.com)[https://duckduckgo.com], the preview search result reveals some more useful commands [: VIM (Vi iMproved) - text editor # Why bother? Vim is a powerful, lightweight, open-source, cross-platform text editor, that comes pre-installed on Unix systems. Vi was written in 1976 by Bill Joy at Sun Microsystems, and has been improved in 1991 to Vim. Vim was designed for maximum efficiency and minimum bandwidth when working on old modems. It does not require use of the mouse or arrow keys. Much of learning Vim is just habit and muscle memory, in the first place this can be frustrating, but soon becomes second nature. But I'm scared? Don't worry here are some useful hints, tips, and tricks for using vim. Pleas note if at any point during this session you feel bewildered, nauseous, or perhaps euphoric, remain calm and press the Esc key to get back to normal. Where should I start? A comprehensive although slightly dry start point for learning vim is through the vimtutor document available as standard with vim (just type vimtutor and hit Enter). A more fun way to get used to moving in vim is playing this fun maze game. https://vim-adventures.com/ A useful cheatsheet: https://vim.rtorr.com/ The Bare Necessities: Vim has three modes: 1. Normal: this is for normal movement through a file (press I for Insert, or V for visual) 2. Insert: this is for editing files and adding text (press Esc to get back to Normal) 3. Visual: this is for highlighting lines in files (press Esc to get back to Normal) Navigation: # Movement is through h , j , k , and l go to the top of the file with gg go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line 0 jump to the end of the line $ Editing: # delete a character x delete a line dd delete 10 lines 10dd change a word cw undo a change u redo a change `Ctrl+r' go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line `` jump to the end of the line $ start editing on line below o start editing at end of line A Highlighting (visual mode): # select a line V select 8 lines 8V yank or copy y paste p search /pattern see next search match n see previous search match N Exiting # Okay enough, get me out of here: * quit a file :q * write changes to a file :w (normal humanoids call this saving a file) * no, really get me out of here, I don't care about saving :q! The stuff they don't teach: * :%s/old/new/gc substitute old pattern for new globally but check each (commonly humanoids refer to this as find and replace) * :10,20s/old/new/g substitute the old pattern for the new only between lines 10 and 20 * select a column of text Ctrl+V+j+j+j+j * comment a column of text Ctrl+V+j+j+j+j+# * go to file explorer :Ex * open a file on my remote vim scp://path/to/file/ * change your ~/.vimrc * set nu add numbering * set hlsearch highlight search results If you are convinced and want to go one step further you can configure vim as an IDE for python development here https://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ vim promotes social good Vim is Charityware. You can use and copy it as much as you like, but you are encouraged to make a donation for needy children in Uganda. Please see kcc below or visit the ICCF web site, available at these URLs: http://iccf-holland.org/ http://www.vim.org/iccf/ http://www.iccf.nl/ You can also sponsor the development of Vim. Vim sponsors can vote for features. See sponsor. The money goes to Uganda anyway. Bonus points # Rogue terminals # We all make mistakes. Sometimes we make mistakes in infinite loops. What do we do when \"Ctrl+C\" is not enough? top or htop allow us to see what processes are running on our computer. (cf. System Monitor @ Mac) Every process has an ID ( pid ) which we can use to send a kill command to it. ps -ef | grep badprocess | awk '{print $2}' | kill `xargs $1` Sometimes badprocess spawns other badprocess processes... so we can loop over them all. ps -ef | grep badprocess | awk '{print $2}' | for f in `xargs $1`; do kill $f; done Parallel programming (sort of) # Run parallel processes on a multi-core system using GNU parallel. Typically, High-Performance Computing clusters have multi-cores (think quad-quad-quad-core), but running your script on the HPC is not enough to exploit it. What if you could run your script multiple times across each of the cores? NUM_JOBS=16 parallel -j=$NUM_JOBS --dry-run <script.sh> Remove --dry-run to actually run the script ;) dry-run shows you what will happen without actually running any code - it's a good way to double-check the expected behaviour of your script before. Custom prompts # You can customise your command prompt by changing the $PS1 variable. Motivational cow # If you need a little inspiration, let the fortune package brighten up your day! Even better, let an ASCII cow lighten up your day! # Install the fortune and cowsay packages sudo apt-get install cowsay fortune # \"Pipe\" the output of fortune into the cowsay command fortune | cowthink _______________________________ / Don't Worry, Be Happy. \\ \\ -- Meher Baba / ------------------------------- O ^__^ o (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Living in command land, or:<br/>how I learned to stop worrying and love the terminal"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#living-in-command-land-orhow-i-learned-to-stop-worrying-and-love-the-terminal","text":"","title":"Living in command land, or:how I learned to stop worrying and love the terminal"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#where-to-start","text":"","title":"Where to start?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#navigating-online-stackoverflow-and-offline-help-the-man-pages","text":"Terminal land is vast and may appear nebulous to the unseasoned scripter. Where do we start? Let search engines light the way, and manual pages hold your hand. Adding \"bash\" or \"in terminal\" to search terms like \"replace whitespaces\" or \"filesize\" will point you in the right direction. \"My internet is down!\" man pages are stored on your computer, so you can reference the documentation even when you are outside the internet! For example... Search: \"replace whitespaces in filename in bash\" Tip! The search engine DuckDuckGo returns StackOverflow answers as the first-hit-preview ;) find -name \"* *\" -type d | rename 's/ /_/g' find -name \"* *\" -type f | rename 's/ /_/g' So what else can we do with rename ? $ man rename > ... -n, -nono No action: print names of files to be renamed, but don't rename. This is helpful when you're not too confident about what your command will do. ... use /<keyword> to search for <keyword> in the manual.","title":"Navigating online (stackoverflow) and offline (--help &amp; the man pages)."},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#command-line-101","text":"","title":"Command Line 101"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#mind-the-command","text":"The first rule of command line is \"be careful what you wish for\". The computer will do exactly what you say, but human's may have trouble speaking the computer's language. This can be dangerous when you're running commands like rm (remove), or mv (move, also used for renaming files). You can \"echo\" your commands to just print the command text without actually running the command. This can save your files and sometimes even your jorb! (Tip! Don't delete all your data with a misplaced mv ) You can create dummy files to use for this tutorial sing the touch command, in case you don't want to operate on real files until you're comfortable with these commands. Let's start by creating a file with space bars in the name. touch space\\ bars\\ .txt Note the use of the escape character \\ to signal that we intend to use the space bar as a character in our filename string. Without the backslashes, the command is interepreted as touch with several separate arguments, so in fact... touch space bars .txt ...will create 3 files seperate files! space , bars , and .txt .","title":"Mind the command"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#where-am-i","text":"pwd prints the name of the current working directory cd .. changes directory to one level/folder up cd ~/ goes to the home directory","title":"Where am I?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#whats-in-my-folder","text":"ls lists the contents in your current dictory. ls -l \"long listing\" format ( -l ) shows the filesize, date of last change, and file permissions tree lists the contents of the current directory and all sub-directories as a tree structure (great for peeking into folder structures!) tree -L 2 limits the tree expansion to 2 levels tree -hs shows file sizes ( -s ) in human-readable format ( -h )","title":"What's in my folder?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#whats-in-my-file","text":"head -n10 $f shows the \"head\" of the file, in this case the top 10 lines tail -n10 $f shows the \"tail\" of the file tail -n10 $f | watch -n1 watches the tail of the file for any changes every second ( -n1 ) tail -f -n10 $f follows ( -f ) the tail of the file every time it changes, useful if you are checking the log of a running program wc $f counts words, lines and characters in a file (separate counts using -w or -l or -c )","title":"What's in my file?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#where-is-my-file","text":"find -name \"<lost_file_name>\" -type f finds files by name find -name \"<lost_dir_name>\" -type d finds directories by name","title":"Where is my file?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#renaming-files","text":"Rename files with rename . For example, to replace all space bars with underscores: rename 's/ /_/g' space\\ bars\\ .txt This command substitutes ( s ) space bars ( / / ) for underscores ( /_/ ) in the entire file name (globally, g ). (The 3 slashes can be replaced by any sequence of 3 characters, so 's# #_#g' would also work and can sometimes be more legible, for example when you need to escape a special character with a backslash.) You can replace multiple characters at a time by using a simple logical OR \"regular expression\" ( | ) such as [ |?] which will replace every space bar or question mark. rename 's/[ |?]/_/g' space\\ bars?.txt (The file will be renamed to space_bars_.txt ) Bonus points: rename 'y/A-Z/a-z/' renames files to all-lowercase rename 'y/a-z/A-Z/' renames files to all-uppercase","title":"Renaming files"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#caveats-for-git-users","text":"Moving files around on your computer can confuse git. If you are git-tracking a file, make sure to use the following alternatives so git knows what's going on. git mv /source/path/$move_me /destination/path/$move_me git rm $remove_me","title":"Caveats for git users"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#data-structures","text":"Variables are declared with a single \"=\" and no spaces. location=\"Lisbon\" Arrays are enclosed in brackets. array=(abc 123 doremi) If you echo the array, you will get the first element. $ echo $array > abc To echo the full array, expand the array with @: $ echo ${array[@]} > abc 123 doremi","title":"Data structures"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#control-flow-and-logic","text":"Every bash statement is separated by a semicolon. This allows us to write one-liners that would normally be spread out over multiple lines. So a for loop... for i in {a..z}; do echo $i; done ...can be written as a one-liner: for i in {a..z}; do echo $i; done","title":"Control flow and logic"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#tricks","text":"Brace expansion allows you to iterate over a range of possible variables. $ echo {0..9} > 0 1 2 3 4 5 6 7 8 9 $ echo {0..9..2} > 0 2 4 8 $ echo happy_birthday.{wav,mp3,flac} > happy_birthday.wav happy_birthday.mp3 happy_birthday.flac","title":"Tricks"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#functions","text":"We can write functions in shell scripts as well! The syntax looks like this... function_name(args) { function_body } You can even define shell functions inside your ~/.bashrc profile when a simple alias just won't do... For example, run a jupyter notebook remotely through an SSH tunnel and forward the connection to your localhost: jupyter_local() { ssh -i ~/.ssh/<key>.pem -NfL \"$1\":localhost:\"$2\" <user>@<host>; } Then we can just write... jupyter_local 8888 8889 ...to run a jupyter server on <host> (@ port 8888) and view it on our local machine (@port 8889)","title":"Functions"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#surfing-the-net","text":"You can send HTTP requests to URLs from the command line. You can retrieve a page by sending a GET request: curl -iX GET https://duckduckgo.com Or just the response header: curl -I https://duckduckgo.com From which you can parse out the status code, which is useful to see if the page is responding (200 OK) or non-existinent (404 File Not Found), etc. curl -I https://duckduckgo.com 2>/dev/null | head -n 1 | cut -d$' ' -f2 where... 2>/dev/null redirects the stderr to oblivion head -n 1 reads the top line only cut -d$' ' -f2 separates the line by the divider (spacebar) and takes the 2nd field (which is the numerical HTTP response status code).","title":"Surfing the net"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#working-remotely-via-ssh","text":"When working via SSH, a connection interruption can terminate your running scripts, lose your environment varaibles and lose your command history! D: There's a way to avoid this. Actually there's two: screen and tmux are two programs that allow you to run a terminal session remotely on a remote server which you can interact with from your own machine via SSH. So if you ever lose connection to the server, your terminal session is still running - you just have to log back into it. You can also run multiple independent terminal sessions on the same server this way.","title":"Working remotely via SSH"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#tmuxaka-terminal-multiplexer","text":"# \"Ping\" the server to check if it's reachable (it \"pongs\" back... get it?) ping <server> # ssh into the server ssh <user>@<server> # Open a tmux session tmux # List existing sessions tmux ls # Attach (a) to a target session (-t #) tmux a -t 1 # Rename the current window Ctrl+b+, # Kill the current pane Ctrl+b+x # Create a new pane Ctrl+b+c # Split windows horizontally into two Ctrl+b+\" # Split windows vertically into two Ctrl+b+% # Tohttps://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ggle between horizontal/vertical splits Ctrl+b+space Tmux can easily be configured by editing the tmux configuration file at ~/.tmux.conf If you search \"tmux cheatsheet\" on (DuckDuckGo.com)[https://duckduckgo.com], the preview search result reveals some more useful commands [:","title":"tmux(aka: terminal multiplexer)"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#vim-vi-improved-text-editor","text":"Why bother? Vim is a powerful, lightweight, open-source, cross-platform text editor, that comes pre-installed on Unix systems. Vi was written in 1976 by Bill Joy at Sun Microsystems, and has been improved in 1991 to Vim. Vim was designed for maximum efficiency and minimum bandwidth when working on old modems. It does not require use of the mouse or arrow keys. Much of learning Vim is just habit and muscle memory, in the first place this can be frustrating, but soon becomes second nature. But I'm scared? Don't worry here are some useful hints, tips, and tricks for using vim. Pleas note if at any point during this session you feel bewildered, nauseous, or perhaps euphoric, remain calm and press the Esc key to get back to normal. Where should I start? A comprehensive although slightly dry start point for learning vim is through the vimtutor document available as standard with vim (just type vimtutor and hit Enter). A more fun way to get used to moving in vim is playing this fun maze game. https://vim-adventures.com/ A useful cheatsheet: https://vim.rtorr.com/ The Bare Necessities: Vim has three modes: 1. Normal: this is for normal movement through a file (press I for Insert, or V for visual) 2. Insert: this is for editing files and adding text (press Esc to get back to Normal) 3. Visual: this is for highlighting lines in files (press Esc to get back to Normal)","title":"VIM (Vi iMproved) - text editor"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#navigation","text":"Movement is through h , j , k , and l go to the top of the file with gg go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line 0 jump to the end of the line $","title":"Navigation:"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#editing","text":"delete a character x delete a line dd delete 10 lines 10dd change a word cw undo a change u redo a change `Ctrl+r' go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line `` jump to the end of the line $ start editing on line below o start editing at end of line A","title":"Editing:"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#highlighting-visual-mode","text":"select a line V select 8 lines 8V yank or copy y paste p search /pattern see next search match n see previous search match N","title":"Highlighting (visual mode):"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#exiting","text":"Okay enough, get me out of here: * quit a file :q * write changes to a file :w (normal humanoids call this saving a file) * no, really get me out of here, I don't care about saving :q! The stuff they don't teach: * :%s/old/new/gc substitute old pattern for new globally but check each (commonly humanoids refer to this as find and replace) * :10,20s/old/new/g substitute the old pattern for the new only between lines 10 and 20 * select a column of text Ctrl+V+j+j+j+j * comment a column of text Ctrl+V+j+j+j+j+# * go to file explorer :Ex * open a file on my remote vim scp://path/to/file/ * change your ~/.vimrc * set nu add numbering * set hlsearch highlight search results If you are convinced and want to go one step further you can configure vim as an IDE for python development here https://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ vim promotes social good Vim is Charityware. You can use and copy it as much as you like, but you are encouraged to make a donation for needy children in Uganda. Please see kcc below or visit the ICCF web site, available at these URLs: http://iccf-holland.org/ http://www.vim.org/iccf/ http://www.iccf.nl/ You can also sponsor the development of Vim. Vim sponsors can vote for features. See sponsor. The money goes to Uganda anyway.","title":"Exiting"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#bonus-points","text":"","title":"Bonus points"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#rogue-terminals","text":"We all make mistakes. Sometimes we make mistakes in infinite loops. What do we do when \"Ctrl+C\" is not enough? top or htop allow us to see what processes are running on our computer. (cf. System Monitor @ Mac) Every process has an ID ( pid ) which we can use to send a kill command to it. ps -ef | grep badprocess | awk '{print $2}' | kill `xargs $1` Sometimes badprocess spawns other badprocess processes... so we can loop over them all. ps -ef | grep badprocess | awk '{print $2}' | for f in `xargs $1`; do kill $f; done","title":"Rogue terminals"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#parallel-programming-sort-of","text":"Run parallel processes on a multi-core system using GNU parallel. Typically, High-Performance Computing clusters have multi-cores (think quad-quad-quad-core), but running your script on the HPC is not enough to exploit it. What if you could run your script multiple times across each of the cores? NUM_JOBS=16 parallel -j=$NUM_JOBS --dry-run <script.sh> Remove --dry-run to actually run the script ;) dry-run shows you what will happen without actually running any code - it's a good way to double-check the expected behaviour of your script before.","title":"Parallel programming (sort of)"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#custom-prompts","text":"You can customise your command prompt by changing the $PS1 variable.","title":"Custom prompts"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#motivational-cow","text":"If you need a little inspiration, let the fortune package brighten up your day! Even better, let an ASCII cow lighten up your day! # Install the fortune and cowsay packages sudo apt-get install cowsay fortune # \"Pipe\" the output of fortune into the cowsay command fortune | cowthink _______________________________ / Don't Worry, Be Happy. \\ \\ -- Meher Baba / ------------------------------- O ^__^ o (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Motivational cow"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/","text":"dotfiles - make working on any computer and feel just like home # On this tutorial, you'll learn about dotfiles , what they are, why are they important and how to use them. What are dotfiles anyway? # Dotfiles are used to configure system settings, you can configure anything from them. From text editor syntax to a list of commands to execute each time you open a terminal session. Dotfiles names start with . , e.g. .bash_profile , .Rprofile , .vimrc and most of them reside in your home folder. As you get familiar with the command line, you are going to start tweaking your computer with personal settings. Maybe setting special shortcuts for common commands (e.g. typing jnb to start Jupyter instead of typing jupyter notebook ). As your system becomes more and more customized, it's going to be pretty different to the original configuration. So imagine you have dozens of nice shortcuts and configuration settings for your favorite applications, then you start working on the DSSG server and all the magic is gone... not cool. A common practice is to store your files in a git repository. This way you have a history of the modifications you've done but more important, a copy you can grab from anywhere (e.g. the DSSG server) and get all your magic. Finding your dotfiles # Most applications store a dotfile in your home folder, type the following in the command line to see yours: find . -name '.*' -maxdepth 1 Here are some of mine: .Rprofile #settings for your R sessions .vimrc #vim settings .bash_profile #shell settings Let's see how my .Rprofile looks: ## Change colors when running R in the terminal if ( Sys.getenv ( \"TERM\" ) == \"xterm-256color\" ) library ( \"colorout\" ) If you have an .Rprofile file, Whatever it is there will be executed when you start an R session. Mine just loads a package colorout which adds nice colors to the R interpreter. Note: even though every dotfile starts with . , not everything that starts with a . is a dotfile. For example .DS_Store is a file you'll find on many folder if you use OS X, this file stores custom attributes for the folder but you don't want to modify it directly, it's just a file the system uses to keep track of folder customizations (e.g. changing the icon) .bashrc and .bash_profile # When you open a terminal, a program called bash starts, this program let's you execute commands such as cd , ls , etc. Bash is highly configurable through its dotfiles: .bashrc and .bash_profile . There are some differences between those two and they get executed at different times, but a nice setting to get started is to make .bash_profile call .bashrc and set your configuration file in the later. To to that follow this steps: # open .bash_profile open ~/.bash_profile Your default editor will open the file, chances are the file contains some settings already, to avoid breaking your system, do not delete anything and just put this at the top of the file. # just load ~/.bashrc if [ -f ~/.bashrc ] ; then source ~/.bashrc fi Now you can start twerking your shell, for example adding shortcuts. Let's create one that outputs only folders in our current working directory. First, open your .bashrc : open ~/.bashrc Add this line: # List only directories alias lsd = 'ls -l | grep \"^d\"' Save the file. Close the terminal and open a new one. Now, every time you execute lsd , your command line will print only folders in the current directory and not the files. Ok, that was a pretty simple example, not let's see how to use git. Using git to manage your dotfiles # As I mentioned before, your dotfiles live in your home folder (type cd ~; pwd to see which is yours). Your home folder contains a lot of stuff and you probably don't want to create a git repo there (please don't). To solve this issue we can do the following: create a folder anywhere in your computer, create your dotfiles there and then link them to your home folder, where your applications expect your dotfiles to be. Let's imagine you want to save your dotfiles in ~/dotfiles . Run the following to create the folder add some files and start a git repository: # create and move to the folder mkdir ~/dotfiles ; cd ~/dotfiles # get the content from your original .bashrc and copy it in your # .bashrc stored in ~/dotfiles, to the same with .bash_profile cat ~/.bashrc > ~/dotfiles/.bashrc cat ~/.bash_profile > ~/dotfiles/.bash_profile # init repo and commit git init git add --all git commit -m 'dotfiles are awesome' Now you have a copy of your .bashrc and .bash_profile outside your home folder and you created a repo to store them. But there's one step missing, if you modify your dotfiles in ~/dotfiles , your computer won't do anything because it will look in your home folder. To fix it we need to link our files in ~/dotfiles to our home folder. To do that we'll create symlinks , which are basically pointers to files, that way you can store your dotfiles anywhere and your computer is still going to find them. # link files in ~/dotfiles to your home folder ln -s ~/dotfiles/.bashrc ~/.bashrc ln -s ~/dotfiles/.bash_profile ~/.bash_profile Now, you can modify, commit, push, pull from ~/dotfiles and still make your computer find them in your home folder! Now, create a remote repository to host them on github. If you don't know how, check out the git tutorial . Using your dot files in another machine # At this point you setup your dotfiles (only two for now) using git, you can version them and backup using github. Let's see how to bring your dotfiles to a new machine. First, login in the new machine and clone your repo: git clone https://github.com/youruser/yourrepo cd yourrepo You just got your files, now it's time to link them to your home folder on this new machine. ln -s .bashrc ~/.bashrc ln -s .bash_profile ~/.bash_profile Now, you can use your local settings in the server! It's all about automation # Manually linking each dotfiles is tedious, let's automate it. The easiest way of doing it is to add a script in your repo to run the code to create the links, the problem is that every time you create a new dotfile, you'll also need to update the script. If you only have a couple of dotfiles this is fine. But if you want superpowers, you can automate the process so next time you use a new machine, setting up your dot files will look like this: git clone https://github.com/youruser/yourrepo cd yourrepo ./install There are many ways of doing the ./install step but you need to be familiar with bash. If you want to see examples of it, see this . Resources # dotfiles - unofficial guide to dotfiles on Github","title":"Pimp my dotfiles!"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#dotfiles-make-working-on-any-computer-and-feel-just-like-home","text":"On this tutorial, you'll learn about dotfiles , what they are, why are they important and how to use them.","title":"dotfiles - make working on any computer and feel just like home"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#what-are-dotfiles-anyway","text":"Dotfiles are used to configure system settings, you can configure anything from them. From text editor syntax to a list of commands to execute each time you open a terminal session. Dotfiles names start with . , e.g. .bash_profile , .Rprofile , .vimrc and most of them reside in your home folder. As you get familiar with the command line, you are going to start tweaking your computer with personal settings. Maybe setting special shortcuts for common commands (e.g. typing jnb to start Jupyter instead of typing jupyter notebook ). As your system becomes more and more customized, it's going to be pretty different to the original configuration. So imagine you have dozens of nice shortcuts and configuration settings for your favorite applications, then you start working on the DSSG server and all the magic is gone... not cool. A common practice is to store your files in a git repository. This way you have a history of the modifications you've done but more important, a copy you can grab from anywhere (e.g. the DSSG server) and get all your magic.","title":"What are dotfiles anyway?"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#finding-your-dotfiles","text":"Most applications store a dotfile in your home folder, type the following in the command line to see yours: find . -name '.*' -maxdepth 1 Here are some of mine: .Rprofile #settings for your R sessions .vimrc #vim settings .bash_profile #shell settings Let's see how my .Rprofile looks: ## Change colors when running R in the terminal if ( Sys.getenv ( \"TERM\" ) == \"xterm-256color\" ) library ( \"colorout\" ) If you have an .Rprofile file, Whatever it is there will be executed when you start an R session. Mine just loads a package colorout which adds nice colors to the R interpreter. Note: even though every dotfile starts with . , not everything that starts with a . is a dotfile. For example .DS_Store is a file you'll find on many folder if you use OS X, this file stores custom attributes for the folder but you don't want to modify it directly, it's just a file the system uses to keep track of folder customizations (e.g. changing the icon)","title":"Finding your dotfiles"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#bashrc-and-bash_profile","text":"When you open a terminal, a program called bash starts, this program let's you execute commands such as cd , ls , etc. Bash is highly configurable through its dotfiles: .bashrc and .bash_profile . There are some differences between those two and they get executed at different times, but a nice setting to get started is to make .bash_profile call .bashrc and set your configuration file in the later. To to that follow this steps: # open .bash_profile open ~/.bash_profile Your default editor will open the file, chances are the file contains some settings already, to avoid breaking your system, do not delete anything and just put this at the top of the file. # just load ~/.bashrc if [ -f ~/.bashrc ] ; then source ~/.bashrc fi Now you can start twerking your shell, for example adding shortcuts. Let's create one that outputs only folders in our current working directory. First, open your .bashrc : open ~/.bashrc Add this line: # List only directories alias lsd = 'ls -l | grep \"^d\"' Save the file. Close the terminal and open a new one. Now, every time you execute lsd , your command line will print only folders in the current directory and not the files. Ok, that was a pretty simple example, not let's see how to use git.","title":".bashrc and .bash_profile"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#using-git-to-manage-your-dotfiles","text":"As I mentioned before, your dotfiles live in your home folder (type cd ~; pwd to see which is yours). Your home folder contains a lot of stuff and you probably don't want to create a git repo there (please don't). To solve this issue we can do the following: create a folder anywhere in your computer, create your dotfiles there and then link them to your home folder, where your applications expect your dotfiles to be. Let's imagine you want to save your dotfiles in ~/dotfiles . Run the following to create the folder add some files and start a git repository: # create and move to the folder mkdir ~/dotfiles ; cd ~/dotfiles # get the content from your original .bashrc and copy it in your # .bashrc stored in ~/dotfiles, to the same with .bash_profile cat ~/.bashrc > ~/dotfiles/.bashrc cat ~/.bash_profile > ~/dotfiles/.bash_profile # init repo and commit git init git add --all git commit -m 'dotfiles are awesome' Now you have a copy of your .bashrc and .bash_profile outside your home folder and you created a repo to store them. But there's one step missing, if you modify your dotfiles in ~/dotfiles , your computer won't do anything because it will look in your home folder. To fix it we need to link our files in ~/dotfiles to our home folder. To do that we'll create symlinks , which are basically pointers to files, that way you can store your dotfiles anywhere and your computer is still going to find them. # link files in ~/dotfiles to your home folder ln -s ~/dotfiles/.bashrc ~/.bashrc ln -s ~/dotfiles/.bash_profile ~/.bash_profile Now, you can modify, commit, push, pull from ~/dotfiles and still make your computer find them in your home folder! Now, create a remote repository to host them on github. If you don't know how, check out the git tutorial .","title":"Using git to manage your dotfiles"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#using-your-dot-files-in-another-machine","text":"At this point you setup your dotfiles (only two for now) using git, you can version them and backup using github. Let's see how to bring your dotfiles to a new machine. First, login in the new machine and clone your repo: git clone https://github.com/youruser/yourrepo cd yourrepo You just got your files, now it's time to link them to your home folder on this new machine. ln -s .bashrc ~/.bashrc ln -s .bash_profile ~/.bash_profile Now, you can use your local settings in the server!","title":"Using your dot files in another machine"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#its-all-about-automation","text":"Manually linking each dotfiles is tedious, let's automate it. The easiest way of doing it is to add a script in your repo to run the code to create the links, the problem is that every time you create a new dotfile, you'll also need to update the script. If you only have a couple of dotfiles this is fine. But if you want superpowers, you can automate the process so next time you use a new machine, setting up your dot files will look like this: git clone https://github.com/youruser/yourrepo cd yourrepo ./install There are many ways of doing the ./install step but you need to be familiar with bash. If you want to see examples of it, see this .","title":"It's all about automation"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#resources","text":"dotfiles - unofficial guide to dotfiles on Github","title":"Resources"},{"location":"curriculum/programming_best_practices/reproducible-software/","text":"Making Projects Reproducible # Scientific software is often developed and used by a single person. It is all too common in academia to be handed a postdoc or graduate student's old code and be unable to replicate the original study, run the software outside of the original development machine, or even get the software to work at all. The goal of this tutorial is to provide some guidelines to make your summer projects reproducible -- this means your project can be installed on another computer and give the same results you got over the summer. At the end of the summer, your project should be understandable and transferable to your future-self and anyone else who may want to pick up where you left off without having to constantly email you about how to get your project running. (Note: Your future-self doesn't have the luxury of being able to email your past-self). What is a reproducible project? # One that... works for someone other than the original team can be easily installed on another computer has documentation that describes any dependencies and how to install them comes with enough tests to indicate the software is running properly README.md(rst) # All projects should have a README that communicates the following: What the project is about A short description of the project (i.e. the problem you are trying to solve). The required dependencies to run the software The can be in the form of a requirements.txt file for Python that lists the dependencies and version numbers. The system-level dependencies. Installation instructions How to install your software and associated binaries. This can be in the form of instructions on how to use pip , apt , yum , or some other binary package manager. Example usage The inputs and outputs of your software (i.e. how to use it) with code examples. Attribution/Licensing Who did what and how others can use your software. Examples: - Chicago Food Inspections - DSSG Police EIS - Linux Kernel What to Do # Use virtual environments . Use automation tools like Make or Drake Keep your directory structure intuitive, interpretable and easy to understand . Keep your database free of \"junk tables.\" Keep only what you need and what's current. Junk tables will only confuse your future-self or others that come fresh to the project. Merge all branches into master. Branches are for adding features or patches. When you have added said feature or patch and you know you won't break the master branch, merge into master and delete the branch. Write commit messages in such a way that your log is helpful (see Git and Github tutorial .) Periodically make database backups . Write unit tests and use continuous integration so you can catch bugs quickly, particularly when you are merging new features into master. (See testing tutorial .) Document all of your functions with docstrings. (See legible, good code tutorial .) Write your python code following the PEP8 standard. (See legible, good code tutorial .) Use (4) spaces instead of tabs in your Python code for indentation. What NOT to Do # Use hard-coded paths . Require Sudo/root privileges to install your project. You can't anticipate whether or not someone will have root access to the machine they are installing your project on, so don't count on it. Additionally, you shouldn't require users to create separate user names for your project. Use non-standard formats for inputs (stick to YAML , XML , JSON , CLA , etc). My one exception to this rule is log files - which you should provide an example of in a README. Otherwise it is easier to just stick with what is already in use. Have a messy repo with random files everywhere . This is confusing, irritating and cancerous to productive enterprise. Commit data or sensitive information like database passcodes to the GitHub repo. Your repository is for your codebase, not the data. Furthermore, your data may be sensitive and need to be protected. Always assume that your repo will be public someday if you are hosting on GitHub (for your DSSG project it will be). Sensitive information also includes architecture decisions about your database. After sensitive information is pushed to GitHub, you cannot remove it completely from the repository. Have code that needs to be operationalized in Jupyter Notebooks. Jupyter notebooks are wonderful for containing your analysis, code and figures in a single document, particularly for doing exploratory analysis. They are not good for keeping the code you will need for your pipeline or code that you will eventually want to turn into a library. Virtual Environments # A virtual environment solves the problem that projectX uses version 1.x of a package while projectY uses version 2.x of a package by keeping dependencies in different environments. Install a virtualenv # pip install --user virtualenv virtualenv dssg-venv --no-site-packages #does not use any global packages You can also install a virtual environment and specify the type of python interpreter you would like to use using the -p option. This is good for keeping Python2 and Python3 dependencies separate. Python2 virtualenv dssg-py2-venv -p $(which python) --no-site-packages Python3 virtualenv dssg-py3-venv -p $(which python3) --no-site-packages Activate a virtualenv # source ./dssg-venv/bin/activate Install Dependencies # pip install -r requirements.txt Freeze Dependencies # pip freeze > requirements.txt #outputs a list of dependencies and version numbers Warning : pip freeze will output every package that was installed using pip or setup.py (setuptools). External dependencies that are from github or some other source not found on PyPi will appear but will not be found when trying to reinstall the dependencies. You can include github repositories from github in your requirements.txt file, you just have to do manual housekeeping. Other external dependencies and how to install them should be recorded in your README.md file. Note: There is also the conda environment created by Continuum Analytics. The conda environment handles creating a environment and package dependencies -- what the virtual environment + pip combination does. Conda, unlike pip, includes many non-python dependencies (e.g, MKL) as precompiled binaries that are necessary for scientific python packages. The author is currently of the opinion that if you are a beginner or using a dated OS then using a conda environment is not the worst of ideas. If you are a developer working on a development machine then compile things yourself -- an important and useful skill. Whatever path you choose be consistent about how you set up your environment and document it thoroughly. Systems Level Dependencies # Systems level dependencies are the libraries installed on your OS. For Ubuntu/Debian Linux you can get a list of them and then install them using the following: #grab systems level dependencies dpkg --get-selections > dependencies.txt #reinstall on a new machine dpkg --clear-selections sudo dpkg --set-selections < dependencies.txt Also courtesy of Tristan Crockett: installing a list of dependencies using apt xargs -a <(awk '/^\\s*[^#]/' dependencies.txt) -r -- sudo apt-get install This will give every package installed on your OS. An easier alternative is to just keep track when you install a new library and manually keep the list in a dependencies.txt file. There are also lightweight vitalization containers like Docker containers, Hyper-V images (Windows), or Ansible playbooks that can be used to \"freeze\" the systems level configuration of an OS. Backup Your Database # In PostGreSQL when a table is dropped, it is gone forever. You don't want to drop your results table on the last day of the fellowship, so it is a good idea to backup periodically. To dump your database in PostGreSQL: pg_dump -Fc --schema='raw|clean|models' -N '*public*' --no-acl -v -h <hostname> -U <dbuser> <dbname> > dssg-$(date +%F).dump Note: This can be automated with a crontab script. To restore your database from a dump: < dump_file psql -U dbuser -h dbhost dbname Hard-coded Paths # Example of Adding Shapefile with hard-coded paths # Hard-coded paths are absolute paths that are native to the machine you are using for development. It is unlikely someone else will keep their data in the exact same directory as you when trying to use your project in a separate environment. Users should be able to set location of files as command line parameters. Below are examples. load_shapefile_hardpath_v1.sh # # Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY shp2pgsql -d -s 4267:2261 -d /mnt/data/syracuse/NY_geol_dd soil.geology | psql Although this script documents the command that runs, it has a hard path and the purpose of the arguments are not clear. This script has the shelf-life of a banana. load_shapefile_hardpath_v2.sh # #!/bin/bash # Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY original_projection=4267 new_projection=2261 #projection of Upstate NY schema='soil' table='geology' shapefile='/mnt/data/syracuse/NY_geol_dd/nygeol_poly_dd.shp' #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data shp2pgsql -d -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql With this version someone can better surmise what is being done. Every time you want to load your data you have to change the filename in the script. It also checks if the table already exists in the database so the command can be used to reload data. load_shapefile_hardpath_v3.sh # #!/bin/bash #ETL script for importing shape files. PROGRAM=$(basename $0) usage=\"${PROGRAM} -s schema -t table -p original_projection [-n new_projection] [-v] shapefilename\" function die() { local errmsg=\"$1\" errcode=\"${2:-1}\" echo \"ERROR: ${errmsg}\" exit ${errcode} } #if called with no command line arguments then output usage if [ ${#} -eq 0 ] then echo ${usage} exit 1; fi #-------------------------------------------------- # process input arguments #-------------------------------------------------- verbose=\"false\" new_projection=\"\" while getopts hp:n:s:t:v OPT; do case \"${OPT}\" in h) echo \"${usage}\"; exit 0 ;; p) original_projection=\"${OPTARG}\" ;; n) new_projection=\"${OPTARG}\" ;; s) schema=\"${OPTARG}\" ;; t) table=\"${OPTARG}\" ;; v) verbose=\"true\" ;; ?) die \"unknown option or missing argument; see -h for usage\" 2 ;; esac done shift $((OPTIND - 1)) shapefile=\"$*\" if [ ${verbose} == \"true\" ] then echo 'original_projection:' $original_projection echo 'new_projection:' $new_projection echo 'schema:' $schema echo 'table:'$table echo 'shapefile:'$shapefile fi #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data if [ -z \"${new_projection}\" ] then shp2pgsql -s ${original_projection} -d ${shapefile} ${schema}.${table} | psql else shp2pgsql -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql fi In this version, you can call the script from the command line and use it for any shapefile. When called with no arguments it prints out a usage so the user does not have to look into the actual script. It also has a verbose mode for debugging. Here, there are no hard paths. Bad Directory Organization # nfp2/ \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 ada_all.yaml \u251c\u2500\u2500 ada_simple_SAMMER.yaml \u251c\u2500\u2500 Add_null_data.ipynb \u251c\u2500\u2500 Add_null_data.py \u251c\u2500\u2500 all.yaml \u251c\u2500\u2500 assemble_long_data.ipynb \u251c\u2500\u2500 binary_classifer.py \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 BRL_file_generation.ipynb \u251c\u2500\u2500 #BRL.py# \u251c\u2500\u2500 BRL.py \u251c\u2500\u2500 classification.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.py \u251c\u2500\u2500 classifier_t1.ipynb \u251c\u2500\u2500 classifier_t3.py \u251c\u2500\u2500 clique_feature_coprus.p \u251c\u2500\u2500 Clique_Features.ipynb \u251c\u2500\u2500 #Clique_Features.py# \u251c\u2500\u2500 Clique_Features.py \u251c\u2500\u2500 Clustering_Scoring.ipynb \u251c\u2500\u2500 cohort_creation.py \u251c\u2500\u2500 convert_nfp_sas_to_csv.R \u251c\u2500\u2500 corpus.ipynb \u251c\u2500\u2500 create_dropout_files.py \u251c\u2500\u2500 cross_val_copy.py \u251c\u2500\u2500 cross_val.ipynb \u251c\u2500\u2500 cross_val.py \u251c\u2500\u2500 dal_test.ipynb \u251c\u2500\u2500 data_cleaning.ipynb \u251c\u2500\u2500 data_cleaning.py \u251c\u2500\u2500 data_creation_1.yaml \u251c\u2500\u2500 data_creation_2.yaml \u251c\u2500\u2500 data_creation_3.yaml \u251c\u2500\u2500 data_creation_4.yaml \u251c\u2500\u2500 data_creation_and_model_applicaition.py \u251c\u2500\u2500 data_creation_and_model_application_1.yaml \u251c\u2500\u2500 data_creation_and_model_application_2.yaml \u251c\u2500\u2500 data_creation_and_model_application_3.yaml \u251c\u2500\u2500 data_creation_and_model_application_4.yaml \u251c\u2500\u2500 data_creation_and_model_application.yaml \u251c\u2500\u2500 data_creation_for_dropout.py \u251c\u2500\u2500 data_creation.yaml \u251c\u2500\u2500 dataframe.py \u251c\u2500\u2500 datasets.flowingdata.com \u251c\u2500\u2500 data_visualization \u251c\u2500\u2500 data_wrangling \u251c\u2500\u2500 decision_tree.yaml \u251c\u2500\u2500 dropout \u251c\u2500\u2500 dropout_explore.ipynb \u251c\u2500\u2500 experiment.log \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_10_month_to_12_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_12_month_to_14_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_14_month_to_18_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_to_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_4_month_to_6_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_6_month_to_10_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_birth_to_4_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_intake_to_birth.dot \u251c\u2500\u2500 Feature Importance.ipynb \u251c\u2500\u2500 find_model.ipynb \u251c\u2500\u2500 find_model.py \u251c\u2500\u2500 #.gitignore# \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph_code.py \u251c\u2500\u2500 Histrogram_Feature_Creation.ipynb \u251c\u2500\u2500 how_to_merge.txt \u251c\u2500\u2500 Imputation.ipynb \u251c\u2500\u2500 Imputation.py \u251c\u2500\u2500 impute \u251c\u2500\u2500 impute_and_filter-Copy0.ipynb \u251c\u2500\u2500 impute_and_filter-Copy1.ipynb \u251c\u2500\u2500 impute_and_filter.ipynb \u251c\u2500\u2500 impute_and_filter.py \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 I_S_O_M_A_P_ _m_o_m_w_t_g_a_i_n___a_n_d___b_i_r_t_h_g_m_s_2___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___s_u_m___a_n_d___P_R_E_P_G_B_M_I___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___t_i_m_e___M_A_T_E_R_N_A_L___s_u_m.png \u251c\u2500\u2500 Jeff_Models-Copy0.ipynb \u251c\u2500\u2500 Jeff_recipe.txt \u251c\u2500\u2500 #KMS.txt# \u251c\u2500\u2500 KMS.txt \u251c\u2500\u2500 legend.html \u251c\u2500\u2500 load_data.py \u251c\u2500\u2500 media \u251c\u2500\u2500 merge.py \u251c\u2500\u2500 meta_data \u251c\u2500\u2500 metr \u251c\u2500\u2500 metrics_r_f_d.p \u251c\u2500\u2500 metrics_will_drop.p \u251c\u2500\u2500 model_pipeline_2-Copy0.ipynb \u251c\u2500\u2500 model_pipeline_2.ipynb \u251c\u2500\u2500 model_pipeline_2.py \u251c\u2500\u2500 model_pipeline_3.py \u251c\u2500\u2500 #model_pipeline_5.py# \u251c\u2500\u2500 model_pipeline_5.py \u251c\u2500\u2500 model_run_rf.txt \u251c\u2500\u2500 models \u251c\u2500\u2500 model_without_pca.yaml \u251c\u2500\u2500 model_with_pca.yaml \u251c\u2500\u2500 model.yaml \u251c\u2500\u2500 name_change.pl \u251c\u2500\u2500 nbstripout \u251c\u2500\u2500 N_Features.ipynb \u251c\u2500\u2500 nfp2-public \u251c\u2500\u2500 nfpt2.tree \u251c\u2500\u2500 notes \u251c\u2500\u2500 out \u251c\u2500\u2500 out.txt \u251c\u2500\u2500 paralllel_coordinates.ipynb \u251c\u2500\u2500 pickle_files \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 pipeline_demo1.py \u251c\u2500\u2500 PipeLine_Phase1.ipynb \u251c\u2500\u2500 PipeLine_Phase1.py \u251c\u2500\u2500 pipeline_utilities.py \u251c\u2500\u2500 plot_binary.py \u251c\u2500\u2500 plot.yaml \u251c\u2500\u2500 Precision-Recall_curve_across_all_intervals.png \u251c\u2500\u2500 prediction_set_maker-Copy0.ipynb \u251c\u2500\u2500 prediction_set_maker-Copy1.ipynb \u251c\u2500\u2500 prediction_set_maker_for_dropout.ipynb \u251c\u2500\u2500 prediction_set_maker.ipynb \u251c\u2500\u2500 prediction_set_maker.py \u251c\u2500\u2500 Prep for R.ipynb \u251c\u2500\u2500 project-pipeline.dia \u251c\u2500\u2500 pyensemble \u251c\u2500\u2500 #pyliny_report.txt# \u251c\u2500\u2500 pyliny_report.txt \u251c\u2500\u2500 python_to_nb.py \u251c\u2500\u2500 Rafael_weeks.ipynb \u251c\u2500\u2500 RandomForestClassifier_on_interval_1.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_2.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_3.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_4.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_5.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_6.png \u251c\u2500\u2500 R_code \u251c\u2500\u2500 README.md \u251c\u2500\u2500 Receiver_operating_characteristic_curve_across_all_intervals.png \u251c\u2500\u2500 results_pipeline2-Copy0.ipynb \u251c\u2500\u2500 results_pipeline2.html \u251c\u2500\u2500 results_pipeline2.ipynb \u251c\u2500\u2500 results_pipeline2.py \u251c\u2500\u2500 roc_auc_score_across_all_intervals.png \u251c\u2500\u2500 rollin_visit.ipynb \u251c\u2500\u2500 run.sh \u251c\u2500\u2500 run_sklearn_model.py \u251c\u2500\u2500 run_some_pipelines.sh \u251c\u2500\u2500 run_weka.pl \u251c\u2500\u2500 sanity_check_pipeline.ipynb \u251c\u2500\u2500 sanity_check_Rafael_pipeline.ipynb \u251c\u2500\u2500 sarah_a.ipynb \u251c\u2500\u2500 Secondary_feature_gen.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael-Copy0.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.py \u251c\u2500\u2500 secondary_features.ipynb \u251c\u2500\u2500 secondary_features_on_visit_data.ipynb \u251c\u2500\u2500 see_test_model_results.ipynb \u251c\u2500\u2500 sklearn_DT.yaml \u251c\u2500\u2500 sklearn.yaml \u251c\u2500\u2500 sklearn.yaml_bk \u251c\u2500\u2500 Slicer-Copy0.ipynb \u251c\u2500\u2500 Slicer.ipynb \u251c\u2500\u2500 summary_statistics.ipynb \u251c\u2500\u2500 temporal_data_creation_bk.ipynb \u251c\u2500\u2500 temporal_data_creation.ipynb \u251c\u2500\u2500 temporal_data_creation.py \u251c\u2500\u2500 test.d \u251c\u2500\u2500 testing_fiber_2_split.p \u251c\u2500\u2500 test_model.ipynb \u251c\u2500\u2500 test.py \u251c\u2500\u2500 time_based_cross_validation-Rafael.ipynb \u251c\u2500\u2500 time_cv_impute.ipynb \u251c\u2500\u2500 timeline_creation_driver.py \u251c\u2500\u2500 Timeline_Help.ipynb \u251c\u2500\u2500 tree.dot \u251c\u2500\u2500 tr_te_to_head.py \u251c\u2500\u2500 Untitled0.ipynb \u251c\u2500\u2500 Untitled1.ipynb \u251c\u2500\u2500 Untitled2.ipynb \u251c\u2500\u2500 Untitled3.ipynb \u251c\u2500\u2500 Untitled4.ipynb \u251c\u2500\u2500 Untitled5.ipynb \u251c\u2500\u2500 Untitled6.ipynb \u251c\u2500\u2500 Untitled7.ipynb \u251c\u2500\u2500 utils \u251c\u2500\u2500 Weka.ipynb \u251c\u2500\u2500 weka_to_pr_jeff.py \u251c\u2500\u2500 weka_to_pr_raf.py \u251c\u2500\u2500 weka_to_roc.py \u2514\u2500\u2500 weka_to_roc_time.py Good Directory Organization # . \u251c\u2500\u2500 config \u251c\u2500\u2500 descriptive_stats \u2502 \u251c\u2500\u2500 mains_streets_stats \u2502 \u2514\u2500\u2500 water_work_orders \u251c\u2500\u2500 etl \u2502 \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 geology \u2502 \u251c\u2500\u2500 road_ratings \u2502 \u251c\u2500\u2500 soil \u2502 \u251c\u2500\u2500 street_line_data \u2502 \u251c\u2500\u2500 tax_data \u2502 \u251c\u2500\u2500 updated_main_data \u2502 \u251c\u2500\u2500 waterorders \u2502 \u2514\u2500\u2500 water_system \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 log \u251c\u2500\u2500 models_evaluation \u2514\u2500\u2500 results \u2514\u2500\u2500 figures Additional Resources/Inspiration for this Tutorial # 10 Rules for Robust Software Good Enough Practices in Scientific Computing Best Practices for Scientific Computing Reproducible Research SSI","title":"Reproducible software"},{"location":"curriculum/programming_best_practices/reproducible-software/#making-projects-reproducible","text":"Scientific software is often developed and used by a single person. It is all too common in academia to be handed a postdoc or graduate student's old code and be unable to replicate the original study, run the software outside of the original development machine, or even get the software to work at all. The goal of this tutorial is to provide some guidelines to make your summer projects reproducible -- this means your project can be installed on another computer and give the same results you got over the summer. At the end of the summer, your project should be understandable and transferable to your future-self and anyone else who may want to pick up where you left off without having to constantly email you about how to get your project running. (Note: Your future-self doesn't have the luxury of being able to email your past-self).","title":"Making Projects Reproducible"},{"location":"curriculum/programming_best_practices/reproducible-software/#what-is-a-reproducible-project","text":"One that... works for someone other than the original team can be easily installed on another computer has documentation that describes any dependencies and how to install them comes with enough tests to indicate the software is running properly","title":"What is a reproducible project?"},{"location":"curriculum/programming_best_practices/reproducible-software/#readmemdrst","text":"All projects should have a README that communicates the following: What the project is about A short description of the project (i.e. the problem you are trying to solve). The required dependencies to run the software The can be in the form of a requirements.txt file for Python that lists the dependencies and version numbers. The system-level dependencies. Installation instructions How to install your software and associated binaries. This can be in the form of instructions on how to use pip , apt , yum , or some other binary package manager. Example usage The inputs and outputs of your software (i.e. how to use it) with code examples. Attribution/Licensing Who did what and how others can use your software. Examples: - Chicago Food Inspections - DSSG Police EIS - Linux Kernel","title":"README.md(rst)"},{"location":"curriculum/programming_best_practices/reproducible-software/#what-to-do","text":"Use virtual environments . Use automation tools like Make or Drake Keep your directory structure intuitive, interpretable and easy to understand . Keep your database free of \"junk tables.\" Keep only what you need and what's current. Junk tables will only confuse your future-self or others that come fresh to the project. Merge all branches into master. Branches are for adding features or patches. When you have added said feature or patch and you know you won't break the master branch, merge into master and delete the branch. Write commit messages in such a way that your log is helpful (see Git and Github tutorial .) Periodically make database backups . Write unit tests and use continuous integration so you can catch bugs quickly, particularly when you are merging new features into master. (See testing tutorial .) Document all of your functions with docstrings. (See legible, good code tutorial .) Write your python code following the PEP8 standard. (See legible, good code tutorial .) Use (4) spaces instead of tabs in your Python code for indentation.","title":"What to Do"},{"location":"curriculum/programming_best_practices/reproducible-software/#what-not-to-do","text":"Use hard-coded paths . Require Sudo/root privileges to install your project. You can't anticipate whether or not someone will have root access to the machine they are installing your project on, so don't count on it. Additionally, you shouldn't require users to create separate user names for your project. Use non-standard formats for inputs (stick to YAML , XML , JSON , CLA , etc). My one exception to this rule is log files - which you should provide an example of in a README. Otherwise it is easier to just stick with what is already in use. Have a messy repo with random files everywhere . This is confusing, irritating and cancerous to productive enterprise. Commit data or sensitive information like database passcodes to the GitHub repo. Your repository is for your codebase, not the data. Furthermore, your data may be sensitive and need to be protected. Always assume that your repo will be public someday if you are hosting on GitHub (for your DSSG project it will be). Sensitive information also includes architecture decisions about your database. After sensitive information is pushed to GitHub, you cannot remove it completely from the repository. Have code that needs to be operationalized in Jupyter Notebooks. Jupyter notebooks are wonderful for containing your analysis, code and figures in a single document, particularly for doing exploratory analysis. They are not good for keeping the code you will need for your pipeline or code that you will eventually want to turn into a library.","title":"What NOT to Do"},{"location":"curriculum/programming_best_practices/reproducible-software/#virtual-environments","text":"A virtual environment solves the problem that projectX uses version 1.x of a package while projectY uses version 2.x of a package by keeping dependencies in different environments.","title":"Virtual Environments"},{"location":"curriculum/programming_best_practices/reproducible-software/#install-a-virtualenv","text":"pip install --user virtualenv virtualenv dssg-venv --no-site-packages #does not use any global packages You can also install a virtual environment and specify the type of python interpreter you would like to use using the -p option. This is good for keeping Python2 and Python3 dependencies separate. Python2 virtualenv dssg-py2-venv -p $(which python) --no-site-packages Python3 virtualenv dssg-py3-venv -p $(which python3) --no-site-packages","title":"Install a virtualenv"},{"location":"curriculum/programming_best_practices/reproducible-software/#activate-a-virtualenv","text":"source ./dssg-venv/bin/activate","title":"Activate a virtualenv"},{"location":"curriculum/programming_best_practices/reproducible-software/#install-dependencies","text":"pip install -r requirements.txt","title":"Install Dependencies"},{"location":"curriculum/programming_best_practices/reproducible-software/#freeze-dependencies","text":"pip freeze > requirements.txt #outputs a list of dependencies and version numbers Warning : pip freeze will output every package that was installed using pip or setup.py (setuptools). External dependencies that are from github or some other source not found on PyPi will appear but will not be found when trying to reinstall the dependencies. You can include github repositories from github in your requirements.txt file, you just have to do manual housekeeping. Other external dependencies and how to install them should be recorded in your README.md file. Note: There is also the conda environment created by Continuum Analytics. The conda environment handles creating a environment and package dependencies -- what the virtual environment + pip combination does. Conda, unlike pip, includes many non-python dependencies (e.g, MKL) as precompiled binaries that are necessary for scientific python packages. The author is currently of the opinion that if you are a beginner or using a dated OS then using a conda environment is not the worst of ideas. If you are a developer working on a development machine then compile things yourself -- an important and useful skill. Whatever path you choose be consistent about how you set up your environment and document it thoroughly.","title":"Freeze Dependencies"},{"location":"curriculum/programming_best_practices/reproducible-software/#systems-level-dependencies","text":"Systems level dependencies are the libraries installed on your OS. For Ubuntu/Debian Linux you can get a list of them and then install them using the following: #grab systems level dependencies dpkg --get-selections > dependencies.txt #reinstall on a new machine dpkg --clear-selections sudo dpkg --set-selections < dependencies.txt Also courtesy of Tristan Crockett: installing a list of dependencies using apt xargs -a <(awk '/^\\s*[^#]/' dependencies.txt) -r -- sudo apt-get install This will give every package installed on your OS. An easier alternative is to just keep track when you install a new library and manually keep the list in a dependencies.txt file. There are also lightweight vitalization containers like Docker containers, Hyper-V images (Windows), or Ansible playbooks that can be used to \"freeze\" the systems level configuration of an OS.","title":"Systems Level Dependencies"},{"location":"curriculum/programming_best_practices/reproducible-software/#backup-your-database","text":"In PostGreSQL when a table is dropped, it is gone forever. You don't want to drop your results table on the last day of the fellowship, so it is a good idea to backup periodically. To dump your database in PostGreSQL: pg_dump -Fc --schema='raw|clean|models' -N '*public*' --no-acl -v -h <hostname> -U <dbuser> <dbname> > dssg-$(date +%F).dump Note: This can be automated with a crontab script. To restore your database from a dump: < dump_file psql -U dbuser -h dbhost dbname","title":"Backup Your Database"},{"location":"curriculum/programming_best_practices/reproducible-software/#hard-coded-paths","text":"","title":"Hard-coded Paths"},{"location":"curriculum/programming_best_practices/reproducible-software/#example-of-adding-shapefile-with-hard-coded-paths","text":"Hard-coded paths are absolute paths that are native to the machine you are using for development. It is unlikely someone else will keep their data in the exact same directory as you when trying to use your project in a separate environment. Users should be able to set location of files as command line parameters. Below are examples.","title":"Example of Adding Shapefile with hard-coded paths"},{"location":"curriculum/programming_best_practices/reproducible-software/#load_shapefile_hardpath_v1sh","text":"# Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY shp2pgsql -d -s 4267:2261 -d /mnt/data/syracuse/NY_geol_dd soil.geology | psql Although this script documents the command that runs, it has a hard path and the purpose of the arguments are not clear. This script has the shelf-life of a banana.","title":"load_shapefile_hardpath_v1.sh"},{"location":"curriculum/programming_best_practices/reproducible-software/#load_shapefile_hardpath_v2sh","text":"#!/bin/bash # Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY original_projection=4267 new_projection=2261 #projection of Upstate NY schema='soil' table='geology' shapefile='/mnt/data/syracuse/NY_geol_dd/nygeol_poly_dd.shp' #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data shp2pgsql -d -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql With this version someone can better surmise what is being done. Every time you want to load your data you have to change the filename in the script. It also checks if the table already exists in the database so the command can be used to reload data.","title":"load_shapefile_hardpath_v2.sh"},{"location":"curriculum/programming_best_practices/reproducible-software/#load_shapefile_hardpath_v3sh","text":"#!/bin/bash #ETL script for importing shape files. PROGRAM=$(basename $0) usage=\"${PROGRAM} -s schema -t table -p original_projection [-n new_projection] [-v] shapefilename\" function die() { local errmsg=\"$1\" errcode=\"${2:-1}\" echo \"ERROR: ${errmsg}\" exit ${errcode} } #if called with no command line arguments then output usage if [ ${#} -eq 0 ] then echo ${usage} exit 1; fi #-------------------------------------------------- # process input arguments #-------------------------------------------------- verbose=\"false\" new_projection=\"\" while getopts hp:n:s:t:v OPT; do case \"${OPT}\" in h) echo \"${usage}\"; exit 0 ;; p) original_projection=\"${OPTARG}\" ;; n) new_projection=\"${OPTARG}\" ;; s) schema=\"${OPTARG}\" ;; t) table=\"${OPTARG}\" ;; v) verbose=\"true\" ;; ?) die \"unknown option or missing argument; see -h for usage\" 2 ;; esac done shift $((OPTIND - 1)) shapefile=\"$*\" if [ ${verbose} == \"true\" ] then echo 'original_projection:' $original_projection echo 'new_projection:' $new_projection echo 'schema:' $schema echo 'table:'$table echo 'shapefile:'$shapefile fi #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data if [ -z \"${new_projection}\" ] then shp2pgsql -s ${original_projection} -d ${shapefile} ${schema}.${table} | psql else shp2pgsql -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql fi In this version, you can call the script from the command line and use it for any shapefile. When called with no arguments it prints out a usage so the user does not have to look into the actual script. It also has a verbose mode for debugging. Here, there are no hard paths.","title":"load_shapefile_hardpath_v3.sh"},{"location":"curriculum/programming_best_practices/reproducible-software/#bad-directory-organization","text":"nfp2/ \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 ada_all.yaml \u251c\u2500\u2500 ada_simple_SAMMER.yaml \u251c\u2500\u2500 Add_null_data.ipynb \u251c\u2500\u2500 Add_null_data.py \u251c\u2500\u2500 all.yaml \u251c\u2500\u2500 assemble_long_data.ipynb \u251c\u2500\u2500 binary_classifer.py \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 BRL_file_generation.ipynb \u251c\u2500\u2500 #BRL.py# \u251c\u2500\u2500 BRL.py \u251c\u2500\u2500 classification.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.py \u251c\u2500\u2500 classifier_t1.ipynb \u251c\u2500\u2500 classifier_t3.py \u251c\u2500\u2500 clique_feature_coprus.p \u251c\u2500\u2500 Clique_Features.ipynb \u251c\u2500\u2500 #Clique_Features.py# \u251c\u2500\u2500 Clique_Features.py \u251c\u2500\u2500 Clustering_Scoring.ipynb \u251c\u2500\u2500 cohort_creation.py \u251c\u2500\u2500 convert_nfp_sas_to_csv.R \u251c\u2500\u2500 corpus.ipynb \u251c\u2500\u2500 create_dropout_files.py \u251c\u2500\u2500 cross_val_copy.py \u251c\u2500\u2500 cross_val.ipynb \u251c\u2500\u2500 cross_val.py \u251c\u2500\u2500 dal_test.ipynb \u251c\u2500\u2500 data_cleaning.ipynb \u251c\u2500\u2500 data_cleaning.py \u251c\u2500\u2500 data_creation_1.yaml \u251c\u2500\u2500 data_creation_2.yaml \u251c\u2500\u2500 data_creation_3.yaml \u251c\u2500\u2500 data_creation_4.yaml \u251c\u2500\u2500 data_creation_and_model_applicaition.py \u251c\u2500\u2500 data_creation_and_model_application_1.yaml \u251c\u2500\u2500 data_creation_and_model_application_2.yaml \u251c\u2500\u2500 data_creation_and_model_application_3.yaml \u251c\u2500\u2500 data_creation_and_model_application_4.yaml \u251c\u2500\u2500 data_creation_and_model_application.yaml \u251c\u2500\u2500 data_creation_for_dropout.py \u251c\u2500\u2500 data_creation.yaml \u251c\u2500\u2500 dataframe.py \u251c\u2500\u2500 datasets.flowingdata.com \u251c\u2500\u2500 data_visualization \u251c\u2500\u2500 data_wrangling \u251c\u2500\u2500 decision_tree.yaml \u251c\u2500\u2500 dropout \u251c\u2500\u2500 dropout_explore.ipynb \u251c\u2500\u2500 experiment.log \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_10_month_to_12_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_12_month_to_14_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_14_month_to_18_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_to_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_4_month_to_6_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_6_month_to_10_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_birth_to_4_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_intake_to_birth.dot \u251c\u2500\u2500 Feature Importance.ipynb \u251c\u2500\u2500 find_model.ipynb \u251c\u2500\u2500 find_model.py \u251c\u2500\u2500 #.gitignore# \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph_code.py \u251c\u2500\u2500 Histrogram_Feature_Creation.ipynb \u251c\u2500\u2500 how_to_merge.txt \u251c\u2500\u2500 Imputation.ipynb \u251c\u2500\u2500 Imputation.py \u251c\u2500\u2500 impute \u251c\u2500\u2500 impute_and_filter-Copy0.ipynb \u251c\u2500\u2500 impute_and_filter-Copy1.ipynb \u251c\u2500\u2500 impute_and_filter.ipynb \u251c\u2500\u2500 impute_and_filter.py \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 I_S_O_M_A_P_ _m_o_m_w_t_g_a_i_n___a_n_d___b_i_r_t_h_g_m_s_2___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___s_u_m___a_n_d___P_R_E_P_G_B_M_I___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___t_i_m_e___M_A_T_E_R_N_A_L___s_u_m.png \u251c\u2500\u2500 Jeff_Models-Copy0.ipynb \u251c\u2500\u2500 Jeff_recipe.txt \u251c\u2500\u2500 #KMS.txt# \u251c\u2500\u2500 KMS.txt \u251c\u2500\u2500 legend.html \u251c\u2500\u2500 load_data.py \u251c\u2500\u2500 media \u251c\u2500\u2500 merge.py \u251c\u2500\u2500 meta_data \u251c\u2500\u2500 metr \u251c\u2500\u2500 metrics_r_f_d.p \u251c\u2500\u2500 metrics_will_drop.p \u251c\u2500\u2500 model_pipeline_2-Copy0.ipynb \u251c\u2500\u2500 model_pipeline_2.ipynb \u251c\u2500\u2500 model_pipeline_2.py \u251c\u2500\u2500 model_pipeline_3.py \u251c\u2500\u2500 #model_pipeline_5.py# \u251c\u2500\u2500 model_pipeline_5.py \u251c\u2500\u2500 model_run_rf.txt \u251c\u2500\u2500 models \u251c\u2500\u2500 model_without_pca.yaml \u251c\u2500\u2500 model_with_pca.yaml \u251c\u2500\u2500 model.yaml \u251c\u2500\u2500 name_change.pl \u251c\u2500\u2500 nbstripout \u251c\u2500\u2500 N_Features.ipynb \u251c\u2500\u2500 nfp2-public \u251c\u2500\u2500 nfpt2.tree \u251c\u2500\u2500 notes \u251c\u2500\u2500 out \u251c\u2500\u2500 out.txt \u251c\u2500\u2500 paralllel_coordinates.ipynb \u251c\u2500\u2500 pickle_files \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 pipeline_demo1.py \u251c\u2500\u2500 PipeLine_Phase1.ipynb \u251c\u2500\u2500 PipeLine_Phase1.py \u251c\u2500\u2500 pipeline_utilities.py \u251c\u2500\u2500 plot_binary.py \u251c\u2500\u2500 plot.yaml \u251c\u2500\u2500 Precision-Recall_curve_across_all_intervals.png \u251c\u2500\u2500 prediction_set_maker-Copy0.ipynb \u251c\u2500\u2500 prediction_set_maker-Copy1.ipynb \u251c\u2500\u2500 prediction_set_maker_for_dropout.ipynb \u251c\u2500\u2500 prediction_set_maker.ipynb \u251c\u2500\u2500 prediction_set_maker.py \u251c\u2500\u2500 Prep for R.ipynb \u251c\u2500\u2500 project-pipeline.dia \u251c\u2500\u2500 pyensemble \u251c\u2500\u2500 #pyliny_report.txt# \u251c\u2500\u2500 pyliny_report.txt \u251c\u2500\u2500 python_to_nb.py \u251c\u2500\u2500 Rafael_weeks.ipynb \u251c\u2500\u2500 RandomForestClassifier_on_interval_1.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_2.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_3.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_4.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_5.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_6.png \u251c\u2500\u2500 R_code \u251c\u2500\u2500 README.md \u251c\u2500\u2500 Receiver_operating_characteristic_curve_across_all_intervals.png \u251c\u2500\u2500 results_pipeline2-Copy0.ipynb \u251c\u2500\u2500 results_pipeline2.html \u251c\u2500\u2500 results_pipeline2.ipynb \u251c\u2500\u2500 results_pipeline2.py \u251c\u2500\u2500 roc_auc_score_across_all_intervals.png \u251c\u2500\u2500 rollin_visit.ipynb \u251c\u2500\u2500 run.sh \u251c\u2500\u2500 run_sklearn_model.py \u251c\u2500\u2500 run_some_pipelines.sh \u251c\u2500\u2500 run_weka.pl \u251c\u2500\u2500 sanity_check_pipeline.ipynb \u251c\u2500\u2500 sanity_check_Rafael_pipeline.ipynb \u251c\u2500\u2500 sarah_a.ipynb \u251c\u2500\u2500 Secondary_feature_gen.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael-Copy0.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.py \u251c\u2500\u2500 secondary_features.ipynb \u251c\u2500\u2500 secondary_features_on_visit_data.ipynb \u251c\u2500\u2500 see_test_model_results.ipynb \u251c\u2500\u2500 sklearn_DT.yaml \u251c\u2500\u2500 sklearn.yaml \u251c\u2500\u2500 sklearn.yaml_bk \u251c\u2500\u2500 Slicer-Copy0.ipynb \u251c\u2500\u2500 Slicer.ipynb \u251c\u2500\u2500 summary_statistics.ipynb \u251c\u2500\u2500 temporal_data_creation_bk.ipynb \u251c\u2500\u2500 temporal_data_creation.ipynb \u251c\u2500\u2500 temporal_data_creation.py \u251c\u2500\u2500 test.d \u251c\u2500\u2500 testing_fiber_2_split.p \u251c\u2500\u2500 test_model.ipynb \u251c\u2500\u2500 test.py \u251c\u2500\u2500 time_based_cross_validation-Rafael.ipynb \u251c\u2500\u2500 time_cv_impute.ipynb \u251c\u2500\u2500 timeline_creation_driver.py \u251c\u2500\u2500 Timeline_Help.ipynb \u251c\u2500\u2500 tree.dot \u251c\u2500\u2500 tr_te_to_head.py \u251c\u2500\u2500 Untitled0.ipynb \u251c\u2500\u2500 Untitled1.ipynb \u251c\u2500\u2500 Untitled2.ipynb \u251c\u2500\u2500 Untitled3.ipynb \u251c\u2500\u2500 Untitled4.ipynb \u251c\u2500\u2500 Untitled5.ipynb \u251c\u2500\u2500 Untitled6.ipynb \u251c\u2500\u2500 Untitled7.ipynb \u251c\u2500\u2500 utils \u251c\u2500\u2500 Weka.ipynb \u251c\u2500\u2500 weka_to_pr_jeff.py \u251c\u2500\u2500 weka_to_pr_raf.py \u251c\u2500\u2500 weka_to_roc.py \u2514\u2500\u2500 weka_to_roc_time.py","title":"Bad Directory Organization"},{"location":"curriculum/programming_best_practices/reproducible-software/#good-directory-organization","text":". \u251c\u2500\u2500 config \u251c\u2500\u2500 descriptive_stats \u2502 \u251c\u2500\u2500 mains_streets_stats \u2502 \u2514\u2500\u2500 water_work_orders \u251c\u2500\u2500 etl \u2502 \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 geology \u2502 \u251c\u2500\u2500 road_ratings \u2502 \u251c\u2500\u2500 soil \u2502 \u251c\u2500\u2500 street_line_data \u2502 \u251c\u2500\u2500 tax_data \u2502 \u251c\u2500\u2500 updated_main_data \u2502 \u251c\u2500\u2500 waterorders \u2502 \u2514\u2500\u2500 water_system \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 log \u251c\u2500\u2500 models_evaluation \u2514\u2500\u2500 results \u2514\u2500\u2500 figures","title":"Good Directory Organization"},{"location":"curriculum/programming_best_practices/reproducible-software/#additional-resourcesinspiration-for-this-tutorial","text":"10 Rules for Robust Software Good Enough Practices in Scientific Computing Best Practices for Scientific Computing Reproducible Research SSI","title":"Additional Resources/Inspiration for this Tutorial"},{"location":"curriculum/programming_best_practices/test-test-test/","text":"Software testing # Slides from 2016 DSSG You've finished your wonderful web prototpe for the Data Fest, your model is awesome and that visualizaiton is going to blow everyone's minds... You get on stage for a live demo and open the webapp only to find that nothing works. Spooky. To prevent that from happening, it's important to write tests. Think about it, when you write a function, when do you know it works \u2122? You probably test it against some ad hoc inputs and, if you get the proper result, you move on. Maybe your function does indeed work today, but what if in the following weeks you change a few lines of the function to make it faster, but you don't want to spend time testing again ( it works \u2122)? Maybe you just introduced a bug and didn't notice. For that reason, it's important to write automated tests to check your code and make sure that everything works consistently , and that you can identify bugs introduced by future changes. I didn't choose the test life, the test life chose me # This tutorial is divided in three parts. The first one addresses unit testing , which is a simple (but powerful) form of testing. The second part is devoted specifically to testing Data Science pipelines. The last section covers Continuous Integration (or CI). CI is a technique to automatically run all of your tests every time you push new code to Github. CI is language-agnostic, but our tutorial specifically addresses Python, because it is the language we use the most. Part 1: Unit Testing with Python (interactive tutorial), How I Learned to Stop Worrying and Love Unit Testing (teachout slides - Kat Rasch ) Part 2: Testing Python Data Science Pipelines Part 3: Continuous Integration External Resources # Unit testing for Data Science - slides from Hanna (2016 DSSG fellow) Unit testing and logging for Data Science - Ori Cohen Best Testing Practices for Data Science - Eric J. Ma Database(Data) Testing Tutorial with Sample TestCases","title":"Writing tests"},{"location":"curriculum/programming_best_practices/test-test-test/#software-testing","text":"Slides from 2016 DSSG You've finished your wonderful web prototpe for the Data Fest, your model is awesome and that visualizaiton is going to blow everyone's minds... You get on stage for a live demo and open the webapp only to find that nothing works. Spooky. To prevent that from happening, it's important to write tests. Think about it, when you write a function, when do you know it works \u2122? You probably test it against some ad hoc inputs and, if you get the proper result, you move on. Maybe your function does indeed work today, but what if in the following weeks you change a few lines of the function to make it faster, but you don't want to spend time testing again ( it works \u2122)? Maybe you just introduced a bug and didn't notice. For that reason, it's important to write automated tests to check your code and make sure that everything works consistently , and that you can identify bugs introduced by future changes.","title":"Software testing"},{"location":"curriculum/programming_best_practices/test-test-test/#i-didnt-choose-the-test-life-the-test-life-chose-me","text":"This tutorial is divided in three parts. The first one addresses unit testing , which is a simple (but powerful) form of testing. The second part is devoted specifically to testing Data Science pipelines. The last section covers Continuous Integration (or CI). CI is a technique to automatically run all of your tests every time you push new code to Github. CI is language-agnostic, but our tutorial specifically addresses Python, because it is the language we use the most. Part 1: Unit Testing with Python (interactive tutorial), How I Learned to Stop Worrying and Love Unit Testing (teachout slides - Kat Rasch ) Part 2: Testing Python Data Science Pipelines Part 3: Continuous Integration","title":"I didn't choose the test life, the test life chose me"},{"location":"curriculum/programming_best_practices/test-test-test/#external-resources","text":"Unit testing for Data Science - slides from Hanna (2016 DSSG fellow) Unit testing and logging for Data Science - Ori Cohen Best Testing Practices for Data Science - Eric J. Ma Database(Data) Testing Tutorial with Sample TestCases","title":"External Resources"},{"location":"curriculum/programming_best_practices/test-test-test/ci/","text":"Continuous integration # Tests are (ideally) cheap to run, so it makes sense to run them every time your code changes. In Part 1 , we ran a test suite using py.test . Running our test suite locally every time your project changes is a good practice, but it's important to also test it on a completely different machine (maybe some of your tests are passing because of local configuration and you don't even know). Furthermore, when working in teams someone may forget to run the tests locally breaking the project. Continuous integration helps you identify those cases (and with proper configuration even reject commits that break the build) just after a push is made. If you are working on an open source project, you can get Continuous Integration for free using Travis CI . In the following sections we'll see how to setup Travis to run your tests every time you push to Github. Using Travis CI # The first step is to create an account and link it to your Github profile. Travis is integrated with Github and requires minimal setup, you just need to create a .travis.yml file (note the leading dot) in your root folder, then go to Travis and activate the repo. Let's see how a .travis.yml file looks like. Travis configuration file for testing scientific Python projects # The content of your configuration file completely depends on your project, but given that most DSSG (if not all) teams use Python + Numpy + Scipy, the following file will work for you with minimal changes. The contents of the file are straightforward and tell Travis how to build your project and run the test suite. language : python python : # list the python version you want to check your tests on - \"2.7\" - \"3.5\" sudo : required install : # this are requirements to install many scentific python packages # such as numpy/scipy, you cannot install them with pip # so we need to use the system package manager - \"sudo apt-get install gfortran python-liblas libblas-dev liblapack-dev libatlas-dev\" # as of June 2, 2016, travis has an outdated version of pip, # this may cause trouble when installing some packages such as sci-kit learn # updating pip before using 'pip install' solves those issues - \"pip install --upgrade pip\" # install specific requirements your test suite uses # e.g. pytest, nose, mock # make sure you have the requirements.txt in your repo's root folder - \"pip install -r requirements.txt\" script : # steps needed to run your scripts, for simple projects # this may be just 'py.test' or 'nosetests', depending on # which library you use - py.test Once Travis is configured it will run your tests every time you push (this is the default configuration but you can change it if you want) and if your tests don't pass it will send you and e-mail (awesome!), you can also see the log to check which tests didn't pass in the Travis website. When configuring Travis you may encounter some issues, feel free to open an issue on this repo if that happens so we can help you out. Project samples using Travis + Scientific Python # Police project sklearn-evaluation - See this if you want to make tests that compare images since it requires a different setup","title":"Continuous integration"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#continuous-integration","text":"Tests are (ideally) cheap to run, so it makes sense to run them every time your code changes. In Part 1 , we ran a test suite using py.test . Running our test suite locally every time your project changes is a good practice, but it's important to also test it on a completely different machine (maybe some of your tests are passing because of local configuration and you don't even know). Furthermore, when working in teams someone may forget to run the tests locally breaking the project. Continuous integration helps you identify those cases (and with proper configuration even reject commits that break the build) just after a push is made. If you are working on an open source project, you can get Continuous Integration for free using Travis CI . In the following sections we'll see how to setup Travis to run your tests every time you push to Github.","title":"Continuous integration"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#using-travis-ci","text":"The first step is to create an account and link it to your Github profile. Travis is integrated with Github and requires minimal setup, you just need to create a .travis.yml file (note the leading dot) in your root folder, then go to Travis and activate the repo. Let's see how a .travis.yml file looks like.","title":"Using Travis CI"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#travis-configuration-file-for-testing-scientific-python-projects","text":"The content of your configuration file completely depends on your project, but given that most DSSG (if not all) teams use Python + Numpy + Scipy, the following file will work for you with minimal changes. The contents of the file are straightforward and tell Travis how to build your project and run the test suite. language : python python : # list the python version you want to check your tests on - \"2.7\" - \"3.5\" sudo : required install : # this are requirements to install many scentific python packages # such as numpy/scipy, you cannot install them with pip # so we need to use the system package manager - \"sudo apt-get install gfortran python-liblas libblas-dev liblapack-dev libatlas-dev\" # as of June 2, 2016, travis has an outdated version of pip, # this may cause trouble when installing some packages such as sci-kit learn # updating pip before using 'pip install' solves those issues - \"pip install --upgrade pip\" # install specific requirements your test suite uses # e.g. pytest, nose, mock # make sure you have the requirements.txt in your repo's root folder - \"pip install -r requirements.txt\" script : # steps needed to run your scripts, for simple projects # this may be just 'py.test' or 'nosetests', depending on # which library you use - py.test Once Travis is configured it will run your tests every time you push (this is the default configuration but you can change it if you want) and if your tests don't pass it will send you and e-mail (awesome!), you can also see the log to check which tests didn't pass in the Travis website. When configuring Travis you may encounter some issues, feel free to open an issue on this repo if that happens so we can help you out.","title":"Travis configuration file for testing scientific Python projects"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#project-samples-using-travis-scientific-python","text":"Police project sklearn-evaluation - See this if you want to make tests that compare images since it requires a different setup","title":"Project samples using Travis + Scientific Python"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/","text":"Testing Python Data Science pipelines # Testing Data Pipelines is hard . By definition you do not expect them to produce the same output over and over again. There isn't a standard way of doing it (or at least I don't know it), but here you'll find some tips to test your pipeline steps. Tips to test your pipeline # Separate the source code and your pipeline steps in different modules # As it was mentioned in Part1 . It's important to separate the source code from your pipeline. Think of source code as the building blocks for processing data, uploading to the database, training models and so on. For example, the training step in your pipeline may look like this: # training step in your pipeline # load features train , test = load_features ([ 'feature1' , 'feature2' , 'feature3' ]) # get a list of models to train to_train = load_models ( 'RandomForest' , 'LogisticRegression' ) # train every model for model in to_train : # fit the model model . fit ( train . y , train . y ) # evaluate some metrics on the test set, # save results in a database, # create HTML/PDF reports evaluate_model ( model , test ) In the training step above load_features , load_models and evaluate_model depend on your project, maybe you are loading data from PostgresSQL, maybe from HDF5. The models you train also depend on your project and the evaluation depends on which metrics are best suited for your project's goal. Those functions are building blocks and the source code for those should be outside your training script. Probably the load_features function does a lot of data transformations, try to divide it in various small functions and run unit tests on them. Make the data assumptions in your code explicit # When working on your pipeline the math/logic may work fine, but what if you are feeding the wrong data? Imagine what would happen if you train a classifier and you forgot to delete the outcome variable in your feature set, that sounds like a dumb mistake, but as your pipeline gets more and more complex, it can happen . To prevent those things from happening you should test your pipeline at runtime , meaning that you should check for red flags while training models. Let's see an example. def load_features ( list_of_features ): \"\"\" This function loads features from the database \"\"\" uri = load_uri () con = db_connection ( uri ) tables = load_from_db ( list_of_features , con ) if 'outcome' in tables : raise Exception ( 'You cannot load the outcome variable as a feature' ) if any ( tables , has_nas ): raise Exception ( 'You cannot load features with NAs' ) return tables In the snippet above we are making two assumptions explicit, the first one is that we shouldn't load a column named 'outcome' in our feature set, the second one means that we cannot load columns with NAs, because this may break the following steps in our pipeline. Test your code with a fake database # Imagine that you implemented a function to perform some complex feature engineering in your database, then you modify some parts to make it faster and you have a test to check that the results hold the same. If your database has millions of rows and a couple hundred features, how long is the test going to take? When testing code that interacts with a lot of data is often a good idea to sample it and put it in a test database, that way you can test faster, but don't force yourself to make your tests run fast. Always remember a fast test is better than slow test, but a slow test is better than not testing at all . How do you change which database your code uses? There are a couple of ways, you can for example define an environment variable to change the behavior of your open_db_connection function. Note: use environmental variables judiciously and don't store any sensitive data. # db.py import os def db_connection (): if os . environ [ 'testing_mode' ]: return connect_to_testing_db () else : return connect_to_production_db () Now, you need to add some custom logic to the script that runs your tests, let's see how to do it: # run_tests.sh export testing_mode = YES py.test # run your tests export testing_mode = NO Dealing with randomness # The hardest part of testing pipelines is dealing with randomness, how do you test a random generator function? or a probabilistic function? One of the simplest ways to do it is to take out the randomness by setting the random seed in your tests. Let's see an example. # random.py def generate_random_number_between ( a , b ): # generate random number using seed value, # a and b return number # test_random.py # set the seed value so you always get random numbers in the same order # during the tests set_random_seed ( 0 ) def test_generate_random_number_below_10 (): assert generate_random_number_between ( 0 , 10 ) == 5 def test_generate_random_number_between_8_12 (): assert generate_random_number_between ( 8 , 12 ) == 10 The example above is a bit naive, but it hopefully gives you an idea on how to take out the randomness by setting the seed, let's see a more robust example. Another approach is to test your function enough number and check the result against an interval and not an specific value. Let's see how to test a function that draws one sample from the normal distribution: # normal_sample.py def normal_dist_sample ( mean = 0 , std = 1 ): # do stuff return sample # test_normal_sample.py import numpy.testing as npt def test_normal_dist_sample_mean (): # draw 1000 samples samples = [ normal_dist_sample () for i in range ( 10000 )] # calculate the mean mean = samples . mean () # check that the mean is almost equal to zero assert npt . assert_almost_equal ( mean , 0 ) As you can see, testing probabilistic code is not trivial, so do it only when your project highly depends on such functions. But make sure you write unit tests and to check the assumptions in your data! Tools for testing # hypothesis - Python library to look for edge cases without explicitly coding them engarde - Use Python decorators to test a function outcome (this project had good potential but it's dead now) feature forge - Testing features for ML models (also seems dead) External resources # Testing for Data Scientists by Trey Causey Where to go from here # Part 3: Continuous Integration","title":"Testing Python Data Science pipelines"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#testing-python-data-science-pipelines","text":"Testing Data Pipelines is hard . By definition you do not expect them to produce the same output over and over again. There isn't a standard way of doing it (or at least I don't know it), but here you'll find some tips to test your pipeline steps.","title":"Testing Python Data Science pipelines"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#tips-to-test-your-pipeline","text":"","title":"Tips to test your pipeline"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#separate-the-source-code-and-your-pipeline-steps-in-different-modules","text":"As it was mentioned in Part1 . It's important to separate the source code from your pipeline. Think of source code as the building blocks for processing data, uploading to the database, training models and so on. For example, the training step in your pipeline may look like this: # training step in your pipeline # load features train , test = load_features ([ 'feature1' , 'feature2' , 'feature3' ]) # get a list of models to train to_train = load_models ( 'RandomForest' , 'LogisticRegression' ) # train every model for model in to_train : # fit the model model . fit ( train . y , train . y ) # evaluate some metrics on the test set, # save results in a database, # create HTML/PDF reports evaluate_model ( model , test ) In the training step above load_features , load_models and evaluate_model depend on your project, maybe you are loading data from PostgresSQL, maybe from HDF5. The models you train also depend on your project and the evaluation depends on which metrics are best suited for your project's goal. Those functions are building blocks and the source code for those should be outside your training script. Probably the load_features function does a lot of data transformations, try to divide it in various small functions and run unit tests on them.","title":"Separate the source code and your pipeline steps in different modules"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#make-the-data-assumptions-in-your-code-explicit","text":"When working on your pipeline the math/logic may work fine, but what if you are feeding the wrong data? Imagine what would happen if you train a classifier and you forgot to delete the outcome variable in your feature set, that sounds like a dumb mistake, but as your pipeline gets more and more complex, it can happen . To prevent those things from happening you should test your pipeline at runtime , meaning that you should check for red flags while training models. Let's see an example. def load_features ( list_of_features ): \"\"\" This function loads features from the database \"\"\" uri = load_uri () con = db_connection ( uri ) tables = load_from_db ( list_of_features , con ) if 'outcome' in tables : raise Exception ( 'You cannot load the outcome variable as a feature' ) if any ( tables , has_nas ): raise Exception ( 'You cannot load features with NAs' ) return tables In the snippet above we are making two assumptions explicit, the first one is that we shouldn't load a column named 'outcome' in our feature set, the second one means that we cannot load columns with NAs, because this may break the following steps in our pipeline.","title":"Make the data assumptions in your code explicit"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#test-your-code-with-a-fake-database","text":"Imagine that you implemented a function to perform some complex feature engineering in your database, then you modify some parts to make it faster and you have a test to check that the results hold the same. If your database has millions of rows and a couple hundred features, how long is the test going to take? When testing code that interacts with a lot of data is often a good idea to sample it and put it in a test database, that way you can test faster, but don't force yourself to make your tests run fast. Always remember a fast test is better than slow test, but a slow test is better than not testing at all . How do you change which database your code uses? There are a couple of ways, you can for example define an environment variable to change the behavior of your open_db_connection function. Note: use environmental variables judiciously and don't store any sensitive data. # db.py import os def db_connection (): if os . environ [ 'testing_mode' ]: return connect_to_testing_db () else : return connect_to_production_db () Now, you need to add some custom logic to the script that runs your tests, let's see how to do it: # run_tests.sh export testing_mode = YES py.test # run your tests export testing_mode = NO","title":"Test your code with a fake database"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#dealing-with-randomness","text":"The hardest part of testing pipelines is dealing with randomness, how do you test a random generator function? or a probabilistic function? One of the simplest ways to do it is to take out the randomness by setting the random seed in your tests. Let's see an example. # random.py def generate_random_number_between ( a , b ): # generate random number using seed value, # a and b return number # test_random.py # set the seed value so you always get random numbers in the same order # during the tests set_random_seed ( 0 ) def test_generate_random_number_below_10 (): assert generate_random_number_between ( 0 , 10 ) == 5 def test_generate_random_number_between_8_12 (): assert generate_random_number_between ( 8 , 12 ) == 10 The example above is a bit naive, but it hopefully gives you an idea on how to take out the randomness by setting the seed, let's see a more robust example. Another approach is to test your function enough number and check the result against an interval and not an specific value. Let's see how to test a function that draws one sample from the normal distribution: # normal_sample.py def normal_dist_sample ( mean = 0 , std = 1 ): # do stuff return sample # test_normal_sample.py import numpy.testing as npt def test_normal_dist_sample_mean (): # draw 1000 samples samples = [ normal_dist_sample () for i in range ( 10000 )] # calculate the mean mean = samples . mean () # check that the mean is almost equal to zero assert npt . assert_almost_equal ( mean , 0 ) As you can see, testing probabilistic code is not trivial, so do it only when your project highly depends on such functions. But make sure you write unit tests and to check the assumptions in your data!","title":"Dealing with randomness"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#tools-for-testing","text":"hypothesis - Python library to look for edge cases without explicitly coding them engarde - Use Python decorators to test a function outcome (this project had good potential but it's dead now) feature forge - Testing features for ML models (also seems dead)","title":"Tools for testing"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#external-resources","text":"Testing for Data Scientists by Trey Causey","title":"External resources"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#where-to-go-from-here","text":"Part 3: Continuous Integration","title":"Where to go from here"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/","text":"Testing Your Python Projects # Your Very First Python Test # Testing in Python is fairly straightforward. Here's an example: Say we wrote a function get_answer , which should return 42 , the answer to life, the universe and everything. (In the real world, the test and the code to be tested would live in different files, but we'll keep them together for simplicity here.) # function to test def get_answer (): return 42 # actual test def test_answer_to_life_is_42 (): assert get_answer () == 42 This is pretty simple. The function is just your everyday Python function, and the test is a separate function that calls the function get_answer . The interesting part here is the keyword assert , which evaluates a condition. If the condition evaluates to True , we say that the test passed ; if it returns False , we say the test failed . There are many types of tests. The case above is called a unit test , which comes from the notion that we are testing a unit of code (not the entire project). The idea is to have one unit test for each unit of code (e.g. a function). Software testing is an entire subject on its own, but keep it simple for now and follow these guidelines when writing your tests: Your tests should check that one thing works (a function for example) Your tests should be independent of other tests (the outcome of one should not affect others) Given the same input, your test should always return the result (when testing a Data Science pipeline this gets tricky given its probabilistic nature) Running Tests with py.test # One of the best tools for testing in Python is pytest , which provides some useful features to reduce the amount of boilerplate code for your tests, as well as a command to automatically find Python files with tests and run them. You can install it with pip : pip install pytest If you want to actually run the test above, you need to get a copy of this repo. git clone https://github.com/dssg/hitchhikers-guide cd hitchhikers-guide To run your tests (note the dot in the middle): py.test pytest will look for all the tests in your project. In our case there's a copy of our test in a file called test_meaning.py on this folder. The output should look like this: ==================== test session starts =========================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: stuff/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_life.py . ==================== 1 passed in 0.01 seconds ======================= Now, imagine that we accidentally modify our function to: # function to test def get_answer (): return 41 If we run py.test again, we'll see the following: ======================== test session starts ==================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: /Users/Edu/development/dsapp/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_meaning.py F ============================== FAILURES ======================== __________________ test_answer_to_file_is_42 __________________ def test_answer_to_file_is_42(): > assert get_answer() == 42 E assert 41 == 42 E + where 41 = get_answer() test_meaning.py:8: AssertionError ===================== 1 failed in 0.02 seconds ================== We can see from the output that our test failed. Now, every time you modify your code just run py.test to make sure your code still works! ( But remember: just because you tested something doesn't necessarily mean it works. You can keep adding new tests as you identify new edge cases and errors). Where to Store Your Tests # The are no strict rules on where to store your tests. In a Data Science project, you are going to have a lot of folders with code for many tasks (e.g. ETL, modeling). The first thing to take into account is to separate your pipeline steps from your source code: simply speaking, source code is functions and classes that you want to reuse in various steps of your pipeline (e.g. creating a database connection). Let's see an example to make this clear: . \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 documentation_here.txt \u251c\u2500\u2500 etl \u2502 \u2514\u2500\u2500 code_to_load_to_db.txt \u251c\u2500\u2500 evaluation \u2502 \u2514\u2500\u2500 code_to_evaluate_models_here.txt \u251c\u2500\u2500 exploration \u2502 \u2514\u2500\u2500 jupyter_notebooks_with_cool_plots_here.txt \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 code_to_generate_features_here.txt \u251c\u2500\u2500 lib \u2502 \u251c\u2500\u2500 lib \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 db \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 load.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 process.py \u2502 \u2502 \u2514\u2500\u2500 util.py \u2502 \u2514\u2500\u2500 tests \u2502 \u251c\u2500\u2500 test_db.py \u2502 \u2514\u2500\u2500 test_util.py \u2514\u2500\u2500 model \u2514\u2500\u2500 code_to_train_models_here.txt In the diagram above, you can see the different steps in your pipeline (ETL, exploration, feature generation, modeling, model evaluation) all those folders will contain a mix of Python, shell and SQL scripts. Then, there's another folder called lib , which stores the source code for this project. Inside such a folder, you'll find another two folders lib and tests , the first one stores the actual source code and the later stores the tests for the code inside lib . Of course, that doesn't mean you should limit your tests to your source code! You should also test your pipeline, but a well designed pipeline will put the complicated parts in the source code, so your pipeline steps are short and simple. The problem with testing a pipeline is that some steps won't be deterministic, but there are some things you can do. Tip: To access the code in lib you can either create a setup.py file to install or add the folder to your PYTHONPATH . # Other Python Testing Tools # unittest or unitest2 if you use Python 3 (part of the Python standard library) nose - another good option to run your tests Where to go From Here # Part 2: Testing Python Data Science pipelines Read the pytest documentation","title":"Testing Your Python Projects"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#testing-your-python-projects","text":"","title":"Testing Your Python Projects"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#your-very-first-python-test","text":"Testing in Python is fairly straightforward. Here's an example: Say we wrote a function get_answer , which should return 42 , the answer to life, the universe and everything. (In the real world, the test and the code to be tested would live in different files, but we'll keep them together for simplicity here.) # function to test def get_answer (): return 42 # actual test def test_answer_to_life_is_42 (): assert get_answer () == 42 This is pretty simple. The function is just your everyday Python function, and the test is a separate function that calls the function get_answer . The interesting part here is the keyword assert , which evaluates a condition. If the condition evaluates to True , we say that the test passed ; if it returns False , we say the test failed . There are many types of tests. The case above is called a unit test , which comes from the notion that we are testing a unit of code (not the entire project). The idea is to have one unit test for each unit of code (e.g. a function). Software testing is an entire subject on its own, but keep it simple for now and follow these guidelines when writing your tests: Your tests should check that one thing works (a function for example) Your tests should be independent of other tests (the outcome of one should not affect others) Given the same input, your test should always return the result (when testing a Data Science pipeline this gets tricky given its probabilistic nature)","title":"Your Very First Python Test"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#running-tests-with-pytest","text":"One of the best tools for testing in Python is pytest , which provides some useful features to reduce the amount of boilerplate code for your tests, as well as a command to automatically find Python files with tests and run them. You can install it with pip : pip install pytest If you want to actually run the test above, you need to get a copy of this repo. git clone https://github.com/dssg/hitchhikers-guide cd hitchhikers-guide To run your tests (note the dot in the middle): py.test pytest will look for all the tests in your project. In our case there's a copy of our test in a file called test_meaning.py on this folder. The output should look like this: ==================== test session starts =========================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: stuff/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_life.py . ==================== 1 passed in 0.01 seconds ======================= Now, imagine that we accidentally modify our function to: # function to test def get_answer (): return 41 If we run py.test again, we'll see the following: ======================== test session starts ==================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: /Users/Edu/development/dsapp/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_meaning.py F ============================== FAILURES ======================== __________________ test_answer_to_file_is_42 __________________ def test_answer_to_file_is_42(): > assert get_answer() == 42 E assert 41 == 42 E + where 41 = get_answer() test_meaning.py:8: AssertionError ===================== 1 failed in 0.02 seconds ================== We can see from the output that our test failed. Now, every time you modify your code just run py.test to make sure your code still works! ( But remember: just because you tested something doesn't necessarily mean it works. You can keep adding new tests as you identify new edge cases and errors).","title":"Running Tests with py.test"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#where-to-store-your-tests","text":"The are no strict rules on where to store your tests. In a Data Science project, you are going to have a lot of folders with code for many tasks (e.g. ETL, modeling). The first thing to take into account is to separate your pipeline steps from your source code: simply speaking, source code is functions and classes that you want to reuse in various steps of your pipeline (e.g. creating a database connection). Let's see an example to make this clear: . \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 documentation_here.txt \u251c\u2500\u2500 etl \u2502 \u2514\u2500\u2500 code_to_load_to_db.txt \u251c\u2500\u2500 evaluation \u2502 \u2514\u2500\u2500 code_to_evaluate_models_here.txt \u251c\u2500\u2500 exploration \u2502 \u2514\u2500\u2500 jupyter_notebooks_with_cool_plots_here.txt \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 code_to_generate_features_here.txt \u251c\u2500\u2500 lib \u2502 \u251c\u2500\u2500 lib \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 db \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 load.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 process.py \u2502 \u2502 \u2514\u2500\u2500 util.py \u2502 \u2514\u2500\u2500 tests \u2502 \u251c\u2500\u2500 test_db.py \u2502 \u2514\u2500\u2500 test_util.py \u2514\u2500\u2500 model \u2514\u2500\u2500 code_to_train_models_here.txt In the diagram above, you can see the different steps in your pipeline (ETL, exploration, feature generation, modeling, model evaluation) all those folders will contain a mix of Python, shell and SQL scripts. Then, there's another folder called lib , which stores the source code for this project. Inside such a folder, you'll find another two folders lib and tests , the first one stores the actual source code and the later stores the tests for the code inside lib . Of course, that doesn't mean you should limit your tests to your source code! You should also test your pipeline, but a well designed pipeline will put the complicated parts in the source code, so your pipeline steps are short and simple. The problem with testing a pipeline is that some steps won't be deterministic, but there are some things you can do. Tip: To access the code in lib you can either create a setup.py file to install or add the folder to your PYTHONPATH .","title":"Where to Store Your Tests"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#other-python-testing-tools","text":"unittest or unitest2 if you use Python 3 (part of the Python standard library) nose - another good option to run your tests","title":"Other Python Testing Tools"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#where-to-go-from-here","text":"Part 2: Testing Python Data Science pipelines Read the pytest documentation","title":"Where to go From Here"},{"location":"curriculum/scoping/deliverables/","text":"Deliverables # Project Scope Project Charter Descriptive Stats Data Stories User Interface Mockup Technical Report Poster 5 minute Presentation (for non technical audiences) 20-30 minute presentation (for a conference or meetup) Github Repository (private with possibly sensitive data) Cleaned up public github Repo Resources # Deliverables checklist Technical report template Poster template Example slides","title":"Project deliverables"},{"location":"curriculum/scoping/deliverables/#deliverables","text":"Project Scope Project Charter Descriptive Stats Data Stories User Interface Mockup Technical Report Poster 5 minute Presentation (for non technical audiences) 20-30 minute presentation (for a conference or meetup) Github Repository (private with possibly sensitive data) Cleaned up public github Repo","title":"Deliverables"},{"location":"curriculum/scoping/deliverables/#resources","text":"Deliverables checklist Technical report template Poster template Example slides","title":"Resources"},{"location":"curriculum/scoping/dme/","text":"Data Maturity Evaluation # Resources # Data Maturity Framework","title":"Data Maturity evaluation"},{"location":"curriculum/scoping/dme/#data-maturity-evaluation","text":"","title":"Data Maturity Evaluation"},{"location":"curriculum/scoping/dme/#resources","text":"Data Maturity Framework","title":"Resources"},{"location":"curriculum/scoping/overview/","text":"Overview # Resources # Data Science Project Scoping Guide Blank Project Scoping Worksheet Filled-out Project Scoping Worksheet","title":"Scoping overview"},{"location":"curriculum/scoping/overview/#overview","text":"","title":"Overview"},{"location":"curriculum/scoping/overview/#resources","text":"Data Science Project Scoping Guide Blank Project Scoping Worksheet Filled-out Project Scoping Worksheet","title":"Resources"},{"location":"curriculum/scoping/project_workflow/","text":"","title":"Project workflow"},{"location":"curriculum/setup/good_repos/","text":"","title":"Good repos"},{"location":"curriculum/setup/machines/","text":"","title":"Machines"},{"location":"curriculum/setup/command-line-tools/","text":"Command Line Tools # Motivation # As data scientists, we often receive data in text-based files. We need to explore these files to understand what they contain, we need to manipulate and clean them and we need to handle them on our file system. The most robust way to do this, even with large files, is the command line. Command line tools are the data scientist's swiss army knife. They are versitile, portable, and have plenty of functions that you may not quite sure how to use, but you're sure they'll be useful at some point. From helping you obtain, clean, and explore your data, to helping you build models and manager your workflow, command line tools are essential to every well-built data science pipeline, will be used throughout DSSG, and should be your starting point as you build your data science toolkit. Slides # Here's the presentation from a previous workshop. The basics # Where am I? # pwd print working directory - this prints the name of the current working directory cd .. changes directory to one level/folder up cd ~/ goes to the home directory cd - return to the previous directory What's in my folder? # ls lists the contents in your current dictory. ls -l \"long listing\" format ( -l ) shows the filesize, date of last change, and file permissions ls -la \"long listing\" format ( -l ), shows all files ( -a ) including hidden dotfiles tree lists the contents of the current directory and all sub-directories as a tree structure (great for peeking into folder structures!) tree -L 2 limits the tree expansion to 2 levels tree -hs shows file sizes ( -s ) in human-readable format ( -h ) What's in my file? # head -n10 $f shows the \"head\" of the file, in this case the top 10 lines tail -n10 $f shows the \"tail\" of the file tail -n10 $f | watch -n1 watches the tail of the file for any changes every second ( -n1 ) tail -f -n10 $f follows ( -f ) the tail of the file every time it changes, useful if you are checking the log of a running program less $f paginated viewer for the contents of a text file wc $f counts words, lines and characters in a file (separate counts using -w or -l or -c ) Where is my file? # find -name \"<lost_file_name>\" -type f finds files by name find -name \"<lost_dir_name>\" -type d finds directories by name Renaming files # Rename files with rename . For example, to replace all space bars with underscores: rename 's/ /_/g' space\\ bars\\ .txt This command substitutes ( s ) space bars ( / / ) for underscores ( /_/ ) in the entire file name (globally, g ). (The 3 slashes can be replaced by any sequence of 3 characters, so 's# #_#g' would also work and can sometimes be more legible, for example when you need to escape a special character with a backslash.) You can replace multiple characters at a time by using a simple logical OR \"regular expression\" ( | ) such as [ |?] which will replace every space bar or question mark. rename 's/[ |?]/_/g' space\\ bars?.txt (The file will be renamed to space_bars_.txt ) Bonus points: rename 'y/A-Z/a-z/' renames files to all-lowercase rename 'y/a-z/A-Z/' renames files to all-uppercase Some useful things to know # Be careful what you wish for, the command line is very powerful, it will do exactly what you ask. This can be dangerous when you're running commands like rm (remove), or mv (move). You can \"echo\" your commands to just print the command text without actually running the command. Use tab completion to type commands faster and find filenames, press the tab key whilst typing to see suggestions tab Prepend man to a command to read the manual for example man rm You can use ctrl + r to search the command line history, and search for previously searched commands. Or type history to see the history. Beware of spaces when creating filenames, this is not generally good practice, if you must you can use the \\ escape character to add blank spaces in a file name. For example touch space\\ bars\\ .txt , if you run touch space bars .txt this will create three files space , bars , and .txt . Have a look into using screen or tmux for keeping processes alive and working with multiple terminals (see further reading living-in-the-terminal ). Use htop for monitoring the usage of your instance ( usage guide ). Have a go at learning the basics of vim , since it is ubiquitous on unix servers (see further reading living-in-the-terminal ). Check out some tutorials on regular expressions if you are not already familiar with them. Command Line for Data Science - Let's talk about the weather # As an exercise, let's take a shot at creating our own weather predictions using data from NOAA. You can find daily data from 2016 for the US here: ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz (The documentation is here ) Getting Data from the Command Line # First we have to get the data. For that we're going to use curl. $ curl ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Whoa! Terminal is going crazy! This may impress your less savvy friends, but it's not going to help you answer your question. We need to stop this process. Try control-c. This is the universal escape command in terminal. We obviously didn't use curl right. Let's look up the manual for the command using man . $ man curl Looks like if we want to write this to a file, we've got to pass the -O argument. $ curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Let's check to see if it worked. $ ls -lah Great. Now we need to know the file format so we know what tool to use to unpack it. $ file 2016.csv.gz Looks like it's a gzip so we'll have to use gunzip . $ gunzip 2016.csv.gz $ ls -lah Now we've got a .csv file we can start playing with. Let's see how big it is using wc Viewing Data from the Command Line # The simpilest streaming command is cat . This dumps the whole file, line by line, into standard out and prints. $ cat 2016.csv That's a bit much. Let's see if we can slow it down by viewing the file page by page using more or less . $ less 2016.csv Great. But let's say I just want to see the top of the file to get a sense of it's structure. We can use head for that. $ head 2016.csv $ head -n 3 2016.csv Similarly, if I'm only interested in viewing the end of the file, I can use tail . $ tail 2016.csv These commands all print things out raw and bunched together. I want to take advantage of the fact that I know this is a csv to get a prettier view of the data. This is where csvkit starts to shine. The first command we'll use from csvkit is csvlook . $ csvlook 2016.csv But that's everything again. We just want to see the top. If only we could take the output from head and send it to csvlook . We can! It's called piping , and you do it like this: head 2016.csv | csvlook The output from head was sent to csvlook for processing. Piping and redirection (more on that later) are two of the most important concepts to keep in mind when using command line tools. Because most commands use text as the interface, you can chain commands together to create simple and powerful data processing pipelines! Filtering Data from the Command Line # It looks like in order for us to make sense of the weather dataset, we're going to need to figure out what these station numbers mean. Let's grab the station dictionary from NOAA and take a look at it. $ curl -O https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt $ head ghcnd-stations.txt Looks like the station description might come in handy. We want to look at just the stations in Chicago. $ grep CHICAGO ghcnd-stations.txt | csvlook -H Let's pick OHARE as the station we'll use for now. Its ID is 'USW00094846' Let's take a look at just the ID column from the weather file. We can do this using cut . $ cut -f 1 2016.csv Looks like cut isn't smart enough to know that we're using a csv. We can either use csvcut, or pass a delimiter argument that specifies comma. $ cut -d , -f 1 2016.csv | head Now let's filter out just the oberservations from OHARE. $ cut -d , -f 1 2016.csv | grep USW00094846 | head Another powerful tool that can do filtering (and much more) is awk . awk treats every file as a set of row-based records and allows you to create contition/{action} pairs for the records in the file. The default {action} in awk is to print the records that meet the condition. Let's try reproducing the above statement using awk . $ cut -d , -f 1 2016.csv | awk '/USW00094846/' | head awk requires familiarity with regular expressions for contitions and has its own language for actions, so man and stack overflow will be your friends if you want to go deep with awk . Editing and Transforming Data # Let's say we want to replace values in the files. PRCP is confusing. Let's change PRCP to RAIN. To do this, we use sed . sed stands for streaming editor, and is very useful for editing large text files because it doesn't have to load all the data into memory to make changes. Here's how we can use sed to replace a string. $ sed s/PRCP/RAIN/ 2016.csv | head Notice the strings have changed! But when we look at the source file $ head 2016.csv Noting has changed. That's because we didn't write it to a file. In fact, none of the changes we've made have. $ sed s/PRCP/RAIN/ 2016.csv > 2016_clean.csv $ head 2016_clean.csv We can also use awk for subsitution, but this time, let's replace \"WSFM\" with \"WINDSPEED\" in all the weather files in the directory. Once again, stackoverflow is your friend here. $ ls -la > files.txt $ awk '$9 ~/2016*/ {gsub(/WSFM/, \"WINDSPEED\"); print;}' files.txt Group Challenges # For group challenges, log onto the training ec2 instance and change directories to /mnt/data/training/yourusername. This should be your working directory for all the excercises. Create a final weather file that just has weather data from OHARE airport for days when it rained, and change PRCP to RAIN. Save the sequence of commands to a shell script so it's replicable by your teammate and push to a training repository you've created on github. Create a separate file with just the weather from OHARE for days when the tempurature was above 70 degrees F. (hint: try using csvgrep to filter a specific column on a range of values) Get ready to explore the relationship between weather and crime in Chicago. Using crime data from 2016 (below), parse the json and convert it to a csv. Explore the fields and cut the dataset down to just day, location, and crime type. Then subset the dataset to just homicides and save as a new file. https://data.cityofchicago.org/resource/6zsd-86xi.json Using just command line tools, can you use the lat and long coordinates of the weather stations to rapidly identify which weather station is closest to the DSSG building? Cheat Sheet # We're going to cover a variety of command line tools that help us obtain, parse, scrub, and explore your data. (The first few steps toward being an OSEMN data scientist). Here's a list of commands and concepts we'll cover: Getting to know you: navigating files and directories in the command line cd mkdir ls file mv cp rm findit (bonus) Getting, unpacking, and parsing Data curl wget (bonus) gunzip tar (bonus) in2csv json2csv (bonus) Exploring Data wc cat less head tail csvlook Filtering, Slicing, and Transforming grep cut sed awk csvgrep csvcut csvjoin (bonus) jq (bonus; sed for JSON) Exploring & Summarizing csvstat Writing shell scripts Further Resources # Jeroen Janssens wrote the book literally on data science in the command line. Also, check out his post on 7 essential command line tools for data scientists. For command line basics, Learning CLI the Hard Way is, as always, a great resource. Potential Teachouts # tmux : Getting your command line organized tmux is a great way to manage many environments at once. Give it a shot!","title":"Command line intro"},{"location":"curriculum/setup/command-line-tools/#command-line-tools","text":"","title":"Command Line Tools"},{"location":"curriculum/setup/command-line-tools/#motivation","text":"As data scientists, we often receive data in text-based files. We need to explore these files to understand what they contain, we need to manipulate and clean them and we need to handle them on our file system. The most robust way to do this, even with large files, is the command line. Command line tools are the data scientist's swiss army knife. They are versitile, portable, and have plenty of functions that you may not quite sure how to use, but you're sure they'll be useful at some point. From helping you obtain, clean, and explore your data, to helping you build models and manager your workflow, command line tools are essential to every well-built data science pipeline, will be used throughout DSSG, and should be your starting point as you build your data science toolkit.","title":"Motivation"},{"location":"curriculum/setup/command-line-tools/#slides","text":"Here's the presentation from a previous workshop.","title":"Slides"},{"location":"curriculum/setup/command-line-tools/#the-basics","text":"","title":"The basics"},{"location":"curriculum/setup/command-line-tools/#where-am-i","text":"pwd print working directory - this prints the name of the current working directory cd .. changes directory to one level/folder up cd ~/ goes to the home directory cd - return to the previous directory","title":"Where am I?"},{"location":"curriculum/setup/command-line-tools/#whats-in-my-folder","text":"ls lists the contents in your current dictory. ls -l \"long listing\" format ( -l ) shows the filesize, date of last change, and file permissions ls -la \"long listing\" format ( -l ), shows all files ( -a ) including hidden dotfiles tree lists the contents of the current directory and all sub-directories as a tree structure (great for peeking into folder structures!) tree -L 2 limits the tree expansion to 2 levels tree -hs shows file sizes ( -s ) in human-readable format ( -h )","title":"What's in my folder?"},{"location":"curriculum/setup/command-line-tools/#whats-in-my-file","text":"head -n10 $f shows the \"head\" of the file, in this case the top 10 lines tail -n10 $f shows the \"tail\" of the file tail -n10 $f | watch -n1 watches the tail of the file for any changes every second ( -n1 ) tail -f -n10 $f follows ( -f ) the tail of the file every time it changes, useful if you are checking the log of a running program less $f paginated viewer for the contents of a text file wc $f counts words, lines and characters in a file (separate counts using -w or -l or -c )","title":"What's in my file?"},{"location":"curriculum/setup/command-line-tools/#where-is-my-file","text":"find -name \"<lost_file_name>\" -type f finds files by name find -name \"<lost_dir_name>\" -type d finds directories by name","title":"Where is my file?"},{"location":"curriculum/setup/command-line-tools/#renaming-files","text":"Rename files with rename . For example, to replace all space bars with underscores: rename 's/ /_/g' space\\ bars\\ .txt This command substitutes ( s ) space bars ( / / ) for underscores ( /_/ ) in the entire file name (globally, g ). (The 3 slashes can be replaced by any sequence of 3 characters, so 's# #_#g' would also work and can sometimes be more legible, for example when you need to escape a special character with a backslash.) You can replace multiple characters at a time by using a simple logical OR \"regular expression\" ( | ) such as [ |?] which will replace every space bar or question mark. rename 's/[ |?]/_/g' space\\ bars?.txt (The file will be renamed to space_bars_.txt ) Bonus points: rename 'y/A-Z/a-z/' renames files to all-lowercase rename 'y/a-z/A-Z/' renames files to all-uppercase","title":"Renaming files"},{"location":"curriculum/setup/command-line-tools/#some-useful-things-to-know","text":"Be careful what you wish for, the command line is very powerful, it will do exactly what you ask. This can be dangerous when you're running commands like rm (remove), or mv (move). You can \"echo\" your commands to just print the command text without actually running the command. Use tab completion to type commands faster and find filenames, press the tab key whilst typing to see suggestions tab Prepend man to a command to read the manual for example man rm You can use ctrl + r to search the command line history, and search for previously searched commands. Or type history to see the history. Beware of spaces when creating filenames, this is not generally good practice, if you must you can use the \\ escape character to add blank spaces in a file name. For example touch space\\ bars\\ .txt , if you run touch space bars .txt this will create three files space , bars , and .txt . Have a look into using screen or tmux for keeping processes alive and working with multiple terminals (see further reading living-in-the-terminal ). Use htop for monitoring the usage of your instance ( usage guide ). Have a go at learning the basics of vim , since it is ubiquitous on unix servers (see further reading living-in-the-terminal ). Check out some tutorials on regular expressions if you are not already familiar with them.","title":"Some useful things to know"},{"location":"curriculum/setup/command-line-tools/#command-line-for-data-science-lets-talk-about-the-weather","text":"As an exercise, let's take a shot at creating our own weather predictions using data from NOAA. You can find daily data from 2016 for the US here: ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz (The documentation is here )","title":"Command Line for Data Science - Let's talk about the weather"},{"location":"curriculum/setup/command-line-tools/#getting-data-from-the-command-line","text":"First we have to get the data. For that we're going to use curl. $ curl ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Whoa! Terminal is going crazy! This may impress your less savvy friends, but it's not going to help you answer your question. We need to stop this process. Try control-c. This is the universal escape command in terminal. We obviously didn't use curl right. Let's look up the manual for the command using man . $ man curl Looks like if we want to write this to a file, we've got to pass the -O argument. $ curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Let's check to see if it worked. $ ls -lah Great. Now we need to know the file format so we know what tool to use to unpack it. $ file 2016.csv.gz Looks like it's a gzip so we'll have to use gunzip . $ gunzip 2016.csv.gz $ ls -lah Now we've got a .csv file we can start playing with. Let's see how big it is using wc","title":"Getting Data from the Command Line"},{"location":"curriculum/setup/command-line-tools/#viewing-data-from-the-command-line","text":"The simpilest streaming command is cat . This dumps the whole file, line by line, into standard out and prints. $ cat 2016.csv That's a bit much. Let's see if we can slow it down by viewing the file page by page using more or less . $ less 2016.csv Great. But let's say I just want to see the top of the file to get a sense of it's structure. We can use head for that. $ head 2016.csv $ head -n 3 2016.csv Similarly, if I'm only interested in viewing the end of the file, I can use tail . $ tail 2016.csv These commands all print things out raw and bunched together. I want to take advantage of the fact that I know this is a csv to get a prettier view of the data. This is where csvkit starts to shine. The first command we'll use from csvkit is csvlook . $ csvlook 2016.csv But that's everything again. We just want to see the top. If only we could take the output from head and send it to csvlook . We can! It's called piping , and you do it like this: head 2016.csv | csvlook The output from head was sent to csvlook for processing. Piping and redirection (more on that later) are two of the most important concepts to keep in mind when using command line tools. Because most commands use text as the interface, you can chain commands together to create simple and powerful data processing pipelines!","title":"Viewing Data from the Command Line"},{"location":"curriculum/setup/command-line-tools/#filtering-data-from-the-command-line","text":"It looks like in order for us to make sense of the weather dataset, we're going to need to figure out what these station numbers mean. Let's grab the station dictionary from NOAA and take a look at it. $ curl -O https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt $ head ghcnd-stations.txt Looks like the station description might come in handy. We want to look at just the stations in Chicago. $ grep CHICAGO ghcnd-stations.txt | csvlook -H Let's pick OHARE as the station we'll use for now. Its ID is 'USW00094846' Let's take a look at just the ID column from the weather file. We can do this using cut . $ cut -f 1 2016.csv Looks like cut isn't smart enough to know that we're using a csv. We can either use csvcut, or pass a delimiter argument that specifies comma. $ cut -d , -f 1 2016.csv | head Now let's filter out just the oberservations from OHARE. $ cut -d , -f 1 2016.csv | grep USW00094846 | head Another powerful tool that can do filtering (and much more) is awk . awk treats every file as a set of row-based records and allows you to create contition/{action} pairs for the records in the file. The default {action} in awk is to print the records that meet the condition. Let's try reproducing the above statement using awk . $ cut -d , -f 1 2016.csv | awk '/USW00094846/' | head awk requires familiarity with regular expressions for contitions and has its own language for actions, so man and stack overflow will be your friends if you want to go deep with awk .","title":"Filtering Data from the Command Line"},{"location":"curriculum/setup/command-line-tools/#editing-and-transforming-data","text":"Let's say we want to replace values in the files. PRCP is confusing. Let's change PRCP to RAIN. To do this, we use sed . sed stands for streaming editor, and is very useful for editing large text files because it doesn't have to load all the data into memory to make changes. Here's how we can use sed to replace a string. $ sed s/PRCP/RAIN/ 2016.csv | head Notice the strings have changed! But when we look at the source file $ head 2016.csv Noting has changed. That's because we didn't write it to a file. In fact, none of the changes we've made have. $ sed s/PRCP/RAIN/ 2016.csv > 2016_clean.csv $ head 2016_clean.csv We can also use awk for subsitution, but this time, let's replace \"WSFM\" with \"WINDSPEED\" in all the weather files in the directory. Once again, stackoverflow is your friend here. $ ls -la > files.txt $ awk '$9 ~/2016*/ {gsub(/WSFM/, \"WINDSPEED\"); print;}' files.txt","title":"Editing and Transforming Data"},{"location":"curriculum/setup/command-line-tools/#group-challenges","text":"For group challenges, log onto the training ec2 instance and change directories to /mnt/data/training/yourusername. This should be your working directory for all the excercises. Create a final weather file that just has weather data from OHARE airport for days when it rained, and change PRCP to RAIN. Save the sequence of commands to a shell script so it's replicable by your teammate and push to a training repository you've created on github. Create a separate file with just the weather from OHARE for days when the tempurature was above 70 degrees F. (hint: try using csvgrep to filter a specific column on a range of values) Get ready to explore the relationship between weather and crime in Chicago. Using crime data from 2016 (below), parse the json and convert it to a csv. Explore the fields and cut the dataset down to just day, location, and crime type. Then subset the dataset to just homicides and save as a new file. https://data.cityofchicago.org/resource/6zsd-86xi.json Using just command line tools, can you use the lat and long coordinates of the weather stations to rapidly identify which weather station is closest to the DSSG building?","title":"Group Challenges"},{"location":"curriculum/setup/command-line-tools/#cheat-sheet","text":"We're going to cover a variety of command line tools that help us obtain, parse, scrub, and explore your data. (The first few steps toward being an OSEMN data scientist). Here's a list of commands and concepts we'll cover: Getting to know you: navigating files and directories in the command line cd mkdir ls file mv cp rm findit (bonus) Getting, unpacking, and parsing Data curl wget (bonus) gunzip tar (bonus) in2csv json2csv (bonus) Exploring Data wc cat less head tail csvlook Filtering, Slicing, and Transforming grep cut sed awk csvgrep csvcut csvjoin (bonus) jq (bonus; sed for JSON) Exploring & Summarizing csvstat Writing shell scripts","title":"Cheat Sheet"},{"location":"curriculum/setup/command-line-tools/#further-resources","text":"Jeroen Janssens wrote the book literally on data science in the command line. Also, check out his post on 7 essential command line tools for data scientists. For command line basics, Learning CLI the Hard Way is, as always, a great resource.","title":"Further Resources"},{"location":"curriculum/setup/command-line-tools/#potential-teachouts","text":"tmux : Getting your command line organized tmux is a great way to manage many environments at once. Give it a shot!","title":"Potential Teachouts"},{"location":"curriculum/setup/command-line-tools/cmdline/","text":"CommandLine, Git, and GitHub Tutorial (DSSG 2018) # Introduction # You are most likely comfortable interacting with a computer using point-and-click interfaces, also known as GUIs , or Graphical User Interfaces . But there is another way that came before and will always be: the CLI , or Command Line Interface . The command line interface allows you to communicate with your computer more directly than you can through a GUI, in a REPL , or read-evaluate-print loop format: The user (you!) types a command, then presses the return key. The computer reads the command, evaluates it, and prints out the output. This term comes from the days when we had to use physical printers called \"teleprinters\" to interact with computers. The software you use to communicate with your computer is known as a shell , because it acts as a \"shell\" from the underlying complexity of the operating system. The most popular shell is known as BASH, the Bourne Again Shell . Bash is the default shell for most UNIX systems and Macs. You can also look into more exotic shells like Xonsh, Zsh, plumbum, and fish. I personally like xonsh and zsh. So, why use the command line instead of a GUI? There are several reasons: In the shell you can easily handle data by chaining programs into a cohesive pipeline to handle large amounts of data. You can automate your workflow through scripting to save time and be more productive. A good rule of thumb is if you do something twice -- script it! Your work will be much more reproducible because there will be a digital record of the steps you took to produce your results compared to point and clicking in a program like Excel. If you're working with remote machines, like supercomputers or machines in the cloud, the CLI is often the only interface available. The CLI is usually much faster -- once you get the hang out of it. The learning objectives of this mini-tutorial are the following: Learn how to navigate a UNIX file system. Learn some basic shell commands. Learn how to use to use git and github. The commandline has always been and always will be. Navigating the File System # Now we'll learn how to navigate though a UNIX file system and how to create , edit , rename and copy files and directories within the system. First, a little background: UNIX has a hierarchical (tree-structure) directory organization known as the file system . In the file system our data is organized into files , which are organized in directories . The base of the directory is called the root directory and denoted by a / . All user-available disk space is combined into a single directory under '/'. Here is an example of the directory tree of a typical Linux system. From this figure, we can see all directories are under the root directory ( / ). The folders under the root directory contain information for the configuration and operation of the operating system, so you shouldn't change these (unless you really know what you are doing). The special folder home contains the files for all users. In this example, we have the directories rick , anna , emmy , bob , which contain files created by users rick, anna, emmy, and bob, respectively. To navigate through the file system, or change directories , we use the cd command. If you just type cd , without any arguements it will take you to your home directory. To see where you are within the file system, you use the pwd , or print working directory command: $ cd $ pwd /home/akumar You can also check who is logged into the machine with the whoami command. $ whoami akumar In my case, my home directory is located at /home/akumar , because as we saw above using the whoami command, my username is akumar . Yours will be something different. /home/akumar/ is a path ; in this case, the path leads to my home directory. A path can also be a path to a file name, for example /home/akumar/Documents/justice_league_meetings_notes.txt . Paths that start with / are called absolute paths , because they begin at the root directory. They're absolute because they start at the root (the beginning of the filesystem), so they will always be the same regardless of your current location . Relative paths start at your current location. So the relative path to the file above from my home directory ( /home/akumar ) would be Documents/justice_leage_meetings_notes.txt . Note that / has two meanings: To signify an absolute path (originating in the root directory), and to separate directories and files in the path. Let's assume that I (user akumar ) have the following directory data: To list all the files and subdirectories in a given directory, we can type ls : $ ls data Documents Try the command in your terminal. We see that the contents of akumar 's home directory are two directories called data and Documents , as we'd expect. If we want to see the contents of one of these directories, we can use the same ls command, but specify the directory whose contents we'd like to see as an argument to the ls command.: $ ls Documents/DailyPlanet_Articles superman_saves_the_day.txt Note that this would have the same output as running: $ cd Documents/DailyPlanet_Articles $ ls superman_saves_the_day.txt where we moved into the DailyPlanet_Articles directory and ran the ls command from within the directory. Say we've navigated to the DailyPlanet_Articles directory, and now we want to navigate to the data directory. There are multiple ways to do this: We could use the absolute path: cd /home/akumar/data/ (remember, that will work from anywhere in the filesystem), or we could use a relative path. But how do we use a relative path when data is not in our current directory, and we have to go up the tree to reach our destination? We can use the shorthand . . One . means our current directory, and .. means move up one directory relative to where we currently are in the filesystem: First, we would navigate to the DailyPlant_Articles directory: cd /home/akumar/Documents/DailyPlant_Articles Now let move up two directories into the data directory. cd ../../data So we move up one directory from DailyPlanet_Articles to Documents , then up one more directory to /home/akumar , then down one directory to data . Note: When you start typing in the path the data directory, you can use TAB to auto-complete . The TAB button is your friend on the command-line. If you ever get lost in the file system, remember that the command cd (without specifying another directory) will always drop you into your home directory. $ cd $ pwd /home/akumar Creating, Editing, Moving, Copying, and Deleting Files # Now let's put the UNIX file system to work for us. Let's get started in our home directory: $ cp <source-file> <destination-file> #copy file $ mv <source-file> <destination-file> #move file $ rm <source-file> <destination-file> #remove a file $ find ~/ -name '*.txt' #find all text files in my home directory Note: There is no Recycle Bin or Trash rm can be a weapon of mass destruction Using Git # Git is a version control system that allows you to take snapshots of your project so you can go back in time or safely add features without breaking your code. This is much much better than taking turns working on software and emailing versions to eachother. Git can be used for any project involving plain text files, including code, books, papers, small datasets. Git and GitHub are used for hosting, distributing, and collaborating on a project with others. Through tools like GitHub issues, GitHub Pull Requests, and branches you can manage large scale collaborations. An example of an open-source project here at DSaPP is Triage or Aequitas. Guidelines to keep in mind for effective collaboration # You are a team, work as a team. You are physically located with your teammate so actually work together. Know what eachother is doing. Collaborate, code review (and learn), Pair Program People come before process (most of the time). Say we are starting a new project called \"where_not_to_eat\". About the chicago food inspections. We are going to combine data from different years into a single CSV. First we are going to clone a project using git . Link to the project on github. $ git clone <add link to repo here> We are going to use the github flow for this project. First we want to configure our git profile. # See how your git config looks git config --list Set your workspace # Adding some if you dont have a user.name or user.email git config --global user.name \"Rayid Ghani\" git config --global user.email \"rayid.ghani@dssg.io\" git config --global color.ui \"auto\" git config --global core.editor 'vim' git config --global push.default current Different work flows: Solo Style mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" # hack # more hack GitHub Style Also know as the [[http://endoflineblog.com/gitflow-considered-harmful][/Anti-gitflow/]] [[https://guides.github.com/introduction/flow/][Github Flow]] (explained with images and animation!) Don't code anything if there isn't a need for it. First create good issues. A good issue is clear defined output actionable should only take a few days at most good: fix this bug , add this method (good to write in the imperative voice) bad: it doesn't work for me , finish the project Now do a git pull to fetch any changes in the remote repository and merge into your repository. $ git pull Now create your own \"branch\" of the project where you can make changes separate from the master branch. The master branch should always be pristine. $ git checkout -b <username>-branch Alternativly, you can name branches with the issue number of the issue you are working on (e.g., add_features_issue#44). Now we do a little hacking where we are going to write a bash script that will clean the header of a CSV file and concatenate all the files. Then we are going to push our changes to the remote repo. And create a pull request. Class Exercise # Download data via the commandline and clean it, concatenate files so it can be read into a database. First clone the project: Then create your own branch: $ git checkout -b <username>-branch Link to download data: https://github.com/avishekrk/where_not_to_eat/archive/master.zip Here is my solution: wget https://github.com/avishekrk/where_not_to_eat/archive/master.zip ; unzip master.zip ; rm -v master.zip mv -v where_not_to_eat-master raw cd ./raw/ pwd find -name \"* *\" | while read f ; do echo ${ f } ; new = $( echo $f | sed \"s/ /_/g\" ) ; echo ${ new } ; mv -v \" $f \" $new ; done mkdir -v ./../staging head -1 food_inspection_2018-01-01.csv | tr '[:upper:]' '[:lower:]' | sed -e \"s/#//g\" -e \"s/ ,/,/g\" -e \"s/ /_/g\" -e s \"/^,//g\" > header cat header > all_inspections.csv for f in food_inspection_2018-0*.csv ; do echo ${ f } ; awk 'NR > 1 {print}' ${ f } >> all_inspections.csv ; done mv -v all_inspections.csv ./../staging After saving our script we can commit our changes on our branch: git add clean.sh git commit -m \"Added script to clean and concatenate files git push #push to our remote repository Then you can create a Pull Request.","title":"CommandLine, Git, and GitHub Tutorial (DSSG 2018)"},{"location":"curriculum/setup/command-line-tools/cmdline/#commandline-git-and-github-tutorial-dssg-2018","text":"","title":"CommandLine, Git, and GitHub Tutorial (DSSG 2018)"},{"location":"curriculum/setup/command-line-tools/cmdline/#introduction","text":"You are most likely comfortable interacting with a computer using point-and-click interfaces, also known as GUIs , or Graphical User Interfaces . But there is another way that came before and will always be: the CLI , or Command Line Interface . The command line interface allows you to communicate with your computer more directly than you can through a GUI, in a REPL , or read-evaluate-print loop format: The user (you!) types a command, then presses the return key. The computer reads the command, evaluates it, and prints out the output. This term comes from the days when we had to use physical printers called \"teleprinters\" to interact with computers. The software you use to communicate with your computer is known as a shell , because it acts as a \"shell\" from the underlying complexity of the operating system. The most popular shell is known as BASH, the Bourne Again Shell . Bash is the default shell for most UNIX systems and Macs. You can also look into more exotic shells like Xonsh, Zsh, plumbum, and fish. I personally like xonsh and zsh. So, why use the command line instead of a GUI? There are several reasons: In the shell you can easily handle data by chaining programs into a cohesive pipeline to handle large amounts of data. You can automate your workflow through scripting to save time and be more productive. A good rule of thumb is if you do something twice -- script it! Your work will be much more reproducible because there will be a digital record of the steps you took to produce your results compared to point and clicking in a program like Excel. If you're working with remote machines, like supercomputers or machines in the cloud, the CLI is often the only interface available. The CLI is usually much faster -- once you get the hang out of it. The learning objectives of this mini-tutorial are the following: Learn how to navigate a UNIX file system. Learn some basic shell commands. Learn how to use to use git and github. The commandline has always been and always will be.","title":"Introduction"},{"location":"curriculum/setup/command-line-tools/cmdline/#navigating-the-file-system","text":"Now we'll learn how to navigate though a UNIX file system and how to create , edit , rename and copy files and directories within the system. First, a little background: UNIX has a hierarchical (tree-structure) directory organization known as the file system . In the file system our data is organized into files , which are organized in directories . The base of the directory is called the root directory and denoted by a / . All user-available disk space is combined into a single directory under '/'. Here is an example of the directory tree of a typical Linux system. From this figure, we can see all directories are under the root directory ( / ). The folders under the root directory contain information for the configuration and operation of the operating system, so you shouldn't change these (unless you really know what you are doing). The special folder home contains the files for all users. In this example, we have the directories rick , anna , emmy , bob , which contain files created by users rick, anna, emmy, and bob, respectively. To navigate through the file system, or change directories , we use the cd command. If you just type cd , without any arguements it will take you to your home directory. To see where you are within the file system, you use the pwd , or print working directory command: $ cd $ pwd /home/akumar You can also check who is logged into the machine with the whoami command. $ whoami akumar In my case, my home directory is located at /home/akumar , because as we saw above using the whoami command, my username is akumar . Yours will be something different. /home/akumar/ is a path ; in this case, the path leads to my home directory. A path can also be a path to a file name, for example /home/akumar/Documents/justice_league_meetings_notes.txt . Paths that start with / are called absolute paths , because they begin at the root directory. They're absolute because they start at the root (the beginning of the filesystem), so they will always be the same regardless of your current location . Relative paths start at your current location. So the relative path to the file above from my home directory ( /home/akumar ) would be Documents/justice_leage_meetings_notes.txt . Note that / has two meanings: To signify an absolute path (originating in the root directory), and to separate directories and files in the path. Let's assume that I (user akumar ) have the following directory data: To list all the files and subdirectories in a given directory, we can type ls : $ ls data Documents Try the command in your terminal. We see that the contents of akumar 's home directory are two directories called data and Documents , as we'd expect. If we want to see the contents of one of these directories, we can use the same ls command, but specify the directory whose contents we'd like to see as an argument to the ls command.: $ ls Documents/DailyPlanet_Articles superman_saves_the_day.txt Note that this would have the same output as running: $ cd Documents/DailyPlanet_Articles $ ls superman_saves_the_day.txt where we moved into the DailyPlanet_Articles directory and ran the ls command from within the directory. Say we've navigated to the DailyPlanet_Articles directory, and now we want to navigate to the data directory. There are multiple ways to do this: We could use the absolute path: cd /home/akumar/data/ (remember, that will work from anywhere in the filesystem), or we could use a relative path. But how do we use a relative path when data is not in our current directory, and we have to go up the tree to reach our destination? We can use the shorthand . . One . means our current directory, and .. means move up one directory relative to where we currently are in the filesystem: First, we would navigate to the DailyPlant_Articles directory: cd /home/akumar/Documents/DailyPlant_Articles Now let move up two directories into the data directory. cd ../../data So we move up one directory from DailyPlanet_Articles to Documents , then up one more directory to /home/akumar , then down one directory to data . Note: When you start typing in the path the data directory, you can use TAB to auto-complete . The TAB button is your friend on the command-line. If you ever get lost in the file system, remember that the command cd (without specifying another directory) will always drop you into your home directory. $ cd $ pwd /home/akumar","title":"Navigating the File System"},{"location":"curriculum/setup/command-line-tools/cmdline/#creating-editing-moving-copying-and-deleting-files","text":"Now let's put the UNIX file system to work for us. Let's get started in our home directory: $ cp <source-file> <destination-file> #copy file $ mv <source-file> <destination-file> #move file $ rm <source-file> <destination-file> #remove a file $ find ~/ -name '*.txt' #find all text files in my home directory Note: There is no Recycle Bin or Trash rm can be a weapon of mass destruction","title":"Creating, Editing, Moving, Copying, and Deleting Files"},{"location":"curriculum/setup/command-line-tools/cmdline/#using-git","text":"Git is a version control system that allows you to take snapshots of your project so you can go back in time or safely add features without breaking your code. This is much much better than taking turns working on software and emailing versions to eachother. Git can be used for any project involving plain text files, including code, books, papers, small datasets. Git and GitHub are used for hosting, distributing, and collaborating on a project with others. Through tools like GitHub issues, GitHub Pull Requests, and branches you can manage large scale collaborations. An example of an open-source project here at DSaPP is Triage or Aequitas.","title":"Using Git"},{"location":"curriculum/setup/command-line-tools/cmdline/#guidelines-to-keep-in-mind-for-effective-collaboration","text":"You are a team, work as a team. You are physically located with your teammate so actually work together. Know what eachother is doing. Collaborate, code review (and learn), Pair Program People come before process (most of the time). Say we are starting a new project called \"where_not_to_eat\". About the chicago food inspections. We are going to combine data from different years into a single CSV. First we are going to clone a project using git . Link to the project on github. $ git clone <add link to repo here> We are going to use the github flow for this project. First we want to configure our git profile. # See how your git config looks git config --list Set your workspace # Adding some if you dont have a user.name or user.email git config --global user.name \"Rayid Ghani\" git config --global user.email \"rayid.ghani@dssg.io\" git config --global color.ui \"auto\" git config --global core.editor 'vim' git config --global push.default current Different work flows: Solo Style mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" # hack # more hack GitHub Style Also know as the [[http://endoflineblog.com/gitflow-considered-harmful][/Anti-gitflow/]] [[https://guides.github.com/introduction/flow/][Github Flow]] (explained with images and animation!) Don't code anything if there isn't a need for it. First create good issues. A good issue is clear defined output actionable should only take a few days at most good: fix this bug , add this method (good to write in the imperative voice) bad: it doesn't work for me , finish the project Now do a git pull to fetch any changes in the remote repository and merge into your repository. $ git pull Now create your own \"branch\" of the project where you can make changes separate from the master branch. The master branch should always be pristine. $ git checkout -b <username>-branch Alternativly, you can name branches with the issue number of the issue you are working on (e.g., add_features_issue#44). Now we do a little hacking where we are going to write a bash script that will clean the header of a CSV file and concatenate all the files. Then we are going to push our changes to the remote repo. And create a pull request.","title":"Guidelines to keep in mind for effective collaboration"},{"location":"curriculum/setup/command-line-tools/cmdline/#class-exercise","text":"Download data via the commandline and clean it, concatenate files so it can be read into a database. First clone the project: Then create your own branch: $ git checkout -b <username>-branch Link to download data: https://github.com/avishekrk/where_not_to_eat/archive/master.zip Here is my solution: wget https://github.com/avishekrk/where_not_to_eat/archive/master.zip ; unzip master.zip ; rm -v master.zip mv -v where_not_to_eat-master raw cd ./raw/ pwd find -name \"* *\" | while read f ; do echo ${ f } ; new = $( echo $f | sed \"s/ /_/g\" ) ; echo ${ new } ; mv -v \" $f \" $new ; done mkdir -v ./../staging head -1 food_inspection_2018-01-01.csv | tr '[:upper:]' '[:lower:]' | sed -e \"s/#//g\" -e \"s/ ,/,/g\" -e \"s/ /_/g\" -e s \"/^,//g\" > header cat header > all_inspections.csv for f in food_inspection_2018-0*.csv ; do echo ${ f } ; awk 'NR > 1 {print}' ${ f } >> all_inspections.csv ; done mv -v all_inspections.csv ./../staging After saving our script we can commit our changes on our branch: git add clean.sh git commit -m \"Added script to clean and concatenate files git push #push to our remote repository Then you can create a Pull Request.","title":"Class Exercise"},{"location":"curriculum/setup/git-and-github/","text":"Git tutorial # What is git? (and why you should be using it for practically everything) # Image source Git is a free version control system which helps you keep track of file changes in your computer. Think of it as a time machine that lets you go back to any point in your project development. While git is most used in software development, you can use it for anything you like ( writing books , for example), as long as your files are plain text (e.g. source code, latex files), you won't have any issue with git (this guide is actually hosted using git, git-ception!). Simply speaking, git saves snapshots of your work called commits , after a commit is done, you can go back and forth to check the state of your project, maybe you were experimenting with some new function and realized the old one was better, no problem, you can bring back anything! Image source The entire development of your project is stored in your computer, but we know that's dangerous, so you can also host a remote copy (just like you do with Dropbox or Google Drive). What is github? # There are many , many providers that let you store your git repositories (that's how you call a git project ) but the most widely used is github (you'll be using it for dssg). Apart from storing a copy of your projects, github comes with a lot of useful features. For example, you can use it to share your projects with your colleagues, so they can see (or modify if you want) your project. git sounds awesome! How do I get it? # Chances are, git is already installed on your computer. If not, you can get it from here . OS X users: use homebrew, if you don't know what homebrew is, you probably didn't read the prerequisites :( Can I get buttons and stuff? # git is a command line tool, which means it doesn't have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote serve (like in dssg) is best way of doing it. However, if you still want a GUI (e.g. for using git in your computer), here are some options available: Options for Mac GitKraken (Windows and Mac) Ok, how do I do the magic? # Resources for beginners # 15 minute tutorial to learn git - This is a must for people to get started. git - the simple guide - A simple guide to get to know the most important concepts. Resouces for becoming a git ninja # A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. A git style guide , complete with branch naming, suggestions on how to handle commit messages, and more. READ THIS BEFORE YOU LEAVE (please) # By default, git saves everything inside the folder where you initiated the repo. When working on software projects there are files you DON'T want to save on git (e.g. database passwords, especially if you have a remote copy). To prevent git from saving files, create a file and name it .gitignore in the folder where you ran git init . In such file, you can add rules to let git know what you want it to ignore. For more information, read this . Besides sensitive data, you want to also ignore intermediate files generated automatically by some programming languages or libraries. There are templates available depending on the tools you use. There's also a nice command line tool to fetch such templates.","title":"What is it?"},{"location":"curriculum/setup/git-and-github/#git-tutorial","text":"","title":"Git tutorial"},{"location":"curriculum/setup/git-and-github/#what-is-git-and-why-you-should-be-using-it-for-practically-everything","text":"Image source Git is a free version control system which helps you keep track of file changes in your computer. Think of it as a time machine that lets you go back to any point in your project development. While git is most used in software development, you can use it for anything you like ( writing books , for example), as long as your files are plain text (e.g. source code, latex files), you won't have any issue with git (this guide is actually hosted using git, git-ception!). Simply speaking, git saves snapshots of your work called commits , after a commit is done, you can go back and forth to check the state of your project, maybe you were experimenting with some new function and realized the old one was better, no problem, you can bring back anything! Image source The entire development of your project is stored in your computer, but we know that's dangerous, so you can also host a remote copy (just like you do with Dropbox or Google Drive).","title":"What is git? (and why you should be using it for practically everything)"},{"location":"curriculum/setup/git-and-github/#what-is-github","text":"There are many , many providers that let you store your git repositories (that's how you call a git project ) but the most widely used is github (you'll be using it for dssg). Apart from storing a copy of your projects, github comes with a lot of useful features. For example, you can use it to share your projects with your colleagues, so they can see (or modify if you want) your project.","title":"What is github?"},{"location":"curriculum/setup/git-and-github/#git-sounds-awesome-how-do-i-get-it","text":"Chances are, git is already installed on your computer. If not, you can get it from here . OS X users: use homebrew, if you don't know what homebrew is, you probably didn't read the prerequisites :(","title":"git sounds awesome! How do I get it?"},{"location":"curriculum/setup/git-and-github/#can-i-get-buttons-and-stuff","text":"git is a command line tool, which means it doesn't have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote serve (like in dssg) is best way of doing it. However, if you still want a GUI (e.g. for using git in your computer), here are some options available: Options for Mac GitKraken (Windows and Mac)","title":"Can I get buttons and stuff?"},{"location":"curriculum/setup/git-and-github/#ok-how-do-i-do-the-magic","text":"","title":"Ok, how do I do the magic?"},{"location":"curriculum/setup/git-and-github/#resources-for-beginners","text":"15 minute tutorial to learn git - This is a must for people to get started. git - the simple guide - A simple guide to get to know the most important concepts.","title":"Resources for beginners"},{"location":"curriculum/setup/git-and-github/#resouces-for-becoming-a-git-ninja","text":"A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. A git style guide , complete with branch naming, suggestions on how to handle commit messages, and more.","title":"Resouces for becoming a git ninja"},{"location":"curriculum/setup/git-and-github/#read-this-before-you-leave-please","text":"By default, git saves everything inside the folder where you initiated the repo. When working on software projects there are files you DON'T want to save on git (e.g. database passwords, especially if you have a remote copy). To prevent git from saving files, create a file and name it .gitignore in the folder where you ran git init . In such file, you can add rules to let git know what you want it to ignore. For more information, read this . Besides sensitive data, you want to also ignore intermediate files generated automatically by some programming languages or libraries. There are templates available depending on the tools you use. There's also a nice command line tool to fetch such templates.","title":"READ THIS BEFORE YOU LEAVE  (please)"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/","text":"Basic github tutorial # Content # Intro Create a repo Using git Github","title":"Basic tutorial"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/#basic-github-tutorial","text":"","title":"Basic github tutorial"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/#content","text":"Intro Create a repo Using git Github","title":"Content"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/01_BasicGit/","text":"Git Overview # The tutorial will give a quick overview of what git is, how to use it and how to work as a team using the GitHub workflow. For this tutorial you will need to have access to Git, a Terminal and a Text Editor. Git in a nutshell # Image source Git is a version control system which helps you keep track changes in files you make during the development of your project. Think of it as a lab notebook that lets you go back-and-forth to any point in your project, an undo button, and a tool to safely and efficiently collaborate with others on a shared project. All serious software projects use version control. While git is mostly used in software development, it can be used for anything you like ( writing books , for example), as long as your files are plain text (e.g., source code, latex files). Simply speaking, git saves snapshots of your work called commits ; after a commit is created, you can go back and forth through different commits in your project -- maybe you were experimenting with some new function and realized the old function was better, no problem, you can bring back anything! The collections of commits and associated metadata form the repository of your project. Image source The entire development of your project, the repository , is stored on your computer, but we know that's dangerous, so you can also host a remote copy on a server like GitHub, Bitbucket, or GitLab. Hosting a project's repository on GitHub also allows for the distribution of your work and collaboration. This prevents endless emailing of source code and the following situation: git sounds awesome! How do I get it? # Chances are, git is already installed on your computer. To check open-up a terminal and type git . If not, you can get it from here . OS X users: use homebrew Can I get buttons and stuff? # git is a command line tool, which means it doesn't natively have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote server you will unlikely be able to use a GUI. However, if you still want a GUI (e.g., for using git on your computer), here are some options available: Options for Mac GitKraken (Windows and Mac) Keep in mind that if you are logging into a remote machine like AWS. A GUI may not be an option. This tutorial will cover the basics of git and hosting a project on GitHub. Configure your Git Profile # First things first. You need to configure you git client so your commits are attributed to you and you get pretty output. Do the following: # How my git configuration currently look like git config --list My workspace # Adding some, if you don't have a user.name or user.email set git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor 'nano' #or vim, emacs sublime For a list of text editors see Software Carpentry's list Also do the following (important). Ask about this during the branching section of the tutorial if you want to know more. git config --global push.default current You now have your git client configured. Next we will create our first repository.","title":"Git Overview"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/01_BasicGit/#git-overview","text":"The tutorial will give a quick overview of what git is, how to use it and how to work as a team using the GitHub workflow. For this tutorial you will need to have access to Git, a Terminal and a Text Editor.","title":"Git Overview"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/01_BasicGit/#git-in-a-nutshell","text":"Image source Git is a version control system which helps you keep track changes in files you make during the development of your project. Think of it as a lab notebook that lets you go back-and-forth to any point in your project, an undo button, and a tool to safely and efficiently collaborate with others on a shared project. All serious software projects use version control. While git is mostly used in software development, it can be used for anything you like ( writing books , for example), as long as your files are plain text (e.g., source code, latex files). Simply speaking, git saves snapshots of your work called commits ; after a commit is created, you can go back and forth through different commits in your project -- maybe you were experimenting with some new function and realized the old function was better, no problem, you can bring back anything! The collections of commits and associated metadata form the repository of your project. Image source The entire development of your project, the repository , is stored on your computer, but we know that's dangerous, so you can also host a remote copy on a server like GitHub, Bitbucket, or GitLab. Hosting a project's repository on GitHub also allows for the distribution of your work and collaboration. This prevents endless emailing of source code and the following situation:","title":"Git in a nutshell"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/01_BasicGit/#git-sounds-awesome-how-do-i-get-it","text":"Chances are, git is already installed on your computer. To check open-up a terminal and type git . If not, you can get it from here . OS X users: use homebrew","title":"git sounds awesome! How do I get it?"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/01_BasicGit/#can-i-get-buttons-and-stuff","text":"git is a command line tool, which means it doesn't natively have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote server you will unlikely be able to use a GUI. However, if you still want a GUI (e.g., for using git on your computer), here are some options available: Options for Mac GitKraken (Windows and Mac) Keep in mind that if you are logging into a remote machine like AWS. A GUI may not be an option. This tutorial will cover the basics of git and hosting a project on GitHub.","title":"Can I get buttons and stuff?"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/01_BasicGit/#configure-your-git-profile","text":"First things first. You need to configure you git client so your commits are attributed to you and you get pretty output. Do the following: # How my git configuration currently look like git config --list My workspace # Adding some, if you don't have a user.name or user.email set git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor 'nano' #or vim, emacs sublime For a list of text editors see Software Carpentry's list Also do the following (important). Ask about this during the branching section of the tutorial if you want to know more. git config --global push.default current You now have your git client configured. Next we will create our first repository.","title":"Configure your Git Profile"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/02_CreateRepo/","text":"A# Create a Repository Let's work on creating our first git repository. All shell commands will be prefixed with > All comments, text not meant to be parsed by the computer, is prefixed with # . Create a git repository # First create a directory for our project and cd into the directory > mkdir -v nyc-311 > cd nyc-311 The -v flag produces verbose output so you can see what happened after the invocation of the command. Now lets initialize the git repository. All git commands start with git <verb> . To initialize the git repo for our project we invoke the command > git init Let's look at the contents of our directory using the command ls -a > ls -a . .. .git/ The -a flag tells the ls command to include hidden directories when displaying files. We can see that there is now a .git directory. Unless you really know what you are doing DO NOT EVER modify anything in this directory. If you delete this directory, the entire history of your project will be gone. Make our first commit!! # All good projects should have a README.md to describe the project. So let's start with that. Fire up you favorite text editor and name a file README.md . Add something like this in your README: # Exploring 311 Calls in NYC ## Description This repo is for an analysis of 311 calls in NYC using Python 3.4 The # are part of markdown syntax for designating headings. Now let's look at the status of our repo using git status : > git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We can see that git knows that we have added a file but it is untracked . What this means is that git knows that this file has been added but it is currently \"untracked\" by git. Like the command said let's use git add to track changes in the the file by invoking the command git add README.md . > git add README.md Now lets look at the status of the repository by invoking the command git status again. > git status On branch master Initial commit Changes to be committed: (use \"git rm --cached <file>...\" to unstage) new file: README.md Now that we have added the file it has been \"staged\" in the staging area. We can now make our first commit! > git commit When you invoke this command an editor should pop-up and you have to leave a commit message. Good commit messages lead to a usable git log and separates the novice git users from the competent practitioners. Generally you should follow these guidelines in a commit message: First line is a one line summary of the commit that is in title case and less then 80 characters, written in the imperative voice. Second line should be blank. Third and subsequent lines should be more details of the commit. Anyone can look at a commit and examine what was changed. The commit message is where you provide a context of what and why you did what you did in the project. A good rule of thumb is If applied this commit will, My commit message is the following: Checking in README file * Added short description of the project * Added python3 as a dependency ### Using Nano as a text editor If you are using nano, to save your text use Ctrl-O, write a filename and to exit use Ctrl-X. Now that we have made our first commit we can examine our log! > git log The first line should output * commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 05 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency The first line is a unique identifier of your commit. The second give information on who made the commit. The third line gives the date. The rest is the commit message. To just get titles of commit messages you can use the following command > git log --oneline Another useful command is > git log --oneline --graph --all --decorate Some helpful commands # Removing files via git allows them to be recovered. git rm FILENAME git rm -r DIRECTORY Renaming files can be done by moving a file and then adding it or you can use the following command: git mv OLD NEW git rm and git mv will stage the changes that will ulimately need to be commited. That is it we have our first commit and repo! Next we are going to write some code!","title":"02 CreateRepo"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/02_CreateRepo/#create-a-git-repository","text":"First create a directory for our project and cd into the directory > mkdir -v nyc-311 > cd nyc-311 The -v flag produces verbose output so you can see what happened after the invocation of the command. Now lets initialize the git repository. All git commands start with git <verb> . To initialize the git repo for our project we invoke the command > git init Let's look at the contents of our directory using the command ls -a > ls -a . .. .git/ The -a flag tells the ls command to include hidden directories when displaying files. We can see that there is now a .git directory. Unless you really know what you are doing DO NOT EVER modify anything in this directory. If you delete this directory, the entire history of your project will be gone.","title":"Create a git repository"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/02_CreateRepo/#make-our-first-commit","text":"All good projects should have a README.md to describe the project. So let's start with that. Fire up you favorite text editor and name a file README.md . Add something like this in your README: # Exploring 311 Calls in NYC ## Description This repo is for an analysis of 311 calls in NYC using Python 3.4 The # are part of markdown syntax for designating headings. Now let's look at the status of our repo using git status : > git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We can see that git knows that we have added a file but it is untracked . What this means is that git knows that this file has been added but it is currently \"untracked\" by git. Like the command said let's use git add to track changes in the the file by invoking the command git add README.md . > git add README.md Now lets look at the status of the repository by invoking the command git status again. > git status On branch master Initial commit Changes to be committed: (use \"git rm --cached <file>...\" to unstage) new file: README.md Now that we have added the file it has been \"staged\" in the staging area. We can now make our first commit! > git commit When you invoke this command an editor should pop-up and you have to leave a commit message. Good commit messages lead to a usable git log and separates the novice git users from the competent practitioners. Generally you should follow these guidelines in a commit message: First line is a one line summary of the commit that is in title case and less then 80 characters, written in the imperative voice. Second line should be blank. Third and subsequent lines should be more details of the commit. Anyone can look at a commit and examine what was changed. The commit message is where you provide a context of what and why you did what you did in the project. A good rule of thumb is If applied this commit will, My commit message is the following: Checking in README file * Added short description of the project * Added python3 as a dependency ### Using Nano as a text editor If you are using nano, to save your text use Ctrl-O, write a filename and to exit use Ctrl-X. Now that we have made our first commit we can examine our log! > git log The first line should output * commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 05 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency The first line is a unique identifier of your commit. The second give information on who made the commit. The third line gives the date. The rest is the commit message. To just get titles of commit messages you can use the following command > git log --oneline Another useful command is > git log --oneline --graph --all --decorate","title":"Make our first commit!!"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/02_CreateRepo/#some-helpful-commands","text":"Removing files via git allows them to be recovered. git rm FILENAME git rm -r DIRECTORY Renaming files can be done by moving a file and then adding it or you can use the following command: git mv OLD NEW git rm and git mv will stage the changes that will ulimately need to be commited. That is it we have our first commit and repo! Next we are going to write some code!","title":"Some helpful commands"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/03_UsingTheGitLog/","text":"Using the Git Log # In this portion of the tutorial we are going to explore the git log and how to go back to prior parts of our project. We left off creating a README file. Now the time has come to actually write some code. Create a python script to output results of analyzing 311 calls # Let's download some data to work with using the following command you can copy and paste. > curl -O https://raw.githubusercontent.com/avishekrk/pandas-cookbook/master/data/311-service-requests.csv Fire up your favorite text editor and let's write a little program called descriptive_stats.py to get the most common complaints from 311 data in NYC. If you are using nano invoke the command > nano descriptive_stats.py Add the following in the text. Don't worry you don't need to know what this means right now. We are loading some data into and finding the top 5 categories of complaints using 311 data. from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print( df_311_calls['Complaint Type'].value_counts()[:5]) Remember, to save the program in nano the command is Ctrl-O and the to exit is Ctrl-X . We have just created a python program. We can run the program using the following syntax. > python <program name> In our case we run the following command: > python descriptive_stats.py When we run the program we should get the following output HEATING 14200 GENERAL CONSTRUCTION 7471 Street Light Condition 7117 DOF Literature Request 5797 PLUMBING 5373 Name: Complaint Type, dtype: int64 These is the top 5 311 complaints for 2010 in NYC. Now that we have a working program lets commit it to the repo, just like before. > git add descriptive_stats.py # to the staging area > git commit -m \"Checking in descriptive_stats.py, output top 5 311 complaints\" In this case rather then launching a text editor to write a commit message we used the -m option to make an in-line commit message. Our change doesn't require a lengthy commit message. Now we should have a commited version of descriptive stats. Let's now decide that we want the top 10 311 complaints and modify our program to output the top 10 results. Our current program should now be: from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') print( df_311_calls['Complaint Type'].value_counts[:10]) Let's commit that change: git add descriptive_stats.py git commit -m \"Changed the top 5 results to the top 10 results in descriptive_stats.py\" If we look at our git log we should new be able to see history of our changes: > git log commit 42c35933c4d52708c2562c1c05361b152a2b9230 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:55:29 2016 -0600 Changed the top 5 results to the top 10 results in descriptive_stats.py commit ab85797b2c3d68fb0c97535080079138888b5556 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:52:52 2016 -0600 Checking-in descriptive_stats.py outputs the top 5 311 complaints commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency Now let's look at the difference between the two commits in the log using the git diff command. The git diff command is important for seeing changes in your source code and comparing one commit against another. If we now invoke the command > git diff HEAD~1 diff --git a/descriptive_stats.py b/descriptive_stats.py index 09b7168..c38d3e3 100644 --- a/descriptive_stats.py +++ b/descriptive_stats.py @@ -3,4 +3,4 @@ import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') -print( df_311_calls['Complaint Type'].value_counts()[:5] ) +print( df_311_calls['Complaint Type'].value_counts()[:10] ) First HEAD is shorthand for the latest commit in the repository. HEAD~1 is a shorthand for the lastest commit minus one. For instance HEAD~20 refers to the a commit 20 commits ago. The output of the diff file is the following. The first line looks similiar to a diff command. The second line shows the commit identifiers of the two commits being compared. The next two lines are the files being compared. The interesting part is at the bottom. The line with the - sign is our prior commit. The line with a + sign the the current commit. We can see the difference is the change between 5 to 10. We can \"checkout\" old versions of our files using the checkout command. This is a very handy feature for when we break something and want to start from a working copy or if we have an old feature that has since been discarded it can be restored. Let's now go back to our prior commit using the git checkout command git checkout HEAD~1 descriptive_stats.py If we look at our file descriptive_stats.py we will have reverted back to our old version. git checkout HEAD descriptive_stats.py Another useful git diff command is: git diff --staged where we can examine the differences between files that have been staged for commit and the last commit. Next up we are going to go over how to host a project on GitHub.","title":"Using the Git Log"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/03_UsingTheGitLog/#using-the-git-log","text":"In this portion of the tutorial we are going to explore the git log and how to go back to prior parts of our project. We left off creating a README file. Now the time has come to actually write some code.","title":"Using the Git Log"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/03_UsingTheGitLog/#create-a-python-script-to-output-results-of-analyzing-311-calls","text":"Let's download some data to work with using the following command you can copy and paste. > curl -O https://raw.githubusercontent.com/avishekrk/pandas-cookbook/master/data/311-service-requests.csv Fire up your favorite text editor and let's write a little program called descriptive_stats.py to get the most common complaints from 311 data in NYC. If you are using nano invoke the command > nano descriptive_stats.py Add the following in the text. Don't worry you don't need to know what this means right now. We are loading some data into and finding the top 5 categories of complaints using 311 data. from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print( df_311_calls['Complaint Type'].value_counts()[:5]) Remember, to save the program in nano the command is Ctrl-O and the to exit is Ctrl-X . We have just created a python program. We can run the program using the following syntax. > python <program name> In our case we run the following command: > python descriptive_stats.py When we run the program we should get the following output HEATING 14200 GENERAL CONSTRUCTION 7471 Street Light Condition 7117 DOF Literature Request 5797 PLUMBING 5373 Name: Complaint Type, dtype: int64 These is the top 5 311 complaints for 2010 in NYC. Now that we have a working program lets commit it to the repo, just like before. > git add descriptive_stats.py # to the staging area > git commit -m \"Checking in descriptive_stats.py, output top 5 311 complaints\" In this case rather then launching a text editor to write a commit message we used the -m option to make an in-line commit message. Our change doesn't require a lengthy commit message. Now we should have a commited version of descriptive stats. Let's now decide that we want the top 10 311 complaints and modify our program to output the top 10 results. Our current program should now be: from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') print( df_311_calls['Complaint Type'].value_counts[:10]) Let's commit that change: git add descriptive_stats.py git commit -m \"Changed the top 5 results to the top 10 results in descriptive_stats.py\" If we look at our git log we should new be able to see history of our changes: > git log commit 42c35933c4d52708c2562c1c05361b152a2b9230 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:55:29 2016 -0600 Changed the top 5 results to the top 10 results in descriptive_stats.py commit ab85797b2c3d68fb0c97535080079138888b5556 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:52:52 2016 -0600 Checking-in descriptive_stats.py outputs the top 5 311 complaints commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency Now let's look at the difference between the two commits in the log using the git diff command. The git diff command is important for seeing changes in your source code and comparing one commit against another. If we now invoke the command > git diff HEAD~1 diff --git a/descriptive_stats.py b/descriptive_stats.py index 09b7168..c38d3e3 100644 --- a/descriptive_stats.py +++ b/descriptive_stats.py @@ -3,4 +3,4 @@ import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') -print( df_311_calls['Complaint Type'].value_counts()[:5] ) +print( df_311_calls['Complaint Type'].value_counts()[:10] ) First HEAD is shorthand for the latest commit in the repository. HEAD~1 is a shorthand for the lastest commit minus one. For instance HEAD~20 refers to the a commit 20 commits ago. The output of the diff file is the following. The first line looks similiar to a diff command. The second line shows the commit identifiers of the two commits being compared. The next two lines are the files being compared. The interesting part is at the bottom. The line with the - sign is our prior commit. The line with a + sign the the current commit. We can see the difference is the change between 5 to 10. We can \"checkout\" old versions of our files using the checkout command. This is a very handy feature for when we break something and want to start from a working copy or if we have an old feature that has since been discarded it can be restored. Let's now go back to our prior commit using the git checkout command git checkout HEAD~1 descriptive_stats.py If we look at our file descriptive_stats.py we will have reverted back to our old version. git checkout HEAD descriptive_stats.py Another useful git diff command is: git diff --staged where we can examine the differences between files that have been staged for commit and the last commit. Next up we are going to go over how to host a project on GitHub.","title":"Create a python script to output results of analyzing 311 calls"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/","text":"GitHub # This part of the tutorial will go over how to host your project on GitHub The virtue of using a .gitignore file The GitHub workflow How to solve a merge conflict What is GitHub? # There are several popular providers -- Bitbucket , GitLab -- that let you store your git repositories but the most widely used is GitHub . Apart from storing a copy of your projects, GitHub comes with a lot of useful features. For example, you can use it to share your projects with your colleagues so they can see or modify your project. Is is also a collaboration tool that you can use to work as a part of a team. Getting started on GitHub? # Go to https://github.com and create a free account. Create a new repository called NYC-311 The repository URL will then be https://github.com/username/NYC-311 Add the remote repository in your local repository > git remote add origin https://github.com/username/nyc-311.git You can then see the remote repository with the following command > git remote -v The remote is named \"origin\" which is a common choice for the primary repository. Now we are going to push the local changes on our repository to the GitHub repository using the command > git push --set-upstream origin master What we have done is taken a copy of our repository and pushed it to GitHub. When you push changes to GitHub in the future you just need to use the command git push origin master To pull in changes in your repository done by yourself or another person you can use the following command git pull origin master To see this in action grab a friend and have them clone your repo with the command. First make sure they also have a GitHub account and add them as a collaborator to your repository. If you can't find a friend you can still do this part of the tutorial just skip the parts about using git clone and git pull . > git clone https://github.com/username/nyc-311.git They will then have a copy of your repository. Now cd into the project folder and add a special file called .gitignore . Fire up a text editor and add the following #Ignore pyc files *.pyc The .gitignore file is a special file that git looks at when trying to make a commit. We added the entry '*.pyc'. This is an instruction for git to not commit files that end in .pyc . Files that end in pyc are python bytecode files that will appear when you run your python code. All lines prefixed with # are comments and will be ignored; they are meant to be read by people not computers. Now lets add this file to our repository git add .gitignore git commit -m \"Added .gitignore\" And push our changes to the repository git push origin master Now on your machine git pull the new state of the repository git pull origin master If we then use the command ls -a you should then see the .gitignore file. And if we look at the log using 'git log` you should see the new commit. GitHub Flow # In this portion of the tutorial we will go over branching and the general GitHub workflow. So far we have been doing the \"solo\" workflow which consists of the following: mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" git push origin master # hack # more hack We are now going to introduce the GitHub flow that is largely done in teams. In the GitHub flow we never code anything unless there is a need to. When there is a need we then create an issue on the GitHub repository. Good issues should be Clear Defined output Actionable (written in the Imperative Voice) Could be completed at most in few days Examples Good : /Fix the bug in .../ Good : /Add a method that does .../ Bad : /Solve the project/ Bad : /Some error happen/ Here is how to create a GitHub issue. Once we have an issue we will then pull from the repo and create a branch . A branch is a copy of the code base separate from the main master branch where we can work on our issue (e.g, fixing a bug, adding a feature) without affecting the master branch during our work and then ultimately merge our change into the master branch. The flow goes something like this: ## Pull from the repo git pull ## Decide what do you want to do and create an issue git checkout -b a-meaningful-name The command git checkout -b creates a new branch which in this case is named \"a-meaningful-name\". We can see what branch we are on by using the command git branch which will show all the branches in the local repository and place an * next to the branch we are currently on. ## hack, hack, hack, add/rm, commit ## Push to the repo and create a remote branch git push ## Create a pull-request and describe your work (Suggest/add a reviewer) ## Someone then reviews your code ## The pull-request is closed and the remote branch is destroyed ## Switch to master locally git checkout master ## Pull the most recent changes (including yours) git pull ## Delete your local branch git branch -d a-meaningful-name Here is how to create a GitHub pull request. Solving a merge conflict # As you work on projects with others you will inevitably run into merge conflicts. A merge conflict is caused when you and another person edits the same line of a file. Git will not know which line is the correct one and create a conflict. Let's make a conflict! #create a branch called drama > git checkout -b drama #now modify the descriptive_stats.py file and change the top 10 #values to top 13 #commit your changes > git add descriptive_stats.py > git commit -m \"Changed top 10 to top 13 in descriptive_stats.py #switch back to master > git checkout master #now modify the descriptive_stats.py file and change the top 10 #values to top 3 # commit your changes > git add descriptive_stats.py > git commit -m \"Changed the top 10 to the top 3 in descriptive_stats.py\" Now we are going to merge the drama branch into the master branch > git merge drama Auto-merging descriptive_stats.py CONFLICT (content): Merge conflict in descriptive_stats.py Automatic merge failed; fix conflicts and then commit the result. Our arbitrary drama has now lead to a conflict. If we check the status we should see the following On branch master You have unmerged paths. (fix conflicts and run \"git commit\") Unmerged paths: (use \"git add <file>...\" to mark resolution) both modified: descriptive_stats.py Let's examine the conflicting file descriptive_stats.py , you should see the following: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) <<<<<<< HEAD print(df_311_calls['Complaint Type'].value_counts()[:3]) ======= print(df_311_calls['Complaint Type'].value_counts()[:13]) >>>>>>> drama The >>>>>>> and <<<<<<< denote the section of the conflicting code. HEAD means the following line if from the maser branch while drama shows that the preceding line is from the drama branch. The lines of the two branches are separated by the ======= . Given a merge conflict we have three choices: either keep the line from the master branch, keep the line from the drama branch or create an entirely new line. In this case we are going to keep the line from the master branch. To do that we delete the merge conflict markers and the line from the drama branch and then make a commit. Your code should look like this: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print(df_311_calls['Complaint Type'].value_counts()[:3]) Now add this file and make a commit. git add descriptive_stats.py git commit -m \"Fixed merge conflict in descriptive stats\" Our merge conflict is know solved. That concludes the tutorial on GitHub! Good Luck! Acknowledgements, References and Further Resources # This tutorial is derived from tutorials created by Eduardo Blancas Reyes, Benedict Kuester, Adolfo De Unanue, Software Carpentry , and ASU PHY-494 . Furthur resources for becoming a git master are: * Software Carpentry -- a more in-depth intro tutorial 15 minute tutorial to learn git - Intro tutorial. git - the simple guide - A simple guide to get to know the most important concepts. A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. Pro Git -- An in-depth discussion written by git masters.","title":"GitHub"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/#github","text":"This part of the tutorial will go over how to host your project on GitHub The virtue of using a .gitignore file The GitHub workflow How to solve a merge conflict","title":"GitHub"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/#what-is-github","text":"There are several popular providers -- Bitbucket , GitLab -- that let you store your git repositories but the most widely used is GitHub . Apart from storing a copy of your projects, GitHub comes with a lot of useful features. For example, you can use it to share your projects with your colleagues so they can see or modify your project. Is is also a collaboration tool that you can use to work as a part of a team.","title":"What is GitHub?"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/#getting-started-on-github","text":"Go to https://github.com and create a free account. Create a new repository called NYC-311 The repository URL will then be https://github.com/username/NYC-311 Add the remote repository in your local repository > git remote add origin https://github.com/username/nyc-311.git You can then see the remote repository with the following command > git remote -v The remote is named \"origin\" which is a common choice for the primary repository. Now we are going to push the local changes on our repository to the GitHub repository using the command > git push --set-upstream origin master What we have done is taken a copy of our repository and pushed it to GitHub. When you push changes to GitHub in the future you just need to use the command git push origin master To pull in changes in your repository done by yourself or another person you can use the following command git pull origin master To see this in action grab a friend and have them clone your repo with the command. First make sure they also have a GitHub account and add them as a collaborator to your repository. If you can't find a friend you can still do this part of the tutorial just skip the parts about using git clone and git pull . > git clone https://github.com/username/nyc-311.git They will then have a copy of your repository. Now cd into the project folder and add a special file called .gitignore . Fire up a text editor and add the following #Ignore pyc files *.pyc The .gitignore file is a special file that git looks at when trying to make a commit. We added the entry '*.pyc'. This is an instruction for git to not commit files that end in .pyc . Files that end in pyc are python bytecode files that will appear when you run your python code. All lines prefixed with # are comments and will be ignored; they are meant to be read by people not computers. Now lets add this file to our repository git add .gitignore git commit -m \"Added .gitignore\" And push our changes to the repository git push origin master Now on your machine git pull the new state of the repository git pull origin master If we then use the command ls -a you should then see the .gitignore file. And if we look at the log using 'git log` you should see the new commit.","title":"Getting started on GitHub?"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/#github-flow","text":"In this portion of the tutorial we will go over branching and the general GitHub workflow. So far we have been doing the \"solo\" workflow which consists of the following: mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" git push origin master # hack # more hack We are now going to introduce the GitHub flow that is largely done in teams. In the GitHub flow we never code anything unless there is a need to. When there is a need we then create an issue on the GitHub repository. Good issues should be Clear Defined output Actionable (written in the Imperative Voice) Could be completed at most in few days Examples Good : /Fix the bug in .../ Good : /Add a method that does .../ Bad : /Solve the project/ Bad : /Some error happen/ Here is how to create a GitHub issue. Once we have an issue we will then pull from the repo and create a branch . A branch is a copy of the code base separate from the main master branch where we can work on our issue (e.g, fixing a bug, adding a feature) without affecting the master branch during our work and then ultimately merge our change into the master branch. The flow goes something like this: ## Pull from the repo git pull ## Decide what do you want to do and create an issue git checkout -b a-meaningful-name The command git checkout -b creates a new branch which in this case is named \"a-meaningful-name\". We can see what branch we are on by using the command git branch which will show all the branches in the local repository and place an * next to the branch we are currently on. ## hack, hack, hack, add/rm, commit ## Push to the repo and create a remote branch git push ## Create a pull-request and describe your work (Suggest/add a reviewer) ## Someone then reviews your code ## The pull-request is closed and the remote branch is destroyed ## Switch to master locally git checkout master ## Pull the most recent changes (including yours) git pull ## Delete your local branch git branch -d a-meaningful-name Here is how to create a GitHub pull request.","title":"GitHub Flow"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/#solving-a-merge-conflict","text":"As you work on projects with others you will inevitably run into merge conflicts. A merge conflict is caused when you and another person edits the same line of a file. Git will not know which line is the correct one and create a conflict. Let's make a conflict! #create a branch called drama > git checkout -b drama #now modify the descriptive_stats.py file and change the top 10 #values to top 13 #commit your changes > git add descriptive_stats.py > git commit -m \"Changed top 10 to top 13 in descriptive_stats.py #switch back to master > git checkout master #now modify the descriptive_stats.py file and change the top 10 #values to top 3 # commit your changes > git add descriptive_stats.py > git commit -m \"Changed the top 10 to the top 3 in descriptive_stats.py\" Now we are going to merge the drama branch into the master branch > git merge drama Auto-merging descriptive_stats.py CONFLICT (content): Merge conflict in descriptive_stats.py Automatic merge failed; fix conflicts and then commit the result. Our arbitrary drama has now lead to a conflict. If we check the status we should see the following On branch master You have unmerged paths. (fix conflicts and run \"git commit\") Unmerged paths: (use \"git add <file>...\" to mark resolution) both modified: descriptive_stats.py Let's examine the conflicting file descriptive_stats.py , you should see the following: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) <<<<<<< HEAD print(df_311_calls['Complaint Type'].value_counts()[:3]) ======= print(df_311_calls['Complaint Type'].value_counts()[:13]) >>>>>>> drama The >>>>>>> and <<<<<<< denote the section of the conflicting code. HEAD means the following line if from the maser branch while drama shows that the preceding line is from the drama branch. The lines of the two branches are separated by the ======= . Given a merge conflict we have three choices: either keep the line from the master branch, keep the line from the drama branch or create an entirely new line. In this case we are going to keep the line from the master branch. To do that we delete the merge conflict markers and the line from the drama branch and then make a commit. Your code should look like this: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print(df_311_calls['Complaint Type'].value_counts()[:3]) Now add this file and make a commit. git add descriptive_stats.py git commit -m \"Fixed merge conflict in descriptive stats\" Our merge conflict is know solved. That concludes the tutorial on GitHub! Good Luck!","title":"Solving a merge conflict"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/04_GitHub/#acknowledgements-references-and-further-resources","text":"This tutorial is derived from tutorials created by Eduardo Blancas Reyes, Benedict Kuester, Adolfo De Unanue, Software Carpentry , and ASU PHY-494 . Furthur resources for becoming a git master are: * Software Carpentry -- a more in-depth intro tutorial 15 minute tutorial to learn git - Intro tutorial. git - the simple guide - A simple guide to get to know the most important concepts. A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. Pro Git -- An in-depth discussion written by git masters.","title":"Acknowledgements, References and Further Resources"},{"location":"curriculum/setup/git-and-github/basic_git_tutorial/data/","text":"Data was taken from the NYC Open Data Portal https://nycopendata.socrata.com","title":"Home"},{"location":"curriculum/setup/git-and-github/branching_and_merging/","text":"Branching and merging # Introduction # Teams want to + work on the same repository simultaneously + version control frequently + share their work + maintain a working version of the library at all time Git is a highly optimised tool to satisfy these complex and conflicting requirements. However, to use it effectively, alignment on a robust workflow is required. This document introduces the key concepts needed. Gitflow # Gitflow workflow is a Git workflow design that was first published and made popular by Vincent Driessen at nvie . The Gitflow workflow defines a strict branching model designed for managing larger projects. The workflow assigns very specific roles to different branches and defines how and when they should interact. The key elements are feature branches, a development branch and a master branch. It facilitates pull requests, isolated experiments, and more efficient collaboration. Branches # Branches are used to isolate work on different issues and experiments. To see existing branches: git branch The current branch is highlighted with a star. Note that this doesn't show all branches but only those that you have shown an interest in in the past. Checkout an existing branch: git checkout branch_name Have a look at git branch to see how the star changed. Note that work present on the old branch, will vanish in your file explorer if they are not present in the new branch. Don't worry, the files will be visible again once you switch back to the old branch. Create a new branch starting from the current branch (and commit) git branch new_branch_name Branch names cannot have spaces and it is advisable to make them lowercase. Check git branch to see that the new branch is created but we are still on the old branch. We can change to the new branch via git checkout new_branch_name . A shortcut for creating and immediately checking out the new branch is git checkout -b new_branch_name Usually, only one person will work on any given feature branch. Positive consequences - Your work is isolated from experiments of others - You can commit even if it is not perfect or even not working - Commit often (many times a day) Committing often is important because - Enable granular version controlling - If bugs appear, you can checkout past versions and find out where the bug krept in - Each bit-size change is easy to parse for you and your teammates - Write a clear commit message explaining why a commit was made. Changes or qwerty is not a good commit message. - Ideally make one commit per file Negative consequences - Your work is not available to others - Your work can become disconnected from the main development To overcome these problems: - Merge dev into the feature branch frequently (once a day) - Break tasks into small to medium size chunks, complete only one chunk on one feature branch, then bring it into dev and start a new feature branch Merging # Git is a fantastic tool for combining work. It is very good at working out what is new and what is old and can even handle multiple changes to the same file. To integrate the latest work from branch branch_name into the current branch, use git merge branch_name To get the work from the remote instead of local, use git merge origin branch_name The main situation where this does not work well, is when there are two conflicting changes to the same line. Git cannot work out on its own how to combine the work and will report a merge conflict. The error message will say CONFLICT (content): Merge conflict in file_name Automatic merge failed; fix conflicts and then commit the result. There are many merge tools to overcome this problem. For example, PyCharm has a very good merge tool . Once you have resolved the conflicts, commit your changes to complete the merge. Pull requests # Pull requests are a tool to receive feedback and quality control on your work before contributing it to the development branch. To make a pull request, first make sure your branch is up-to-date with the main development branch git pull origin dev Then publish the latest version of your work to the remote repository. git push origin feature_branch On the GitHub website, you will see a prompt to open a pull request into the default branch ( dev in our case). Provide a summary of what the feature branch is doing, select reviewers (usually your technical mentor and some team mates) and assign the PR to yourself. Pull requests show diffs, or differences, of the content from both branches. The changes, additions, and subtractions are shown in green and red, respectively. By using GitHub\u2019s @mention system in your pull request message, you can ask for feedback from specific people or teams, whether they\u2019re down the hall or 10 time zones away. It is usually enough to have one person's review. However, if multiple people comment, make sure you address all the comments (either via a discussion or a code change). Once the PR is approved, the person who raised the PR will Click on the green Merge pull request button to merge the changes into dev Click confirm merge Since you're now done with this new branch, delete the branch by clicking the Delete branch button. Celebrate on slack! Stash # Sometimes you might produce a chunk of work on the wrong branch. For example - when thinking the change would just be one line but then turns into a larger piece - when forgetting which branch is currently checked out One usually remembers when trying to commit. But it's not possible to change branch when there is uncommitted work. The solution is git stash Stashing takes the dirty state of your working directory \u2014 that is, your modified tracked files and staged changes \u2014 and saves it on a stack of unfinished changes that you can reapply at any time. Once you ran git stash , your working directory is clean and you can conveniently switch branches. You can then re-apply the changes (the last ones stashed) using git stash apply All stashed changes can be seen with git stash list and older stashes can be applied using git stash apply stash@{ID} , where ID is the number of the stash to be applied (found by inspecting the stash list). Tags # At DSSG, we you to use weekly tags to reference weekly milestones. To create a tag run git tag <tagname> . This will create a local tag with the current state of the branch you are on. When pushing to your remote repo, tags are NOT included by default. You will need to explicitly say that you want to push your tags to your remote repo. To push your tag run git push origin <tag> . Or to push all tags (in the case there are multiple), you'd run git push origin --tags . In our case, we'll just be working with one at a time. Create and push an end-of-week tag each Friday. If feasible, we will also merge dev into master at that time. .gitignore # .gitignore files specify which files are ignored in a git repository. Example: #ignore a single file `mycode.class` #ignore an entire directory `/mydebugdir/` #ignore a file type `*.json` #add an exception (using !) to the preceding rule to track a specific file `!package.json` Let's create an empty .gitignore file (touch .gitignore ) and add some stuff! Some useful commands # This section contains some useful commands: Discard changes git checkout file_name.py : discard changes made to file file_name.py git checkout . : discard all changes made to files currently tracked by git git reset --hard : discard changes in the working directory and changes that have been staged but not committed git stash : discard all local changes but store them for potential re-use later git clean : remove untracked files that are unknown to git (i.e. gitignored files are not removed) Sharing with others Pull: - git pull origin branch_name : fetch the latest version of branch_name from github and merge into the current branch - git pull : fetch the latest version of all branches from github. If the current branch is tracking a remote branch, merge the remote into the local branch. Push: - git push origin branch_name : merge the latest version of the current branch into the remote branch called branch_name . This only works if a branch called branch_name exists. - git push -u origin branch_name : create a remote branch called branch_name with the content of the current branch. - git push : Abbreviation for git push origin branch_name where branch_name is the remote branch tracked by the current branch. Check changes git diff : Show all changes made since the last commit git diff file_name : Show changes made to the file named file_name . If the file is not in your directory, write the whole path. Plan for the teaching session # Introduce gitflow Everyone to discard changes from last session Pull latest version from remote to get the dev branch Introduce branches View branches, checkout dev branch Each team member, take on the task to update the readme one of the README.md sections: Project title & project summary table of contents partners contributors Create a feature branch from dev for the task. Give the branch a suitable name. Checkout the branch. Spend 1 min to make a small amount of progress on your task. Diff your changes, add, commit. It's not perfect but ok since you're on a feature branch. Create a second branch from dev Edit the same section in the README.md but differently, diff, add, commit. Merge the two feature branches (direction up to you) Make another commit on the feature branch TMs to demo raising a PR, requesting changes, addressing, approving, merging. Everyone to raise a PR for their work in progress feature branches References: # There is a countless number of git tutorials out there and you should be able to find answers to most questions on stackoverflow. Some resources that we like: - Cheat sheet - Git hello world tutorial","title":"Advanced notes"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#branching-and-merging","text":"","title":"Branching and merging"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#introduction","text":"Teams want to + work on the same repository simultaneously + version control frequently + share their work + maintain a working version of the library at all time Git is a highly optimised tool to satisfy these complex and conflicting requirements. However, to use it effectively, alignment on a robust workflow is required. This document introduces the key concepts needed.","title":"Introduction"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#gitflow","text":"Gitflow workflow is a Git workflow design that was first published and made popular by Vincent Driessen at nvie . The Gitflow workflow defines a strict branching model designed for managing larger projects. The workflow assigns very specific roles to different branches and defines how and when they should interact. The key elements are feature branches, a development branch and a master branch. It facilitates pull requests, isolated experiments, and more efficient collaboration.","title":"Gitflow"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#branches","text":"Branches are used to isolate work on different issues and experiments. To see existing branches: git branch The current branch is highlighted with a star. Note that this doesn't show all branches but only those that you have shown an interest in in the past. Checkout an existing branch: git checkout branch_name Have a look at git branch to see how the star changed. Note that work present on the old branch, will vanish in your file explorer if they are not present in the new branch. Don't worry, the files will be visible again once you switch back to the old branch. Create a new branch starting from the current branch (and commit) git branch new_branch_name Branch names cannot have spaces and it is advisable to make them lowercase. Check git branch to see that the new branch is created but we are still on the old branch. We can change to the new branch via git checkout new_branch_name . A shortcut for creating and immediately checking out the new branch is git checkout -b new_branch_name Usually, only one person will work on any given feature branch. Positive consequences - Your work is isolated from experiments of others - You can commit even if it is not perfect or even not working - Commit often (many times a day) Committing often is important because - Enable granular version controlling - If bugs appear, you can checkout past versions and find out where the bug krept in - Each bit-size change is easy to parse for you and your teammates - Write a clear commit message explaining why a commit was made. Changes or qwerty is not a good commit message. - Ideally make one commit per file Negative consequences - Your work is not available to others - Your work can become disconnected from the main development To overcome these problems: - Merge dev into the feature branch frequently (once a day) - Break tasks into small to medium size chunks, complete only one chunk on one feature branch, then bring it into dev and start a new feature branch","title":"Branches"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#merging","text":"Git is a fantastic tool for combining work. It is very good at working out what is new and what is old and can even handle multiple changes to the same file. To integrate the latest work from branch branch_name into the current branch, use git merge branch_name To get the work from the remote instead of local, use git merge origin branch_name The main situation where this does not work well, is when there are two conflicting changes to the same line. Git cannot work out on its own how to combine the work and will report a merge conflict. The error message will say CONFLICT (content): Merge conflict in file_name Automatic merge failed; fix conflicts and then commit the result. There are many merge tools to overcome this problem. For example, PyCharm has a very good merge tool . Once you have resolved the conflicts, commit your changes to complete the merge.","title":"Merging"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#pull-requests","text":"Pull requests are a tool to receive feedback and quality control on your work before contributing it to the development branch. To make a pull request, first make sure your branch is up-to-date with the main development branch git pull origin dev Then publish the latest version of your work to the remote repository. git push origin feature_branch On the GitHub website, you will see a prompt to open a pull request into the default branch ( dev in our case). Provide a summary of what the feature branch is doing, select reviewers (usually your technical mentor and some team mates) and assign the PR to yourself. Pull requests show diffs, or differences, of the content from both branches. The changes, additions, and subtractions are shown in green and red, respectively. By using GitHub\u2019s @mention system in your pull request message, you can ask for feedback from specific people or teams, whether they\u2019re down the hall or 10 time zones away. It is usually enough to have one person's review. However, if multiple people comment, make sure you address all the comments (either via a discussion or a code change). Once the PR is approved, the person who raised the PR will Click on the green Merge pull request button to merge the changes into dev Click confirm merge Since you're now done with this new branch, delete the branch by clicking the Delete branch button. Celebrate on slack!","title":"Pull requests"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#stash","text":"Sometimes you might produce a chunk of work on the wrong branch. For example - when thinking the change would just be one line but then turns into a larger piece - when forgetting which branch is currently checked out One usually remembers when trying to commit. But it's not possible to change branch when there is uncommitted work. The solution is git stash Stashing takes the dirty state of your working directory \u2014 that is, your modified tracked files and staged changes \u2014 and saves it on a stack of unfinished changes that you can reapply at any time. Once you ran git stash , your working directory is clean and you can conveniently switch branches. You can then re-apply the changes (the last ones stashed) using git stash apply All stashed changes can be seen with git stash list and older stashes can be applied using git stash apply stash@{ID} , where ID is the number of the stash to be applied (found by inspecting the stash list).","title":"Stash"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#tags","text":"At DSSG, we you to use weekly tags to reference weekly milestones. To create a tag run git tag <tagname> . This will create a local tag with the current state of the branch you are on. When pushing to your remote repo, tags are NOT included by default. You will need to explicitly say that you want to push your tags to your remote repo. To push your tag run git push origin <tag> . Or to push all tags (in the case there are multiple), you'd run git push origin --tags . In our case, we'll just be working with one at a time. Create and push an end-of-week tag each Friday. If feasible, we will also merge dev into master at that time.","title":"Tags"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#gitignore","text":".gitignore files specify which files are ignored in a git repository. Example: #ignore a single file `mycode.class` #ignore an entire directory `/mydebugdir/` #ignore a file type `*.json` #add an exception (using !) to the preceding rule to track a specific file `!package.json` Let's create an empty .gitignore file (touch .gitignore ) and add some stuff!","title":".gitignore"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#some-useful-commands","text":"This section contains some useful commands:","title":"Some useful commands"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#plan-for-the-teaching-session","text":"Introduce gitflow Everyone to discard changes from last session Pull latest version from remote to get the dev branch Introduce branches View branches, checkout dev branch Each team member, take on the task to update the readme one of the README.md sections: Project title & project summary table of contents partners contributors Create a feature branch from dev for the task. Give the branch a suitable name. Checkout the branch. Spend 1 min to make a small amount of progress on your task. Diff your changes, add, commit. It's not perfect but ok since you're on a feature branch. Create a second branch from dev Edit the same section in the README.md but differently, diff, add, commit. Merge the two feature branches (direction up to you) Make another commit on the feature branch TMs to demo raising a PR, requesting changes, addressing, approving, merging. Everyone to raise a PR for their work in progress feature branches","title":"Plan for the teaching session"},{"location":"curriculum/setup/git-and-github/branching_and_merging/#references","text":"There is a countless number of git tutorials out there and you should be able to find answers to most questions on stackoverflow. Some resources that we like: - Cheat sheet - Git hello world tutorial","title":"References:"},{"location":"curriculum/setup/git-and-github/githubflow/","text":"Github flow # Some notes about github flow...","title":"Github flow"},{"location":"curriculum/setup/git-and-github/githubflow/#github-flow","text":"Some notes about github flow...","title":"Github flow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/","text":"Agenda # Some thoughts about life First things first Different ways of do stuff \"The Flow\" \"Issue oriented coding\" Practice makes the master Some thoughts about life # Git is a collaboration tool # Git is a version control system that allows you to take snapshots of your project so you can go back in time or safely add features safely without breaking your code. This is much better then taking turns working on software and emailing version back to eachother. Git can be used for any project involving plain text files, including code, books, papers, small datasets. Git is a communication tool # Git and Github are used for hosting, distributing and colloborating on your project with others. Through tools like GitHub issues, GitHub Pull Requests and branches you can manage large scale collaborations. Some examples of open-source projects managed on GitHub: Software Carpentry Git Lesson MDAnalysis Numpy The first golden Rule # You are a team, work as a team You are physically located at the same table, so talk with your teammate Know what he/she doing, Collaborate, Code review (and learn), Pair programming (and learn) First things first # Open a terminal \u2026 Command line magic Use the up-and-down keys to scroll through the history of our commands `TAB` completion `Ctrl-R` to recursively search for previous commands Who am I? Where am I? `whoami` `pwd` git <> github Try zsh or oh-my-zsh My workspace # # How my git configuration looks like? git config --list My workspace # # Adding some, if you don't have a user.name or user.email set git config --global user.name \"Rayid Ghani\" git config --global user.email \"rayid.ghani@dssg.io\" git config --global color.ui \"auto\" git config --global core.editor 'subl -n -w' #or nano, vim, emacs My workspace # ## This is very important!! ## YOU NEED TO DO THIS git config --global push.default current Different ways of do things # Different ways of do things # \"Solo\" workflow Open source workflow Git flow Github flow \"Solo\" workflow # mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" # hack # more hack ... This works if you are working by yourself This is not our case Opensource workflow # Some differences: You need a github account Choose a repository in github.com in which you want to participate Lets pretend: http://github.com/the-repo-you-want-to-participate Push the Fork button Opensource workflow # Open your terminal \u2026 git clone http://github.com/my-copy-of-the-repo-you-want-to-participate cd the-repo-you-want-to-participate hack # hack # git add some_file.py git commit -m \"Working with some awesome idea\" hack # hack # git push Opensource workflow # Create a pull-request and describe your work Wait Opensource workflow # This works very well if you want to collaborate, but you don't know the other people involved This is not our case And this has several more steps: What if I want to pull the most recent changes in the original repo? See the \"remotes\" # git remote -v Add the original repo # git remote add original-repo http://github.com/the-repo-you-want-to-participate Pull the changes of the original repo to your local copy # git pull original-repo master Push the added changes to your repo # git push origin master etc # More prone to errors, merges, conflicts, etc. Git flow # This is more oriented to software development with a lot of git -gurus involved \"The Flow\" # Github flow # Also know as the Anti-gitflow Github Flow (explained with images and animation!) \"Issue oriented coding\" # The second golden rule # Don't code anything if there is not a need of doing it A good issue # Clear Defined output Actionable (written in the Imperative Voice) Could be completed at most in few days Examples Good : Fix the bug in \u2026 Good : Add a method that does \u2026 Bad : Solve the dssg project Bad : Some error happen x About code reviewing # DEMO # Into \"the flow\" # Github flow # Short-lived branches Pull from the repo # git pull Decide what do you want to do and create an issue # git checkout -b a-meaningful-name hack, hack, hack, add/rm, commit # Push to the repo and create a remote branch # git push Create a pull-request and describe your work (Suggest/add a reviewer) # Code review # The pull-request is closed and the remote branch is destroyed # Switch to master locally # git checkout master Destroy your local branch # git branch -d a-meaningful-name Pull the most recent changes (including yours) # git pull Practice makes the master # Goal: Simulate the creation of a pipeline You will work in your team repo Create a directory called: test The pipeline is composed for 3 steps: They have a function called do_stuff() (no args) This function prints on the screen I'm step X , where X is the number of the step There's a master.py that uses that 3 steps in order. Create an issue for deleting the test directory Delete it Easy right?","title":"Agenda"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#agenda","text":"Some thoughts about life First things first Different ways of do stuff \"The Flow\" \"Issue oriented coding\" Practice makes the master","title":"Agenda"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#some-thoughts-about-life","text":"","title":"Some thoughts about  life"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#git-is-a-collaboration-tool","text":"Git is a version control system that allows you to take snapshots of your project so you can go back in time or safely add features safely without breaking your code. This is much better then taking turns working on software and emailing version back to eachother. Git can be used for any project involving plain text files, including code, books, papers, small datasets.","title":"Git is a collaboration tool"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#git-is-a-communication-tool","text":"Git and Github are used for hosting, distributing and colloborating on your project with others. Through tools like GitHub issues, GitHub Pull Requests and branches you can manage large scale collaborations. Some examples of open-source projects managed on GitHub: Software Carpentry Git Lesson MDAnalysis Numpy","title":"Git is a communication tool"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#the-first-golden-rule","text":"You are a team, work as a team You are physically located at the same table, so talk with your teammate Know what he/she doing, Collaborate, Code review (and learn), Pair programming (and learn)","title":"The first golden Rule"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#first-things-first","text":"Open a terminal \u2026 Command line magic Use the up-and-down keys to scroll through the history of our commands `TAB` completion `Ctrl-R` to recursively search for previous commands Who am I? Where am I? `whoami` `pwd` git <> github Try zsh or oh-my-zsh","title":"First things first"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#my-workspace","text":"# How my git configuration looks like? git config --list","title":"My workspace"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#my-workspace_1","text":"# Adding some, if you don't have a user.name or user.email set git config --global user.name \"Rayid Ghani\" git config --global user.email \"rayid.ghani@dssg.io\" git config --global color.ui \"auto\" git config --global core.editor 'subl -n -w' #or nano, vim, emacs","title":"My workspace"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#my-workspace_2","text":"## This is very important!! ## YOU NEED TO DO THIS git config --global push.default current","title":"My workspace"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#different-ways-of-do-things","text":"","title":"Different ways of do things"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#different-ways-of-do-things_1","text":"\"Solo\" workflow Open source workflow Git flow Github flow","title":"Different ways of do things"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#solo-workflow","text":"mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" # hack # more hack ... This works if you are working by yourself This is not our case","title":"\"Solo\" workflow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#opensource-workflow","text":"Some differences: You need a github account Choose a repository in github.com in which you want to participate Lets pretend: http://github.com/the-repo-you-want-to-participate Push the Fork button","title":"Opensource workflow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#opensource-workflow_1","text":"Open your terminal \u2026 git clone http://github.com/my-copy-of-the-repo-you-want-to-participate cd the-repo-you-want-to-participate","title":"Opensource workflow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#hack","text":"","title":"hack"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#hack_1","text":"git add some_file.py git commit -m \"Working with some awesome idea\"","title":"hack"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#hack_2","text":"","title":"hack"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#hack_3","text":"git push","title":"hack"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#opensource-workflow_2","text":"Create a pull-request and describe your work Wait","title":"Opensource workflow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#opensource-workflow_3","text":"This works very well if you want to collaborate, but you don't know the other people involved This is not our case And this has several more steps: What if I want to pull the most recent changes in the original repo?","title":"Opensource workflow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#see-the-remotes","text":"git remote -v","title":"See the \"remotes\""},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#add-the-original-repo","text":"git remote add original-repo http://github.com/the-repo-you-want-to-participate","title":"Add the original repo"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#pull-the-changes-of-the-original-repo-to-your-local-copy","text":"git pull original-repo master","title":"Pull the changes of the original repo to your local copy"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#push-the-added-changes-to-your-repo","text":"git push origin master","title":"Push the added changes to your repo"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#etc","text":"More prone to errors, merges, conflicts, etc.","title":"etc"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#git-flow","text":"This is more oriented to software development with a lot of git -gurus involved","title":"Git flow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#the-flow","text":"","title":"\"The Flow\""},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#github-flow","text":"Also know as the Anti-gitflow Github Flow (explained with images and animation!)","title":"Github flow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#issue-oriented-coding","text":"","title":"\"Issue oriented coding\""},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#the-second-golden-rule","text":"Don't code anything if there is not a need of doing it","title":"The second golden rule"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#a-good-issue","text":"Clear Defined output Actionable (written in the Imperative Voice) Could be completed at most in few days Examples Good : Fix the bug in \u2026 Good : Add a method that does \u2026 Bad : Solve the dssg project Bad : Some error happen x","title":"A good issue"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#about-code-reviewing","text":"","title":"About code reviewing"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#demo","text":"","title":"DEMO"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#into-the-flow","text":"","title":"Into \"the flow\""},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#github-flow_1","text":"Short-lived branches","title":"Github flow"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#pull-from-the-repo","text":"git pull","title":"Pull from the repo"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#decide-what-do-you-want-to-do-and-create-an-issue","text":"git checkout -b a-meaningful-name","title":"Decide what do you want to do and create an issue"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#hack-hack-hack-addrm-commit","text":"","title":"hack, hack, hack, add/rm, commit"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#push-to-the-repo-and-create-a-remote-branch","text":"git push","title":"Push to the repo and create a remote branch"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#create-a-pull-request-and-describe-your-work-suggestadd-a-reviewer","text":"","title":"Create a pull-request and describe your work (Suggest/add a reviewer)"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#code-review","text":"","title":"Code review"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#the-pull-request-is-closed-and-the-remote-branch-is-destroyed","text":"","title":"The pull-request is closed and the remote branch is destroyed"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#switch-to-master-locally","text":"git checkout master","title":"Switch to master locally"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#destroy-your-local-branch","text":"git branch -d a-meaningful-name","title":"Destroy your local branch"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#pull-the-most-recent-changes-including-yours","text":"git pull","title":"Pull the most recent changes (including yours)"},{"location":"curriculum/setup/git-and-github/githubflow/github-flow/#practice-makes-the-master","text":"Goal: Simulate the creation of a pipeline You will work in your team repo Create a directory called: test The pipeline is composed for 3 steps: They have a function called do_stuff() (no args) This function prints on the screen I'm step X , where X is the number of the step There's a master.py that uses that 3 steps in order. Create an issue for deleting the test directory Delete it Easy right?","title":"Practice makes the master"},{"location":"curriculum/setup/git-and-github/group_tutorial/","text":"Group tutorial # Contents # First steps Show to others Adding on The secret sauce Back to the past MOAR","title":"Group tutorial"},{"location":"curriculum/setup/git-and-github/group_tutorial/#group-tutorial","text":"","title":"Group tutorial"},{"location":"curriculum/setup/git-and-github/group_tutorial/#contents","text":"First steps Show to others Adding on The secret sauce Back to the past MOAR","title":"Contents"},{"location":"curriculum/setup/git-and-github/group_tutorial/01_firststeps/","text":"In this tutorial, we will go through some git basics, using commands that you will use again and again in your work. Git is a pretty amazing tool, and you will only be scratching the surface in the next half hour. Still, you should be able to use git to keep a history of your changes to files, to exchange your work with others, and to build on their changes in return. Don't worry if the commands seem like arcane incantations for now; we will cover many of the concepts that underlie them in the coming weeks. Your group is composed of fellows with different levels of experience with git. Help each other loads, and ask each other all the questions! Commands you'll learn # In this tutorial you'll be using the following commands: * ls -a : List files in a directory ( ls ) even ones that begin with . ( -a ) * cd : Change directories * git clone : Clone a repository from GitHub (or another \"remote\") * git add : Tell git you'd like to save changes to this file in your history * git commit : Tell git you'd like to save the changes you've git add ed * git status : Your favorite command: \"What's going on?!\" Concepts you will learn # Navigating a file system Cloning a repository Taking a snapshot of your work Create a git configuration # #Check to see what your git configuration looks like git config --list It is important to set up your configuration so you commits are attribted to you. #Set up your git configuration git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor \"nano\" #you can set this to whatever text editor you are comfortable with Also really important to do git config --global push.default current Create a repository # First things first: You need to create a repository (which is the place that stores your work and changes). There are several ways to do this, but one of the most convenient ones is to start on GitHub - that way, your repo will be super easy to share with your... fellow fellows. Learner A : Go to github.com , log in, and find the 'New Repository' button. Name your repo 'soup', and give it a short description if you like; the remaining options can be left at their default. (Ask your partners for details if you're interested!) Learner A : Now, you need to get the repo on your local machine! Find (or have your partners point you to) the 'Clone or download' button, and copy the URL that pops up. Then go to your command line and do: git clone https://github.com/username/rhymes.git , where you need to substitute the URL for the one that you just copied from GitHub. This will then create a complete copy of the entire project. Learner A : Once git is done copying all kinds of things from your newly creating repo on github, we can look around a bit. First, go into the new folder that has the same name as your repo: cd soup . Say ls -a ; you will see that git has created the (hidden) directory called .git , where it stores all its inner workings. If you say git remote -v , you can see where you cloned your repo from (and where changes will go to later!). Saying git status tells you that you are at the start of working on this repo, and, importantly, that there is 'nothing to commit'. This means that you haven't told git to keep track of anything yet. Learner A : In your 'rhymes' folder, add a new file called words.md . Open it, and add a few words that rhyme. Pro tip: Ask your partners for suggestions as to what rhymes with 'orange'. Learner A : Do git status again. What has changed? Learner A : You need to tell git that you'd like take a snapshot of your work (which you can later go back to and do all kinds of interesting things with). Say git add words.md . Then check git status again. Do you see how git now tells you how words.md is now among the 'changes to be commited' (i.e., the changes that will go into your snapshot)? Learner A : It's time to take the snapshot! Say git commit -m 'blablabla' , where you should substitute the blablabla with a short description of what you have done in this snapshot, like 'create file with rhymes' . Congratulations! You have made a commit to your repo. Check git status again. Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Go to the next tutorial . Image source","title":"01 firststeps"},{"location":"curriculum/setup/git-and-github/group_tutorial/01_firststeps/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * ls -a : List files in a directory ( ls ) even ones that begin with . ( -a ) * cd : Change directories * git clone : Clone a repository from GitHub (or another \"remote\") * git add : Tell git you'd like to save changes to this file in your history * git commit : Tell git you'd like to save the changes you've git add ed * git status : Your favorite command: \"What's going on?!\"","title":"Commands you'll learn"},{"location":"curriculum/setup/git-and-github/group_tutorial/01_firststeps/#concepts-you-will-learn","text":"Navigating a file system Cloning a repository Taking a snapshot of your work","title":"Concepts you will learn"},{"location":"curriculum/setup/git-and-github/group_tutorial/01_firststeps/#create-a-git-configuration","text":"#Check to see what your git configuration looks like git config --list It is important to set up your configuration so you commits are attribted to you. #Set up your git configuration git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor \"nano\" #you can set this to whatever text editor you are comfortable with Also really important to do git config --global push.default current","title":"Create a git configuration"},{"location":"curriculum/setup/git-and-github/group_tutorial/01_firststeps/#create-a-repository","text":"First things first: You need to create a repository (which is the place that stores your work and changes). There are several ways to do this, but one of the most convenient ones is to start on GitHub - that way, your repo will be super easy to share with your... fellow fellows. Learner A : Go to github.com , log in, and find the 'New Repository' button. Name your repo 'soup', and give it a short description if you like; the remaining options can be left at their default. (Ask your partners for details if you're interested!) Learner A : Now, you need to get the repo on your local machine! Find (or have your partners point you to) the 'Clone or download' button, and copy the URL that pops up. Then go to your command line and do: git clone https://github.com/username/rhymes.git , where you need to substitute the URL for the one that you just copied from GitHub. This will then create a complete copy of the entire project. Learner A : Once git is done copying all kinds of things from your newly creating repo on github, we can look around a bit. First, go into the new folder that has the same name as your repo: cd soup . Say ls -a ; you will see that git has created the (hidden) directory called .git , where it stores all its inner workings. If you say git remote -v , you can see where you cloned your repo from (and where changes will go to later!). Saying git status tells you that you are at the start of working on this repo, and, importantly, that there is 'nothing to commit'. This means that you haven't told git to keep track of anything yet. Learner A : In your 'rhymes' folder, add a new file called words.md . Open it, and add a few words that rhyme. Pro tip: Ask your partners for suggestions as to what rhymes with 'orange'. Learner A : Do git status again. What has changed? Learner A : You need to tell git that you'd like take a snapshot of your work (which you can later go back to and do all kinds of interesting things with). Say git add words.md . Then check git status again. Do you see how git now tells you how words.md is now among the 'changes to be commited' (i.e., the changes that will go into your snapshot)? Learner A : It's time to take the snapshot! Say git commit -m 'blablabla' , where you should substitute the blablabla with a short description of what you have done in this snapshot, like 'create file with rhymes' . Congratulations! You have made a commit to your repo. Check git status again. Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Go to the next tutorial . Image source","title":"Create a repository"},{"location":"curriculum/setup/git-and-github/group_tutorial/02_showtheothers/","text":"Learner A has done some work on the repo. It's time to share that work with the other two! Commands you'll learn # In this tutorial you'll be using the following commands: * git log : Show a list of your commits so far * git push : Sync your commits back to your repo's GitHub page (or another 'remote') Share What You Did # Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Learner A : Now, it's time to sync your commit back to GitHub. Say git push origin master . Go to your repo's webpage on GitHub and refresh it. See your rhymes? Learner B and C : Clone Learner A's repo. Show Learner A the local files that you got from cloning. Show Learner A a git log on your clone. The commit message is there, too! Learner B and C : High five Learner A. (And Matt and Joe. They love that.) Go to the next tutorial .","title":"02 showtheothers"},{"location":"curriculum/setup/git-and-github/group_tutorial/02_showtheothers/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * git log : Show a list of your commits so far * git push : Sync your commits back to your repo's GitHub page (or another 'remote')","title":"Commands you'll learn"},{"location":"curriculum/setup/git-and-github/group_tutorial/02_showtheothers/#share-what-you-did","text":"Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Learner A : Now, it's time to sync your commit back to GitHub. Say git push origin master . Go to your repo's webpage on GitHub and refresh it. See your rhymes? Learner B and C : Clone Learner A's repo. Show Learner A the local files that you got from cloning. Show Learner A a git log on your clone. The commit message is there, too! Learner B and C : High five Learner A. (And Matt and Joe. They love that.) Go to the next tutorial .","title":"Share What You Did"},{"location":"curriculum/setup/git-and-github/group_tutorial/03_addingon/","text":"While having a repo for yourself is nice for version control, how do you add to somebody else's work? There are some pretty neat ways of handling this, but right now, we'll go through one of the simplest. Commands you'll learn # In this tutorial you'll be using the following commands: * git pull : Get changes from the repo's GitHub page (or another 'remote') into your local folder * git push : Sync your commits back to your repo's GitHub page (or another 'remote') Adding to It # Learner C : Create a new repo called 'soup', and add Learner A and Learner B as collaborators. Learner C : Add a file ingredients.md . Populate it with a short list of ingredients that would make a medieval witch scratch her head. Commit your list. Add a few more ingredients (or delete some!). Commit again. Push your commits with git push origin master . Learners A and B : Get the necessary URL from Learner C, then clone Learner C's 'soup' repo ( git clone https://github.com/learnerc/soup.git , where you need to substitute the correct URL), and go into the repo's folder: cd soup/ Learners A and B : Do git log to see what Learner C has been up to so far. Learner C : Add another ingredient to your file, commit and push again. Learner A and B : As Learner C has added more stuff, you need to update your local clone of their repository! To do this, call git pull . Check what just happened with git log . Do you see the new commit that Learner A only added to their repo after you first cloned it? Learner A : In the repo's folder, create a new file called spices.md . In that file, list some fantasmorgacically hot spices. Do a git status . What does have git to say about your new file so far? Learner A : Do git add spices.md to tell git that it should prepare for a snapshot of your changes to that file. (Just like in the first tutorial.) Check git status . Learner A : Do git commit -m 'blablabla' to create a new snapshot (commit) of the repo as you have it now, which includes your new file now! You should probably make the commit message more meaningful than 'blablabla'. Then, push your changes: git push origin master , to synchronize your local commits with your repo on GitHub (your 'remote'). Learner B and C : Pull the changes that Learner A made, and show Learner A that you now have the spice list. Also show Learner A your git log . Learner A's commit is there! (Great success. ) Here is the next tutorial . Talking of spicy food, here is YouTube channel of a Dutchman eating chili peppers while interviewing people .","title":"03 addingon"},{"location":"curriculum/setup/git-and-github/group_tutorial/03_addingon/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * git pull : Get changes from the repo's GitHub page (or another 'remote') into your local folder * git push : Sync your commits back to your repo's GitHub page (or another 'remote')","title":"Commands you'll learn"},{"location":"curriculum/setup/git-and-github/group_tutorial/03_addingon/#adding-to-it","text":"Learner C : Create a new repo called 'soup', and add Learner A and Learner B as collaborators. Learner C : Add a file ingredients.md . Populate it with a short list of ingredients that would make a medieval witch scratch her head. Commit your list. Add a few more ingredients (or delete some!). Commit again. Push your commits with git push origin master . Learners A and B : Get the necessary URL from Learner C, then clone Learner C's 'soup' repo ( git clone https://github.com/learnerc/soup.git , where you need to substitute the correct URL), and go into the repo's folder: cd soup/ Learners A and B : Do git log to see what Learner C has been up to so far. Learner C : Add another ingredient to your file, commit and push again. Learner A and B : As Learner C has added more stuff, you need to update your local clone of their repository! To do this, call git pull . Check what just happened with git log . Do you see the new commit that Learner A only added to their repo after you first cloned it? Learner A : In the repo's folder, create a new file called spices.md . In that file, list some fantasmorgacically hot spices. Do a git status . What does have git to say about your new file so far? Learner A : Do git add spices.md to tell git that it should prepare for a snapshot of your changes to that file. (Just like in the first tutorial.) Check git status . Learner A : Do git commit -m 'blablabla' to create a new snapshot (commit) of the repo as you have it now, which includes your new file now! You should probably make the commit message more meaningful than 'blablabla'. Then, push your changes: git push origin master , to synchronize your local commits with your repo on GitHub (your 'remote'). Learner B and C : Pull the changes that Learner A made, and show Learner A that you now have the spice list. Also show Learner A your git log . Learner A's commit is there! (Great success. ) Here is the next tutorial . Talking of spicy food, here is YouTube channel of a Dutchman eating chili peppers while interviewing people .","title":"Adding to It"},{"location":"curriculum/setup/git-and-github/group_tutorial/04_thesecretsauce/","text":"Sometimes, you want to keep a files in your repo's directory local, and not share it. Git has an app, errm, file, for that! Commands you'll learn # In this tutorial you'll be using the following commands: * .gitignore : A file that lists files (or file and folder name patterns) that should be excluded from commits. Keeping It Special # Learner B : You would like to add an ingredient to the soup which should not be mentioned to the public, or your partners. Create a file sauce.secret , and name your secret ingredient in it. Do a git status . What does git think about your new file? Learner B : Create a .gitignore file, and add a line saying *.secret to it. Do git status again. What has changed? Your secret file should no longer appear there. Learner B : Tell git that you want it to include the .gitignore file in your commit: git add .gitignore . Then commit and push your changes. Learner A and C : Get the changes that Learner B just made: Say git pull . List all files (includen hidden ones) with ls -a . Did you receive the .gitignore ? What about the sauce.secret ? Which of those two files appears in the repo on GitHub? Also have a look at your git log . Here is the next tutorial .","title":"04 thesecretsauce"},{"location":"curriculum/setup/git-and-github/group_tutorial/04_thesecretsauce/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * .gitignore : A file that lists files (or file and folder name patterns) that should be excluded from commits.","title":"Commands you'll learn"},{"location":"curriculum/setup/git-and-github/group_tutorial/04_thesecretsauce/#keeping-it-special","text":"Learner B : You would like to add an ingredient to the soup which should not be mentioned to the public, or your partners. Create a file sauce.secret , and name your secret ingredient in it. Do a git status . What does git think about your new file? Learner B : Create a .gitignore file, and add a line saying *.secret to it. Do git status again. What has changed? Your secret file should no longer appear there. Learner B : Tell git that you want it to include the .gitignore file in your commit: git add .gitignore . Then commit and push your changes. Learner A and C : Get the changes that Learner B just made: Say git pull . List all files (includen hidden ones) with ls -a . Did you receive the .gitignore ? What about the sauce.secret ? Which of those two files appears in the repo on GitHub? Also have a look at your git log . Here is the next tutorial .","title":"Keeping It Special"},{"location":"curriculum/setup/git-and-github/group_tutorial/05_backtothepast/","text":"Sometimes, you check out a repo, happily working along, and then just screw your files up badly. Let's do that. Commands you'll learn # In this tutorial you'll be using the following commands: * git checkout : Take a snapshot (commit) and make your working directory look like it. I Just Want To Go Back # Learner A, B, and C : Go to your 'soup' repo from the previous exercise. Just to be sure that everybody is up to date, everybody should call git pull . You can also have a look at your git log ; they should all look the same. Learner A, B, and C : Individually, do something bad to your local files (only in the repo, of course!). Delete the ingredients.md , or delete lines in one of the files, and then save them. Learner B ! Do not delete your sauce.secret ! It has never been committed, so you cannot get it back! Learner A, B, and C : Now, again individually, you decide that those working changes were really not great, and you want to restore the last commit. There are several ways of doing this in git. Say git status - it will recommend one possible way. Learner A, B, and C : As git status tells you, you can use git checkout -- <filename> to discard your working changes. If you call git checkout -- . , you will discard all your working changes in your current directory, and thus be thrown back to the last commit. Be super careful. This discards your working changes. They will be lost. (If you feel like it, google git stash at this point if you would like to back up your working changes, just to be safe.) Learner A, B, and C : What do you do if you have committed changes that you would like to undo? That is, if you want to go back to a commit far back in time? There are really neat ways of doing this in git, even if you have already pushed (shared) your newer commits to GitHub. Cliffhanger: You will learn about this in the git branching tutorial. ( git revert and git reset is where it's at.) Image source","title":"05 backtothepast"},{"location":"curriculum/setup/git-and-github/group_tutorial/05_backtothepast/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * git checkout : Take a snapshot (commit) and make your working directory look like it.","title":"Commands you'll learn"},{"location":"curriculum/setup/git-and-github/group_tutorial/05_backtothepast/#i-just-want-to-go-back","text":"Learner A, B, and C : Go to your 'soup' repo from the previous exercise. Just to be sure that everybody is up to date, everybody should call git pull . You can also have a look at your git log ; they should all look the same. Learner A, B, and C : Individually, do something bad to your local files (only in the repo, of course!). Delete the ingredients.md , or delete lines in one of the files, and then save them. Learner B ! Do not delete your sauce.secret ! It has never been committed, so you cannot get it back! Learner A, B, and C : Now, again individually, you decide that those working changes were really not great, and you want to restore the last commit. There are several ways of doing this in git. Say git status - it will recommend one possible way. Learner A, B, and C : As git status tells you, you can use git checkout -- <filename> to discard your working changes. If you call git checkout -- . , you will discard all your working changes in your current directory, and thus be thrown back to the last commit. Be super careful. This discards your working changes. They will be lost. (If you feel like it, google git stash at this point if you would like to back up your working changes, just to be safe.) Learner A, B, and C : What do you do if you have committed changes that you would like to undo? That is, if you want to go back to a commit far back in time? There are really neat ways of doing this in git, even if you have already pushed (shared) your newer commits to GitHub. Cliffhanger: You will learn about this in the git branching tutorial. ( git revert and git reset is where it's at.) Image source","title":"I Just Want To Go Back"},{"location":"curriculum/setup/git-and-github/group_tutorial/06_moar/","text":"Interesting Stuff To Do # If you're familar with git already, do two things: Set up a repo and share it with your collaborators. Practice creating branches, and then submitting pull requests to master for your changes. Somebody else should approve and merge your changes. Rotate, so that everybody in your group gets to approve somebody else pull request. Individually, do this awesome tutorial .","title":"Interesting Stuff To Do"},{"location":"curriculum/setup/git-and-github/group_tutorial/06_moar/#interesting-stuff-to-do","text":"If you're familar with git already, do two things: Set up a repo and share it with your collaborators. Practice creating branches, and then submitting pull requests to master for your changes. Somebody else should approve and merge your changes. Rotate, so that everybody in your group gets to approve somebody else pull request. Individually, do this awesome tutorial .","title":"Interesting Stuff To Do"},{"location":"curriculum/setup/software-setup/","text":"Software to Install # In order to be ready for the summer, you need to install some packages on your computer: Required # Git (for version control) DBeaver (GUI to access various databases) Python tools pyenv + virtualenv for python version management Python 3.8 or higher Python Packages pandas matplotlib seaborn scikit-learn psycopg2 ipython jupyterlab SSH Keypair Text Editor for Coding (your favorite, or VSCode) Highly Recommended # psql (PostgreSQL command line interface) Tableau (students can request a free education license) Installing these pre-requisites # OS X users - Follow these instructions Linux users - You probably know how to do it, but you can follow the OS X instructions substituting your appropriate package manage for homebrew Windows users - Follow these instructions Running into setup issues? # Feel free to post your questions in the #tech_help channel on the slack workspace for the summer. We'll also have some tech setup help sessions to resolve any lingering setup issues (and help you get familiar with the remote servers we'll be using for the projects) during the first week of the fellowship.","title":"Software you need"},{"location":"curriculum/setup/software-setup/#software-to-install","text":"In order to be ready for the summer, you need to install some packages on your computer:","title":"Software to Install"},{"location":"curriculum/setup/software-setup/#required","text":"Git (for version control) DBeaver (GUI to access various databases) Python tools pyenv + virtualenv for python version management Python 3.8 or higher Python Packages pandas matplotlib seaborn scikit-learn psycopg2 ipython jupyterlab SSH Keypair Text Editor for Coding (your favorite, or VSCode)","title":"Required"},{"location":"curriculum/setup/software-setup/#highly-recommended","text":"psql (PostgreSQL command line interface) Tableau (students can request a free education license)","title":"Highly Recommended"},{"location":"curriculum/setup/software-setup/#installing-these-pre-requisites","text":"OS X users - Follow these instructions Linux users - You probably know how to do it, but you can follow the OS X instructions substituting your appropriate package manage for homebrew Windows users - Follow these instructions","title":"Installing these pre-requisites"},{"location":"curriculum/setup/software-setup/#running-into-setup-issues","text":"Feel free to post your questions in the #tech_help channel on the slack workspace for the summer. We'll also have some tech setup help sessions to resolve any lingering setup issues (and help you get familiar with the remote servers we'll be using for the projects) during the first week of the fellowship.","title":"Running into setup issues?"},{"location":"curriculum/setup/software-setup/further_resources/","text":"Further Resources # Some of the tools we'll be using at DSSG might be new to you, but don't worry if so -- this summer will be a great opportunity to pick up some new skills that will be useful in the future as well! Below are a few resources we've collected to help you get familiar with some of these tools: Terminal and the Command Line # Hitchhiker's Guide intro to the command line General command line navigation Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Git and GitHub # Hitchhiker's Guide intro to git Installing Git Complete Beginner\u2019s Guide to Git and Github 10-minute Hello World Tutorial to using Git and Github Github\u2019s interactive web tutorial Python # Python Programming Resources: Hitchhiker's Guide intro to python Writing efficient Python Tips for Idiomatic Python Introduction to Python debugging tools Jupyter Notebooks and Jupyter Lab Beginner's Guide to Jupyter Lab Example of how IPython notebooks are used in data science Tutorial for setting up and opening IPython notebook Amazing examples of IPython notebooks Databases and SQL # Hitchhiker's Guide intro to SQL Another useful SQL introduction SQL Cheatsheet Machine Learning Concepts # A few useful things to know about machine learning Survey of machine learning tools for social scientists Machine Learning book chapter from Big Data and Social Science Causal Inference # Intro to Causal Inference Causal Inference in Social Science","title":"Further Resources"},{"location":"curriculum/setup/software-setup/further_resources/#further-resources","text":"Some of the tools we'll be using at DSSG might be new to you, but don't worry if so -- this summer will be a great opportunity to pick up some new skills that will be useful in the future as well! Below are a few resources we've collected to help you get familiar with some of these tools:","title":"Further Resources"},{"location":"curriculum/setup/software-setup/further_resources/#terminal-and-the-command-line","text":"Hitchhiker's Guide intro to the command line General command line navigation Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line.","title":"Terminal and the Command Line"},{"location":"curriculum/setup/software-setup/further_resources/#git-and-github","text":"Hitchhiker's Guide intro to git Installing Git Complete Beginner\u2019s Guide to Git and Github 10-minute Hello World Tutorial to using Git and Github Github\u2019s interactive web tutorial","title":"Git and GitHub"},{"location":"curriculum/setup/software-setup/further_resources/#python","text":"Python Programming Resources: Hitchhiker's Guide intro to python Writing efficient Python Tips for Idiomatic Python Introduction to Python debugging tools Jupyter Notebooks and Jupyter Lab Beginner's Guide to Jupyter Lab Example of how IPython notebooks are used in data science Tutorial for setting up and opening IPython notebook Amazing examples of IPython notebooks","title":"Python"},{"location":"curriculum/setup/software-setup/further_resources/#databases-and-sql","text":"Hitchhiker's Guide intro to SQL Another useful SQL introduction SQL Cheatsheet","title":"Databases and SQL"},{"location":"curriculum/setup/software-setup/further_resources/#machine-learning-concepts","text":"A few useful things to know about machine learning Survey of machine learning tools for social scientists Machine Learning book chapter from Big Data and Social Science","title":"Machine Learning Concepts"},{"location":"curriculum/setup/software-setup/further_resources/#causal-inference","text":"Intro to Causal Inference Causal Inference in Social Science","title":"Causal Inference"},{"location":"curriculum/setup/software-setup/setup_osx/","text":"Setting up your MacOS X machine # This guide helps you walk through how to set up the various technical tools you'll need for the summer and is focused on MacOS X users (if you're on Windows, see the related guide here ; if you're on Linux, most of the instructions here should work with the appropriate package manager depending on your distribution) Package Manager # We'll use a package manager called Homebrew to manage the installation of many of the other tools we'll need below. To get started, install Homebrew from the terminal: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" To check that it installed, run the command which brew in the terminal. If it returns: /usr/local/bin/brew , it means that homebrew is installed; if it returns brew not found , it means homebrew is not installed. If that doesn't work, you need to add the following to your PATH: export PATH = /usr/local/bin:/usr/local/sbin: $PATH If you don't know how to do that, run this: echo 'export PATH=/usr/local/bin:/usr/local/sbin:$PATH' >> ~/.profile Learning more about working at the command line # If you haven't used the terminal/command line much, here are a few resources that might be helpful to explore. Git and GitHub Account # Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. If you don't have a GitHub account, make one ! Install git using homebrew: In the terminal, type: brew update brew install git Test your installation. For example, create a directory, and make it a git repo: mkdir mytestdir cd mytestdir/ git init > Initialized empty Git repository in [...]/mytestdir/.git/ You can un-git the directory by deleting the .git folder: rm -r .git (or simply delete mytestdir entirely with command rmdir mytestdir ). Learning more about git # If you haven't used git/github before, here are a couple of useful resources where you can learn a bit more. Python # We'll primarily use the python programming language for scripting, doing analyses, and building models throughout the summer, so let's make sure we have the right version and packages installed. pyenv vs anaconda This is a contentious topic! Some people argue that they find Anaconda ( conda or mini-conda ) easier to get up and running while others argue for the consistency and flexibility of pyenv . In general, python's library system is a bit of a mess and in constant evolution. We favor pyenv here, since we think it provides you with more flexibility and teaches you about how python works. Note that in MacOS you will already have python installed, but that python is the one use by your operative system for doing stuff, so probably you don\u2019t want to mess with it. Instead, we will install a different python , for you exclusive use. First we will install some libraries 1 : xcode-select --install brew install openssl readline sqlite3 xz zlib To manage different python versions and virtual environments, we will install a tool called pyenv $ curl https://pyenv.run | bash This command will generate some instructions at the end (mostly about adding some lines to your .bashrc , .bash_profile or .zsh_profile or similar.) Follow these then restart your terminal. Generally, this will look something like: And add the following lines to the end of your .bashrc : # pyenv export PYENV_ROOT=\"$HOME/.pyenv\" export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" To test things out, restart your terminal then type which pyenv to make sure the system can find pyenv and pyenv versions to see the currently-installed python versions. Note that if you're using a shell other than bash , you might need to check out the instructions here (if you're just using the default that came with terminal, it's most likely bash). Virtual environment # As a last step, we will create a virtual environment . A virtual environment is a tool that helps to keep dependencies required by different projects separate. By default, every project on your system will use the same directory to store and retrieve third party libraries (called site packages). A virtual environment helps avoid conflicts between requirements for different projects and it isolates dependencies. For example, different projects may use different versions of Python. To create an environment called dssg-3.8.10 with Python 3.8.10 in pyenv , install the python version: $ pyenv install 3.8.10 This will take several minutes. Once complete, create the environment $ pyenv virtualenv 3.8.10 dssg-3.8.10 And then assign it as the virtual environment to use in your directory of choice with $ echo dssg-3.8.10 > .python-version This will ensure that whenever you are inside that directory, the dssg-3.8.10 environment will be activated. If not, you can manually activate the environment: $ activate dssg-3.8.10 Depending on your command shell ( bash , zsh , csh , etc) configuration you should get some info that the environment is in use, if not you can check it with $ pyenv version dssg-3.8.10 (set by /home/user/projects/.python-version) Package installations # Packages are installed using pip. To install a single package: $ pip install pandas To install many packages at once, list all the packages needed in a file (usually called requirements.txt ), navigate to the folder of the file and execute $ pip install -r requirements.txt To try it out, use this file: requirements.txt . Jupyter Jupyter notebooks are a convenient environment for experimentation, prototyping, and sharing exploratory work. Jupyter notebooks require a kernel that executes the code. It should link to the virtual environment: $ pyenv activate dssg-3.8.10 $ python -m ipykernel install --user --name=dssg-3.8.10 --display-name \"dssg-3.8.10-env\" It's time to test! In order to test that both jupyter and the python packages installed appropriately, you should do the following: Download the file SoftwareSetup.ipynb into your directory. Type in the terminal $ jupyter lab Your browser will open a new tab with the jupyterlab interface. Click on SoftwareSetup.ipynb to open the notebook Follow the instructions in the notebook to run each cell. Learning more about python # Python is a powerful, expressive, and easy to read (even by non-programmers) programming language. If you're still relatively new to it, you might find some of the resources here helpful. SSH # SSH helps you access the remote servers using your laptop. For this to work, we generate a key-pair that consists of a Public Key (something that you would share with the server), and a private key (something that you would NEVER share with anyone!). Inside your terminal, we can use a built-in utility to generate a new keypair: $ ssh-keygen This will prompt you to select a location for storing the key (the default is generally fine, but you may want to give it a different name if you already have an existing keypair), and give you the option to add a passphrase to the key. If you want to use the default locaion and not use a passphrase, you just have to hit return. Then, your keys will be stored in the place your specified. By default, - there'll be a .ssh folder in your home directory ~/.ssh/ - private key would be named id_rsa - public key would be named id_rsa.pub You've successfully generated the Keys! After having generated the key pair, you should set the correct file permissions for your private key: SSH requires that only you, the owner, are able to read/write it, and will give you an error otherwise. You can set the right permissions with this command: chmod 600 ~/.ssh/nameofyourprivatekey (where you'll have to substitute in the path and name of your private key that you chose during key generation). Text Editor # You'll need a good text editor for all the amazing code you're going to write this summer. If you've already got one you like (vim, emacs, sublime, etc), that's great! If not, we reccomend using VSCode , which has some great functionality for complex projects, including github integrations and allowing you to work on files directly on a remote server. You can download and install VSCode here . As we said above, one of the most useful features of VSCode is that it let's you edit code directly on a remote server using SSH. To use this feature, you should install the Remote-SSH extention for VSCode . Database Tools # The data for most of the projects will be stored in a PostgreSQL database, and you'll need software on your computer to be able to access it. You won't be able to access the database itself until you get to DSSG, so for now we'll just set up the tools you'll need, and then walk through setting up your connection at the beginning of the summer. DBeaver # There are several GUI tools for connecting to databases, including DBeaver, DataGrip, DBVizualizer, etc. For the summer, we'll prefer DBeaver, which you can install directly from the DBeaver Website , since it offers a free version and works with every operating system. NOTE: you'll want to install the free \"community edition\" version. psql # There is also a command-line tool for accessing postgres databases called psql , which can be useful for quickly or programmatically working with the database as well. To get it, you'll need to install the libpq library: $ brew update $ brew install libpq Note for Linux Users Most of the instructions for OS X are similar for linux. However, for installing the postgresql client on linux, they're actually a bit different: $ sudo apt-get update $ sudo apt-get install postgresql-client Unfortunately, this won't add the location of the psql client to your path, so you'll need to do so manually. If you're on an intel-based mac, this should be: echo 'export PATH=\"/usr/local/opt/libpq/bin:$PATH\"' >> ~/.bash_profile source ~/.bash_profile If you're on a mac with apple silicon (e.g. M1), this should be: echo 'export PATH=\"/opt/homebrew/opt/libpq/bin:$PATH\"' >> ~/.bash_profile source ~/.bash_profile To tesst it out, try typing psql into the terminal, you should see the following output. $ psql psql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"? Learning more about databases and sql # SQL can provide a very efficient way to process large amounts of data quickly, for instance, for ingesting/cleaning raw inputs, performing data exploration, building features for modeling, and keeping track of model artifacts and results. If you're still relatively new to using relational databases, you might find some of the resources here helpful. Congratulations -- You Made It! # Good news -- that's it in terms of software setup (for now)! Take some time to familiarize yourself with them before the summer, and check out the resources here for some helpful guides. On that page, you'll also find some good background information on machine learning concepts and causal inference which may be helpful as well. for an updated version of this instructions and troubleshooting FAQs see this page \u21a9","title":"Setting up your MacOS X machine"},{"location":"curriculum/setup/software-setup/setup_osx/#setting-up-your-macos-x-machine","text":"This guide helps you walk through how to set up the various technical tools you'll need for the summer and is focused on MacOS X users (if you're on Windows, see the related guide here ; if you're on Linux, most of the instructions here should work with the appropriate package manager depending on your distribution)","title":"Setting up your MacOS X machine"},{"location":"curriculum/setup/software-setup/setup_osx/#package-manager","text":"We'll use a package manager called Homebrew to manage the installation of many of the other tools we'll need below. To get started, install Homebrew from the terminal: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" To check that it installed, run the command which brew in the terminal. If it returns: /usr/local/bin/brew , it means that homebrew is installed; if it returns brew not found , it means homebrew is not installed. If that doesn't work, you need to add the following to your PATH: export PATH = /usr/local/bin:/usr/local/sbin: $PATH If you don't know how to do that, run this: echo 'export PATH=/usr/local/bin:/usr/local/sbin:$PATH' >> ~/.profile","title":"Package Manager"},{"location":"curriculum/setup/software-setup/setup_osx/#learning-more-about-working-at-the-command-line","text":"If you haven't used the terminal/command line much, here are a few resources that might be helpful to explore.","title":"Learning more about working at the command line"},{"location":"curriculum/setup/software-setup/setup_osx/#git-and-github-account","text":"Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. If you don't have a GitHub account, make one ! Install git using homebrew: In the terminal, type: brew update brew install git Test your installation. For example, create a directory, and make it a git repo: mkdir mytestdir cd mytestdir/ git init > Initialized empty Git repository in [...]/mytestdir/.git/ You can un-git the directory by deleting the .git folder: rm -r .git (or simply delete mytestdir entirely with command rmdir mytestdir ).","title":"Git and GitHub Account"},{"location":"curriculum/setup/software-setup/setup_osx/#learning-more-about-git","text":"If you haven't used git/github before, here are a couple of useful resources where you can learn a bit more.","title":"Learning more about git"},{"location":"curriculum/setup/software-setup/setup_osx/#python","text":"We'll primarily use the python programming language for scripting, doing analyses, and building models throughout the summer, so let's make sure we have the right version and packages installed. pyenv vs anaconda This is a contentious topic! Some people argue that they find Anaconda ( conda or mini-conda ) easier to get up and running while others argue for the consistency and flexibility of pyenv . In general, python's library system is a bit of a mess and in constant evolution. We favor pyenv here, since we think it provides you with more flexibility and teaches you about how python works. Note that in MacOS you will already have python installed, but that python is the one use by your operative system for doing stuff, so probably you don\u2019t want to mess with it. Instead, we will install a different python , for you exclusive use. First we will install some libraries 1 : xcode-select --install brew install openssl readline sqlite3 xz zlib To manage different python versions and virtual environments, we will install a tool called pyenv $ curl https://pyenv.run | bash This command will generate some instructions at the end (mostly about adding some lines to your .bashrc , .bash_profile or .zsh_profile or similar.) Follow these then restart your terminal. Generally, this will look something like: And add the following lines to the end of your .bashrc : # pyenv export PYENV_ROOT=\"$HOME/.pyenv\" export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\" To test things out, restart your terminal then type which pyenv to make sure the system can find pyenv and pyenv versions to see the currently-installed python versions. Note that if you're using a shell other than bash , you might need to check out the instructions here (if you're just using the default that came with terminal, it's most likely bash).","title":"Python"},{"location":"curriculum/setup/software-setup/setup_osx/#virtual-environment","text":"As a last step, we will create a virtual environment . A virtual environment is a tool that helps to keep dependencies required by different projects separate. By default, every project on your system will use the same directory to store and retrieve third party libraries (called site packages). A virtual environment helps avoid conflicts between requirements for different projects and it isolates dependencies. For example, different projects may use different versions of Python. To create an environment called dssg-3.8.10 with Python 3.8.10 in pyenv , install the python version: $ pyenv install 3.8.10 This will take several minutes. Once complete, create the environment $ pyenv virtualenv 3.8.10 dssg-3.8.10 And then assign it as the virtual environment to use in your directory of choice with $ echo dssg-3.8.10 > .python-version This will ensure that whenever you are inside that directory, the dssg-3.8.10 environment will be activated. If not, you can manually activate the environment: $ activate dssg-3.8.10 Depending on your command shell ( bash , zsh , csh , etc) configuration you should get some info that the environment is in use, if not you can check it with $ pyenv version dssg-3.8.10 (set by /home/user/projects/.python-version)","title":"Virtual environment"},{"location":"curriculum/setup/software-setup/setup_osx/#package-installations","text":"Packages are installed using pip. To install a single package: $ pip install pandas To install many packages at once, list all the packages needed in a file (usually called requirements.txt ), navigate to the folder of the file and execute $ pip install -r requirements.txt To try it out, use this file: requirements.txt .","title":"Package installations"},{"location":"curriculum/setup/software-setup/setup_osx/#learning-more-about-python","text":"Python is a powerful, expressive, and easy to read (even by non-programmers) programming language. If you're still relatively new to it, you might find some of the resources here helpful.","title":"Learning more about python"},{"location":"curriculum/setup/software-setup/setup_osx/#ssh","text":"SSH helps you access the remote servers using your laptop. For this to work, we generate a key-pair that consists of a Public Key (something that you would share with the server), and a private key (something that you would NEVER share with anyone!). Inside your terminal, we can use a built-in utility to generate a new keypair: $ ssh-keygen This will prompt you to select a location for storing the key (the default is generally fine, but you may want to give it a different name if you already have an existing keypair), and give you the option to add a passphrase to the key. If you want to use the default locaion and not use a passphrase, you just have to hit return. Then, your keys will be stored in the place your specified. By default, - there'll be a .ssh folder in your home directory ~/.ssh/ - private key would be named id_rsa - public key would be named id_rsa.pub You've successfully generated the Keys! After having generated the key pair, you should set the correct file permissions for your private key: SSH requires that only you, the owner, are able to read/write it, and will give you an error otherwise. You can set the right permissions with this command: chmod 600 ~/.ssh/nameofyourprivatekey (where you'll have to substitute in the path and name of your private key that you chose during key generation).","title":"SSH"},{"location":"curriculum/setup/software-setup/setup_osx/#text-editor","text":"You'll need a good text editor for all the amazing code you're going to write this summer. If you've already got one you like (vim, emacs, sublime, etc), that's great! If not, we reccomend using VSCode , which has some great functionality for complex projects, including github integrations and allowing you to work on files directly on a remote server. You can download and install VSCode here . As we said above, one of the most useful features of VSCode is that it let's you edit code directly on a remote server using SSH. To use this feature, you should install the Remote-SSH extention for VSCode .","title":"Text Editor"},{"location":"curriculum/setup/software-setup/setup_osx/#database-tools","text":"The data for most of the projects will be stored in a PostgreSQL database, and you'll need software on your computer to be able to access it. You won't be able to access the database itself until you get to DSSG, so for now we'll just set up the tools you'll need, and then walk through setting up your connection at the beginning of the summer.","title":"Database Tools"},{"location":"curriculum/setup/software-setup/setup_osx/#dbeaver","text":"There are several GUI tools for connecting to databases, including DBeaver, DataGrip, DBVizualizer, etc. For the summer, we'll prefer DBeaver, which you can install directly from the DBeaver Website , since it offers a free version and works with every operating system. NOTE: you'll want to install the free \"community edition\" version.","title":"DBeaver"},{"location":"curriculum/setup/software-setup/setup_osx/#psql","text":"There is also a command-line tool for accessing postgres databases called psql , which can be useful for quickly or programmatically working with the database as well. To get it, you'll need to install the libpq library: $ brew update $ brew install libpq Note for Linux Users Most of the instructions for OS X are similar for linux. However, for installing the postgresql client on linux, they're actually a bit different: $ sudo apt-get update $ sudo apt-get install postgresql-client Unfortunately, this won't add the location of the psql client to your path, so you'll need to do so manually. If you're on an intel-based mac, this should be: echo 'export PATH=\"/usr/local/opt/libpq/bin:$PATH\"' >> ~/.bash_profile source ~/.bash_profile If you're on a mac with apple silicon (e.g. M1), this should be: echo 'export PATH=\"/opt/homebrew/opt/libpq/bin:$PATH\"' >> ~/.bash_profile source ~/.bash_profile To tesst it out, try typing psql into the terminal, you should see the following output. $ psql psql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"?","title":"psql"},{"location":"curriculum/setup/software-setup/setup_osx/#learning-more-about-databases-and-sql","text":"SQL can provide a very efficient way to process large amounts of data quickly, for instance, for ingesting/cleaning raw inputs, performing data exploration, building features for modeling, and keeping track of model artifacts and results. If you're still relatively new to using relational databases, you might find some of the resources here helpful.","title":"Learning more about databases and sql"},{"location":"curriculum/setup/software-setup/setup_osx/#congratulations-you-made-it","text":"Good news -- that's it in terms of software setup (for now)! Take some time to familiarize yourself with them before the summer, and check out the resources here for some helpful guides. On that page, you'll also find some good background information on machine learning concepts and causal inference which may be helpful as well. for an updated version of this instructions and troubleshooting FAQs see this page \u21a9","title":"Congratulations -- You Made It!"},{"location":"curriculum/setup/software-setup/setup_session_guide/","text":"Software Setup Session # Motivation # Every team will settle on a specific setup with their tech mentors. This setup will determine, for example: which Python version to use versioning via virtual environments maintaining package dependencies continuous testing tools your version control workflow. Today, we're not doing that. We're making sure that everybody has some basic tools they will need for the tutorials and the beginning of the fellowship, and that you can log into the server and database. Hopefully you already got many of these tools set up on your computer before arriving, but we'll quickly check that your local enviroment is working (and resolving any lingering issues), and then talk a bit about some of the remote servers and tools we'll be using throughout the summer. You might not understand every detail of the commands we go through today, but don't worry if not -- we'll go into a lot more detail in the following sessions throughout the week. For today, we just want to be sure everyone has the tools they'll need for working on their projects. Getting help Work through the prerequisites below, making sure that all the software you installed works. Affix three kinds of post-it notes to your laptop: one with your operating system, e.g. Ubuntu 18.04 if you get an app working (e.g. ssh ), write its name on a green post-it and stick it to your screen if you tried - but the app failed - write its name on a red post-it If you're stuck with a step, ask a person with a corresponding green post-it (and preferrably your operating system) for help. The tech mentors will hover around and help with red stickers. You will need a few credentials for training accounts. We'll post them up front. Important The notes below aren't self-explanatory and will not cover all (or even the majority) of errors you might encounter. Make good use of the people in the room! Let's get started! Windows Users: Setting up WSL # We'll be using a lot of unix-based tools, but fortunately windows now provides a tool called \"windows subsystem for linux\" (WSL) that provides support for them. If you're using windows and didn't already set up WSL, let's do that now: WSL Instructions for Windows SSH # The data you'll be using for the summer is generally of a sensitive nature and needs to be protected by remaining in a secure compute environment. As such, all of your work directly with the data will be on one of the remote servers we've set up. Let's see how connecting to those resources works: Creating an SSH Keypair # The setup information we sent out before the summer had some details on creating an SSH keypair, but if you need to revisit it, you can find that here: Instructions for Windows Instructions for MacOS/Linux Reaching the Compute Server # Now let's make sure you can connect to the server. In your terminal, type: ssh {your_andrew_id}@training.dssg.io If you got an error about your ssh key, you might need to tell ssh where to find the private key, which you can do with the -i option: ssh -i {/path/to/your/private_key} {your_andrew_id}@training.dssg.io If you connected successfully, you should see a welcome message and see a prompt like: yourandrewid@dssg-primary: ~$ To confirm that you're connected, let's look at the output of the hostname command: Type hostname at the shell prompt and then hit return Did you get dssg-primary ? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out. PRO tip Your life will be easier if you set up a .ssh/config file Accessing the Database # Reaching the Database from the Compute Server # We'll be using a database running PostgreSQL for much of our project data. One way to connect to the database is via the command line from the server using psql . Since we're already logged onto the server, let's give that a try: $ psql -h db.dssg.io -U {your_andrew_id} food_inspections If all goes well, you should see something like: psql (11.6 (Ubuntu 11.6-1.pgdg18.04+1), server 11.5) SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off) Type \"help\" for help. food_inspections=> Let's make sure you can run interact with the server: Type SELECT CURRENT_USER; and then hit return Did you get back your andrew id? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out. Finally, exit out of psql with \\q : food_inspections=> \\q Your postgres password You might have noticed that psql didn't ask you for a password when you connected to the server. This is because we've stored your password in a configuration file called .pgpass . You can view it at the bash prompt with: $ cat ~/.pgpass Your password is everything after the last colon: db.dssg.io:5432:*:{andrew_id}:{YOUR_PASSWORD} Reaching the Database from Your Computer # Often it can be a little easier to access the database via GUI system from your computer. To do so, we recommend setting up dbeaver, since it offers a free version and works with every operating system (but if you already have a SQL client like dbvisualizer or data grip that you know how to use and prefer, feel free to use that instead). Protecting your data Although it's fine to run queries and look at the data from a SQL client like dbeaver running on your computer, you should never use that client to download an extract of the data (in whole or part) to your local computer, excel, google sheets, etc. To get DBeaver, you can install it directly from the DBeaver Website . NOTE: you'll want to install the free \"community edition\" version. Because the database is only accessible from our compute servers, we'll have to use an SSH Tunnel to connect to it via dbeaver. Here's how to set that up: First, create a new connection to a postgres database: Next, fill in the details for the database server on the window that pops up, following the example below: Note that your password here can be found in the .pgpass file that we looked at on the server. Finally, to actually reach the database server, we need to set up the SSH tunnel. Under the SSH tab, fill in these details: You might want to try clicking Test tunnel configuration to make sure that's working, then Test Configuration ... to ensure you can reach the database. Got an error like invalid privatekey: [B@7696c31f ? You may need to install a different SSH package called \"SSHJ\" -- under the Help menu, choose \"Install new Software\" then search for SSHJ and install the package (you'll need to restart dbeaver). After restarting, choose \u201cSSHJ\u201d in the drop-down under advanced (should be labeled either \u201dImplementation\u201d or \u201cMethod\u201d) when setting up the tunnel Let's make sure you can connect to the database: Try connecting to the database and opening a new sql script Type SELECT CURRENT_USER; and then press Ctrl-Enter to run it Did you get back your andrew id? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out. Text Editor # You'll want a good text editor to write code installed on your local machine. We recommend using VSCode because it's both free and allows you to directly edit code stored on a remote machine over SSH (this comes in very handy since all of your code will need to run on the server in order to keep the data in our secure environment). If you already have a different editor that you prefer (e.g., Sublime, PyCharm, emacs, etc.), you're welcome to use that (though note that you may need do some extra work to make sure you keep your local code in sync with the server). We sent out some instructions on installing VSCode before the summer, but if you need to revisit them, you can find them below (be sure to install the Remote-SSH extension as well): Instructions for Windows Instructions for MacOS/Linux PRO tip You might also want to install the microsoft python extension, which provides some additional features such as improved auto-completion when coding in python. Editing Files Remotely Over SSH # In the same way we set up DBeaver to use SSH to talk to our remote infrastructure, we can set up VSCode to remotely edit files stored on the server using the Remote-SSH extension you should have installed in the step above. Let's set that up and make sure we can talk to the server: Configure our training server as an SSH host: With the SSH plugin installed, we can tell VSCode how to log into the server. In this step we'll be entering our connection string and saving it in a file, making it easy to connect in the future. Press ctrl+shift+p (Linux/Windows) or \u2318+shift+p (MacOS) to open the command pallette, and select Remote-SSH: Connect to Host Select Add New SSH Host... Enter ssh {andrewid}@training.dssg.io (remember from above that you may also need to use the -i parameter to tell ssh where to find your private key: ssh -i {path to your private key} {andrewid}@training.dssg.io ) Select the first option to store your login config: Connect VSCode to the course server: Connect to the CMU Full VPN Press ctrl+shift+p (Linux/Windows) or \u2318+shift+p (MacOS) to open the command pallette, and select Remote-SSH: Connect to Host Select the ssh config we just created: training.dssg.io Enter your private key passcode if VSCode prompts you to (it will open a box at the top of the screen). You should be connected to the training server. This should be indicated in the bottom of your VSCode window: Open a workspace folder: Now that VSCode is connected via SSH, you can browse all of the files and folders on the server. In this step, we select a folder containing some code to edit and test. Select the folder menu button Select Open Folder Select a folder to work in Let's make sure you can connect to the server through VSCode: Were you able to SSH and open the remote food_inspections folder? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out. Access from Off-Campus: CMU VPN # For an additional layer of security, our infrastructure can only be reached from from the CMU campus network. As such, if you're not on campus, you'll need to use the CMU VPN to reach the server: Download Cisco Anyconnect VPN client from here and install it Open the Anyconnect client Enter login credentials (be sure to select the \"Full VPN\" Group): Connect to: vpn.cmu.edu Group: \u201cFull VPN\u201d Username: your andrewid Password: your CMU password Click connect Understanding The \"Big Picture\" # Finally, before we go, let's take a minute to zoom out to the \"big picture\" of the infrastructure we'll be using and see how all these pieces fit together: Other (Optional) Local Tools # Most of your work this summer will be on the remote server for your project, but there are a few of other tools that might be handy to have installed locally if you feel inclined: Python (using pyenv) and jupyterlab [ windows ] [ macos/linux ] Github client [ windows ] [ macos/linux ] psql CLI [ windows ] [ macos/linux ] Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license.","title":"Software setup session"},{"location":"curriculum/setup/software-setup/setup_session_guide/#software-setup-session","text":"","title":"Software Setup Session"},{"location":"curriculum/setup/software-setup/setup_session_guide/#motivation","text":"Every team will settle on a specific setup with their tech mentors. This setup will determine, for example: which Python version to use versioning via virtual environments maintaining package dependencies continuous testing tools your version control workflow. Today, we're not doing that. We're making sure that everybody has some basic tools they will need for the tutorials and the beginning of the fellowship, and that you can log into the server and database. Hopefully you already got many of these tools set up on your computer before arriving, but we'll quickly check that your local enviroment is working (and resolving any lingering issues), and then talk a bit about some of the remote servers and tools we'll be using throughout the summer. You might not understand every detail of the commands we go through today, but don't worry if not -- we'll go into a lot more detail in the following sessions throughout the week. For today, we just want to be sure everyone has the tools they'll need for working on their projects. Getting help Work through the prerequisites below, making sure that all the software you installed works. Affix three kinds of post-it notes to your laptop: one with your operating system, e.g. Ubuntu 18.04 if you get an app working (e.g. ssh ), write its name on a green post-it and stick it to your screen if you tried - but the app failed - write its name on a red post-it If you're stuck with a step, ask a person with a corresponding green post-it (and preferrably your operating system) for help. The tech mentors will hover around and help with red stickers. You will need a few credentials for training accounts. We'll post them up front. Important The notes below aren't self-explanatory and will not cover all (or even the majority) of errors you might encounter. Make good use of the people in the room! Let's get started!","title":"Motivation"},{"location":"curriculum/setup/software-setup/setup_session_guide/#windows-users-setting-up-wsl","text":"We'll be using a lot of unix-based tools, but fortunately windows now provides a tool called \"windows subsystem for linux\" (WSL) that provides support for them. If you're using windows and didn't already set up WSL, let's do that now: WSL Instructions for Windows","title":"Windows Users: Setting up WSL"},{"location":"curriculum/setup/software-setup/setup_session_guide/#ssh","text":"The data you'll be using for the summer is generally of a sensitive nature and needs to be protected by remaining in a secure compute environment. As such, all of your work directly with the data will be on one of the remote servers we've set up. Let's see how connecting to those resources works:","title":"SSH"},{"location":"curriculum/setup/software-setup/setup_session_guide/#creating-an-ssh-keypair","text":"The setup information we sent out before the summer had some details on creating an SSH keypair, but if you need to revisit it, you can find that here: Instructions for Windows Instructions for MacOS/Linux","title":"Creating an SSH Keypair"},{"location":"curriculum/setup/software-setup/setup_session_guide/#reaching-the-compute-server","text":"Now let's make sure you can connect to the server. In your terminal, type: ssh {your_andrew_id}@training.dssg.io If you got an error about your ssh key, you might need to tell ssh where to find the private key, which you can do with the -i option: ssh -i {/path/to/your/private_key} {your_andrew_id}@training.dssg.io If you connected successfully, you should see a welcome message and see a prompt like: yourandrewid@dssg-primary: ~$ To confirm that you're connected, let's look at the output of the hostname command: Type hostname at the shell prompt and then hit return Did you get dssg-primary ? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out. PRO tip Your life will be easier if you set up a .ssh/config file","title":"Reaching the Compute Server"},{"location":"curriculum/setup/software-setup/setup_session_guide/#accessing-the-database","text":"","title":"Accessing the Database"},{"location":"curriculum/setup/software-setup/setup_session_guide/#reaching-the-database-from-the-compute-server","text":"We'll be using a database running PostgreSQL for much of our project data. One way to connect to the database is via the command line from the server using psql . Since we're already logged onto the server, let's give that a try: $ psql -h db.dssg.io -U {your_andrew_id} food_inspections If all goes well, you should see something like: psql (11.6 (Ubuntu 11.6-1.pgdg18.04+1), server 11.5) SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off) Type \"help\" for help. food_inspections=> Let's make sure you can run interact with the server: Type SELECT CURRENT_USER; and then hit return Did you get back your andrew id? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out. Finally, exit out of psql with \\q : food_inspections=> \\q Your postgres password You might have noticed that psql didn't ask you for a password when you connected to the server. This is because we've stored your password in a configuration file called .pgpass . You can view it at the bash prompt with: $ cat ~/.pgpass Your password is everything after the last colon: db.dssg.io:5432:*:{andrew_id}:{YOUR_PASSWORD}","title":"Reaching the Database from the Compute Server"},{"location":"curriculum/setup/software-setup/setup_session_guide/#reaching-the-database-from-your-computer","text":"Often it can be a little easier to access the database via GUI system from your computer. To do so, we recommend setting up dbeaver, since it offers a free version and works with every operating system (but if you already have a SQL client like dbvisualizer or data grip that you know how to use and prefer, feel free to use that instead). Protecting your data Although it's fine to run queries and look at the data from a SQL client like dbeaver running on your computer, you should never use that client to download an extract of the data (in whole or part) to your local computer, excel, google sheets, etc. To get DBeaver, you can install it directly from the DBeaver Website . NOTE: you'll want to install the free \"community edition\" version. Because the database is only accessible from our compute servers, we'll have to use an SSH Tunnel to connect to it via dbeaver. Here's how to set that up: First, create a new connection to a postgres database: Next, fill in the details for the database server on the window that pops up, following the example below: Note that your password here can be found in the .pgpass file that we looked at on the server. Finally, to actually reach the database server, we need to set up the SSH tunnel. Under the SSH tab, fill in these details: You might want to try clicking Test tunnel configuration to make sure that's working, then Test Configuration ... to ensure you can reach the database. Got an error like invalid privatekey: [B@7696c31f ? You may need to install a different SSH package called \"SSHJ\" -- under the Help menu, choose \"Install new Software\" then search for SSHJ and install the package (you'll need to restart dbeaver). After restarting, choose \u201cSSHJ\u201d in the drop-down under advanced (should be labeled either \u201dImplementation\u201d or \u201cMethod\u201d) when setting up the tunnel Let's make sure you can connect to the database: Try connecting to the database and opening a new sql script Type SELECT CURRENT_USER; and then press Ctrl-Enter to run it Did you get back your andrew id? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out.","title":"Reaching the Database from Your Computer"},{"location":"curriculum/setup/software-setup/setup_session_guide/#text-editor","text":"You'll want a good text editor to write code installed on your local machine. We recommend using VSCode because it's both free and allows you to directly edit code stored on a remote machine over SSH (this comes in very handy since all of your code will need to run on the server in order to keep the data in our secure environment). If you already have a different editor that you prefer (e.g., Sublime, PyCharm, emacs, etc.), you're welcome to use that (though note that you may need do some extra work to make sure you keep your local code in sync with the server). We sent out some instructions on installing VSCode before the summer, but if you need to revisit them, you can find them below (be sure to install the Remote-SSH extension as well): Instructions for Windows Instructions for MacOS/Linux PRO tip You might also want to install the microsoft python extension, which provides some additional features such as improved auto-completion when coding in python.","title":"Text Editor"},{"location":"curriculum/setup/software-setup/setup_session_guide/#editing-files-remotely-over-ssh","text":"In the same way we set up DBeaver to use SSH to talk to our remote infrastructure, we can set up VSCode to remotely edit files stored on the server using the Remote-SSH extension you should have installed in the step above. Let's set that up and make sure we can talk to the server: Configure our training server as an SSH host: With the SSH plugin installed, we can tell VSCode how to log into the server. In this step we'll be entering our connection string and saving it in a file, making it easy to connect in the future. Press ctrl+shift+p (Linux/Windows) or \u2318+shift+p (MacOS) to open the command pallette, and select Remote-SSH: Connect to Host Select Add New SSH Host... Enter ssh {andrewid}@training.dssg.io (remember from above that you may also need to use the -i parameter to tell ssh where to find your private key: ssh -i {path to your private key} {andrewid}@training.dssg.io ) Select the first option to store your login config: Connect VSCode to the course server: Connect to the CMU Full VPN Press ctrl+shift+p (Linux/Windows) or \u2318+shift+p (MacOS) to open the command pallette, and select Remote-SSH: Connect to Host Select the ssh config we just created: training.dssg.io Enter your private key passcode if VSCode prompts you to (it will open a box at the top of the screen). You should be connected to the training server. This should be indicated in the bottom of your VSCode window: Open a workspace folder: Now that VSCode is connected via SSH, you can browse all of the files and folders on the server. In this step, we select a folder containing some code to edit and test. Select the folder menu button Select Open Folder Select a folder to work in Let's make sure you can connect to the server through VSCode: Were you able to SSH and open the remote food_inspections folder? If so, you're all set! Put a green post-it on the back of your monitor! If not, put a red post-it on the back of your monitor and we'll help you out.","title":"Editing Files Remotely Over SSH"},{"location":"curriculum/setup/software-setup/setup_session_guide/#access-from-off-campus-cmu-vpn","text":"For an additional layer of security, our infrastructure can only be reached from from the CMU campus network. As such, if you're not on campus, you'll need to use the CMU VPN to reach the server: Download Cisco Anyconnect VPN client from here and install it Open the Anyconnect client Enter login credentials (be sure to select the \"Full VPN\" Group): Connect to: vpn.cmu.edu Group: \u201cFull VPN\u201d Username: your andrewid Password: your CMU password Click connect","title":"Access from Off-Campus: CMU VPN"},{"location":"curriculum/setup/software-setup/setup_session_guide/#understanding-the-big-picture","text":"Finally, before we go, let's take a minute to zoom out to the \"big picture\" of the infrastructure we'll be using and see how all these pieces fit together:","title":"Understanding The \"Big Picture\""},{"location":"curriculum/setup/software-setup/setup_session_guide/#other-optional-local-tools","text":"Most of your work this summer will be on the remote server for your project, but there are a few of other tools that might be handy to have installed locally if you feel inclined: Python (using pyenv) and jupyterlab [ windows ] [ macos/linux ] Github client [ windows ] [ macos/linux ] psql CLI [ windows ] [ macos/linux ] Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license.","title":"Other (Optional) Local Tools"},{"location":"curriculum/setup/software-setup/setup_windows/","text":"Sertting up your Windows Machine # This guide helps you walk through how to set up the various technical tools you'll need for the summer and is focused on Windows users (if you're on MacOS or Linux, see the related guide here ) Windows Subsystem for Linux (WSL) # Newer versions of windows have the option of running a linux environment directly on windows, and we recommend using that as your development environment. You can learn more about WSL here . First we have to install WSL on Windows. We'll give you the quick installation guide, if you want to customize things, please refer the detailed installation guide . First, open a PowerShell or a Command Prompt Window as an Administrator. Next, we can see the available Linux distributions for install by using: $ wsl --list --online Then, you can install the version of Linux you would like to install. We recommend picking one of the Ubuntu distributions and this guide assumes an Ubuntu installation for WSL. We can install Ubuntu 20.04 by: $ wsl --install -d Ubuntu-20.04 This will take a few minutes, and will prompt you to provide a UNIX username and a password. Please note that you might have to restart your computer at some point during the installation for things to take full effect. Now, you can use Linux from within your Windows machine. You should have a shortcut in your start menu to launch WSL, and when you launch it should open up a CLI. Note that this will have no GUI and you'll have to rely on the CLI. If you need to access the file system of WSL through the Windows File Explorer, you can type the following in the address bar of the File Explorer. \\\\wsl.localhost\\Ubuntu-20.04 This will take you to the root folder of the linux file system. Note - Appending \\home\\<username> to the above address will take you to your home directory. For the next few pieces of software, we'll provide you instructions on how to run things on both WSL and on 'pure' Windows. Git and GitHub Account # If you don't have a GitHub account, make one ! Install Git on your machine Git comes preinstalled with WSL. Therefore, nothing to do here. When you type in the command git to the terminal, it should show you a long output that starts with the following $ git usage: git [--version] [--help] [-C <path>] [-c <name>=<value>] [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path] [-p | --paginate | --no-pager] [--no-replace-objects] [--bare] [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>] <command> [<args>] If you don't see this output by any chance, you can install git by using the following commands: $ sudo apt-get update $ sudo apt-get install git Test your installation. For example, create a directory, and make it a git repo: $ mkdir mytestdir $ cd mytestdir $ git init Then, you should see this output: > Initialized empty Git repository in [...]/mytestdir/.git/ Note: In windows, you can download and install Git here Learning more about git # If you haven't used git/github before, here are a couple of useful resources where you can learn a bit more. Setting up Python and Related Tools # We'll primarily use the python programming language for scripting, doing analyses, and building models throughout the summer, so let's make sure we have the right version and packages installed. pyenv vs anaconda This is a contentious topic! Some people argue that they find Anaconda ( conda or mini-conda ) easier to get up and running while others argue for the consistency and flexibility of pyenv . In general, python's library system is a bit of a mess and in constant evolution. We favor pyenv here, since we think it provides you with more flexibility and teaches you about how python works. Your WSL system should come with python preinstalled. With some distros it does not, you can install system level python. However, this is not necessary as we won't be using the system level Python anyway. So, feel free to skip to version management. $ sudo apt-get install python Allow it to restart services when prompted. Once installed, you should be able to try it out. $ python Python 2.7.17 (default, Mar 18 2022, 13:21:42) [GCC 7.5.0] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> version managemet and virtual environments # Now you have system level Python installed. But, we don\u2019t want to mess with it. So we will install a different python, for your exclusive use. First, we will install some libraries $ sudo apt-get update $ sudo apt-get install --no-install-recommends make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev Some of these might be already installed, but good to make sure. It's good to be able to manage different versions of Python on your dev environment (different projects you work on might require different versions of Python). We recommend using pyenv as the python version manager. $ curl https://pyenv.run | bash Once this finishes, you will see some instructions at the end for adding pyenv to the load path. To do this, we update a dot file on your home drive named .bashrc 1 . You can open and edit your bashrc file on the terminal using either vi or nano . If you are not familiar with either, we recommend using nano . $ nano ~/.bashrc navigate to the end of the file, and add the following snippet to the file. This will add to pyenv to the load path. export PYENV_ROOT=\"$HOME/.pyenv\" command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\" Don't close the file yet! We need to install pyenv-virtualenv to enable us to use virtual enviroments. This is the manager that helps us maintain different dev environments with different python versions and different python package versions. To enable pyenv-virtualenv , add the following line to the .bashrc below the above snippet. eval \"$(pyenv virtualenv-init -)\" Now we can save and close the file. In order for the changes to take effect, either you have restart the terminal, or type: $ source ~/.bashrc Now we should have pyenv and pyenv-virtualenv installed. Let's make sure: $ pyenv --version pyenv 2.3.0 $ pyenv virtualenv pyenv-virtualenv: no virtualenv name given. If you see those outputs (pyenv version might be different depending on when you run this), you have installed both. Let's create a virtualenv with a specific python version and install the base packages we would need. Let's create an environment named dssg-3.8.10 (this could be any name you like) with Python 3.8.10 . First, install the Python version we need on pyenv . Note: You can check all available Python versions on pyenv by using $ pyenv install --list $ pyenv install 3.8.10 This will take several minutes. Once complete, create the environment $ pyenv virtualenv 3.8.10 dssg-3.8.10 Now you have created the virtual environment. To use it with a specific project, you can navigate to the project folder and assign it to the directory: $ echo dssg-3.8.10 > .python-version This will ensure that whenever you are inside that directory, the dssg-3.8.10 environment will be activated. If not, you can manually activate the environment: $ activate dssg-3.8.10 Once you have activated the environment you can start installing Python packages. Package Installations # Packages are installed using pip. To install a single package: $ pip install pandas To install many packages at once, list all the packages needed in a file (usually called requirements.txt), navigate to the folder of the file and execute $ pip install -r requirements.txt to try it out use this file: requirements.txt Jupyter Jupyter notebooks are a convenient environment for experimentation, prototyping, and sharing exploratory work. We install Jupyter in the above requirements.txt file. Jupyter notebooks require a kernel that executes the code. It should link to the virtual environment: $ pyenv activate dssg-3.8.10 $ python -m ipykernel install --user --name=dssg-3.8.10 --display-name \"dssg-3.8.10\" Note that you should have the virtual environment activated when you issue this command. It's time to test! In order to test that both jupyter and the python packages installed appropriately, you should do the following: Download the file SoftwareSetup.ipynb into your directory. Type in the terminal $ jupyter lab Your browser should open a new tab with the jupyter lab interface. Click on SoftwareSetup.ipynb to open the notebook Follow the instructions in the notebook to run each cell. Learning more about python # Python is a powerful, expressive, and easy to read (even by non-programmers) programming language. If you're still relatively new to it, you might find some of the resources here helpful. SSH Keys # SSH helps you access the remote servers using your laptop. For this to work, we generate a key-pair that consists of a Public Key (something that you would share with the server), and a private key (something that you would NEVER share with anyone!). Option A - WSL Inside WSL, we can use the same process as a UNIX system to generate keys. $ ssh-keygen This will prompt you to select a location for storing the key, and give you the option to add a passphrase to the key. If you want to use the default locaion (Recommended!) and not use a passphrase, you just have to hit return. Then, your keys will be stored in the place your specified. By default, - there'll be a .ssh folder in your home directory ~/.ssh/ - private key would be named id_rsa - public key would be named id_rsa.pub You've successfully generated the Keys! After having generated the key pair, you should set the correct file permissions for your private key: SSH requires that only you, the owner, are able to read/write it, and will give you an error otherwise. You can set the right permissions with this command: $ chmod 600 ~/.ssh/nameofyourprivatekey (where you'll have to substitute in the path and name of your private key that you chose during key generation). Option B - Windows Luckily, Windows 10/11 have OpenSSH already installed, and we don't need to use Putty anymore \ud83e\udd73. Just to make sure that it's installed, open up a Powershell window and enter ssh . When you hit return you should see an output like this. usage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-B bind_interface] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] destination [command] If you do not see this output, you can use this guide to install OpenSSH . Once you have OpenSSH, you can use the same command as WSL to generate the Keys on a Powershell Window. As with WSL, you would be prompted to select the location to store the keys, and then the option to add a passphrase. You can just hit return to use the default location (recommended!) and not have a passphrase. By default, the keys will be stored in C:\\Users\\<windows_username>/.ssh/ and the file names would be as same as the WSL one. VSCode # Visual Studio Code is a free text editor that enables you to code directly on a remote server. You can download VSCode for windows here . If you development environment is on WSL, you can install the Remote-WSL extension for VSCode and navigate to your project folder on the WSL terminal and type: $ code . This will launch a VScode window that will let you develop on your WSL machine. As we said above, one of the most useful features of VSCode is that it let's you edit code directly on a remote server using SSH. To use this feature, you should install the Remote-SSH extention for VSCode . We need to tell VSCode where your private key is to authenticate the SSH connection. VSCode would automatically check for the default private key named id_rsa at the default Windows location C:\\Users\\<windows_username>/.ssh/ . As we created the SSH keys in WSL, they keys would be inside the WSL file system. We can copy the keys to the windows location. $ mkdir /mnt/c/<windows username>/.ssh $ cp -r ~/.ssh/ /mnt/c/<windows username>/.ssh/ Note that this will overwrite an existing ssh key in the Windows folder. Database # DBeaver We woud install DBeaver on directly on Windows. You can download the installer here . It's important to not that when you create a DB connection, You might need to provide the path to your SSH key to get access to a remote DB server. As we did with VSCode, you have the option of either keeping a copy of your private key in the Windows home directory ( C:/Users/<windows username>/.ssh ), or any other directory, and pointing DBeaver to that copy, or using the path to the key stored inside your WSL machine. PSQL On WSL terminal, you can enter the following commands to install psql $ sudo apt-get update $ sudo apt-get install postgresql-client Try typing psql into the terminal, you should see the following output. $ psql psql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"? Congratulations -- You Made It! # Good news -- that's it in terms of software setup (for now)! Take some time to familiarize yourself with them before the summer, and check out the resources here for some helpful guides. On that page, you'll also find some good background information on machine learning concepts and causal inference which may be helpful as well. More info about the .bashrc file \u21a9","title":"Sertting up your Windows Machine"},{"location":"curriculum/setup/software-setup/setup_windows/#sertting-up-your-windows-machine","text":"This guide helps you walk through how to set up the various technical tools you'll need for the summer and is focused on Windows users (if you're on MacOS or Linux, see the related guide here )","title":"Sertting up your Windows Machine"},{"location":"curriculum/setup/software-setup/setup_windows/#windows-subsystem-for-linux-wsl","text":"Newer versions of windows have the option of running a linux environment directly on windows, and we recommend using that as your development environment. You can learn more about WSL here . First we have to install WSL on Windows. We'll give you the quick installation guide, if you want to customize things, please refer the detailed installation guide . First, open a PowerShell or a Command Prompt Window as an Administrator. Next, we can see the available Linux distributions for install by using: $ wsl --list --online Then, you can install the version of Linux you would like to install. We recommend picking one of the Ubuntu distributions and this guide assumes an Ubuntu installation for WSL. We can install Ubuntu 20.04 by: $ wsl --install -d Ubuntu-20.04 This will take a few minutes, and will prompt you to provide a UNIX username and a password. Please note that you might have to restart your computer at some point during the installation for things to take full effect. Now, you can use Linux from within your Windows machine. You should have a shortcut in your start menu to launch WSL, and when you launch it should open up a CLI. Note that this will have no GUI and you'll have to rely on the CLI. If you need to access the file system of WSL through the Windows File Explorer, you can type the following in the address bar of the File Explorer. \\\\wsl.localhost\\Ubuntu-20.04 This will take you to the root folder of the linux file system. Note - Appending \\home\\<username> to the above address will take you to your home directory. For the next few pieces of software, we'll provide you instructions on how to run things on both WSL and on 'pure' Windows.","title":"Windows Subsystem for Linux (WSL)"},{"location":"curriculum/setup/software-setup/setup_windows/#git-and-github-account","text":"If you don't have a GitHub account, make one ! Install Git on your machine Git comes preinstalled with WSL. Therefore, nothing to do here. When you type in the command git to the terminal, it should show you a long output that starts with the following $ git usage: git [--version] [--help] [-C <path>] [-c <name>=<value>] [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path] [-p | --paginate | --no-pager] [--no-replace-objects] [--bare] [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>] <command> [<args>] If you don't see this output by any chance, you can install git by using the following commands: $ sudo apt-get update $ sudo apt-get install git Test your installation. For example, create a directory, and make it a git repo: $ mkdir mytestdir $ cd mytestdir $ git init Then, you should see this output: > Initialized empty Git repository in [...]/mytestdir/.git/ Note: In windows, you can download and install Git here","title":"Git and GitHub Account"},{"location":"curriculum/setup/software-setup/setup_windows/#learning-more-about-git","text":"If you haven't used git/github before, here are a couple of useful resources where you can learn a bit more.","title":"Learning more about git"},{"location":"curriculum/setup/software-setup/setup_windows/#setting-up-python-and-related-tools","text":"We'll primarily use the python programming language for scripting, doing analyses, and building models throughout the summer, so let's make sure we have the right version and packages installed. pyenv vs anaconda This is a contentious topic! Some people argue that they find Anaconda ( conda or mini-conda ) easier to get up and running while others argue for the consistency and flexibility of pyenv . In general, python's library system is a bit of a mess and in constant evolution. We favor pyenv here, since we think it provides you with more flexibility and teaches you about how python works. Your WSL system should come with python preinstalled. With some distros it does not, you can install system level python. However, this is not necessary as we won't be using the system level Python anyway. So, feel free to skip to version management. $ sudo apt-get install python Allow it to restart services when prompted. Once installed, you should be able to try it out. $ python Python 2.7.17 (default, Mar 18 2022, 13:21:42) [GCC 7.5.0] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>>","title":"Setting up Python and Related Tools"},{"location":"curriculum/setup/software-setup/setup_windows/#version-managemet-and-virtual-environments","text":"Now you have system level Python installed. But, we don\u2019t want to mess with it. So we will install a different python, for your exclusive use. First, we will install some libraries $ sudo apt-get update $ sudo apt-get install --no-install-recommends make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev Some of these might be already installed, but good to make sure. It's good to be able to manage different versions of Python on your dev environment (different projects you work on might require different versions of Python). We recommend using pyenv as the python version manager. $ curl https://pyenv.run | bash Once this finishes, you will see some instructions at the end for adding pyenv to the load path. To do this, we update a dot file on your home drive named .bashrc 1 . You can open and edit your bashrc file on the terminal using either vi or nano . If you are not familiar with either, we recommend using nano . $ nano ~/.bashrc navigate to the end of the file, and add the following snippet to the file. This will add to pyenv to the load path. export PYENV_ROOT=\"$HOME/.pyenv\" command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\" Don't close the file yet! We need to install pyenv-virtualenv to enable us to use virtual enviroments. This is the manager that helps us maintain different dev environments with different python versions and different python package versions. To enable pyenv-virtualenv , add the following line to the .bashrc below the above snippet. eval \"$(pyenv virtualenv-init -)\" Now we can save and close the file. In order for the changes to take effect, either you have restart the terminal, or type: $ source ~/.bashrc Now we should have pyenv and pyenv-virtualenv installed. Let's make sure: $ pyenv --version pyenv 2.3.0 $ pyenv virtualenv pyenv-virtualenv: no virtualenv name given. If you see those outputs (pyenv version might be different depending on when you run this), you have installed both. Let's create a virtualenv with a specific python version and install the base packages we would need. Let's create an environment named dssg-3.8.10 (this could be any name you like) with Python 3.8.10 . First, install the Python version we need on pyenv . Note: You can check all available Python versions on pyenv by using $ pyenv install --list $ pyenv install 3.8.10 This will take several minutes. Once complete, create the environment $ pyenv virtualenv 3.8.10 dssg-3.8.10 Now you have created the virtual environment. To use it with a specific project, you can navigate to the project folder and assign it to the directory: $ echo dssg-3.8.10 > .python-version This will ensure that whenever you are inside that directory, the dssg-3.8.10 environment will be activated. If not, you can manually activate the environment: $ activate dssg-3.8.10 Once you have activated the environment you can start installing Python packages.","title":"version managemet and virtual environments"},{"location":"curriculum/setup/software-setup/setup_windows/#package-installations","text":"Packages are installed using pip. To install a single package: $ pip install pandas To install many packages at once, list all the packages needed in a file (usually called requirements.txt), navigate to the folder of the file and execute $ pip install -r requirements.txt to try it out use this file: requirements.txt","title":"Package Installations"},{"location":"curriculum/setup/software-setup/setup_windows/#learning-more-about-python","text":"Python is a powerful, expressive, and easy to read (even by non-programmers) programming language. If you're still relatively new to it, you might find some of the resources here helpful.","title":"Learning more about python"},{"location":"curriculum/setup/software-setup/setup_windows/#ssh-keys","text":"SSH helps you access the remote servers using your laptop. For this to work, we generate a key-pair that consists of a Public Key (something that you would share with the server), and a private key (something that you would NEVER share with anyone!). Option A - WSL Inside WSL, we can use the same process as a UNIX system to generate keys. $ ssh-keygen This will prompt you to select a location for storing the key, and give you the option to add a passphrase to the key. If you want to use the default locaion (Recommended!) and not use a passphrase, you just have to hit return. Then, your keys will be stored in the place your specified. By default, - there'll be a .ssh folder in your home directory ~/.ssh/ - private key would be named id_rsa - public key would be named id_rsa.pub You've successfully generated the Keys! After having generated the key pair, you should set the correct file permissions for your private key: SSH requires that only you, the owner, are able to read/write it, and will give you an error otherwise. You can set the right permissions with this command: $ chmod 600 ~/.ssh/nameofyourprivatekey (where you'll have to substitute in the path and name of your private key that you chose during key generation). Option B - Windows Luckily, Windows 10/11 have OpenSSH already installed, and we don't need to use Putty anymore \ud83e\udd73. Just to make sure that it's installed, open up a Powershell window and enter ssh . When you hit return you should see an output like this. usage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-B bind_interface] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-E log_file] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] destination [command] If you do not see this output, you can use this guide to install OpenSSH . Once you have OpenSSH, you can use the same command as WSL to generate the Keys on a Powershell Window. As with WSL, you would be prompted to select the location to store the keys, and then the option to add a passphrase. You can just hit return to use the default location (recommended!) and not have a passphrase. By default, the keys will be stored in C:\\Users\\<windows_username>/.ssh/ and the file names would be as same as the WSL one.","title":"SSH Keys"},{"location":"curriculum/setup/software-setup/setup_windows/#vscode","text":"Visual Studio Code is a free text editor that enables you to code directly on a remote server. You can download VSCode for windows here . If you development environment is on WSL, you can install the Remote-WSL extension for VSCode and navigate to your project folder on the WSL terminal and type: $ code . This will launch a VScode window that will let you develop on your WSL machine. As we said above, one of the most useful features of VSCode is that it let's you edit code directly on a remote server using SSH. To use this feature, you should install the Remote-SSH extention for VSCode . We need to tell VSCode where your private key is to authenticate the SSH connection. VSCode would automatically check for the default private key named id_rsa at the default Windows location C:\\Users\\<windows_username>/.ssh/ . As we created the SSH keys in WSL, they keys would be inside the WSL file system. We can copy the keys to the windows location. $ mkdir /mnt/c/<windows username>/.ssh $ cp -r ~/.ssh/ /mnt/c/<windows username>/.ssh/ Note that this will overwrite an existing ssh key in the Windows folder.","title":"VSCode"},{"location":"curriculum/setup/software-setup/setup_windows/#database","text":"DBeaver We woud install DBeaver on directly on Windows. You can download the installer here . It's important to not that when you create a DB connection, You might need to provide the path to your SSH key to get access to a remote DB server. As we did with VSCode, you have the option of either keeping a copy of your private key in the Windows home directory ( C:/Users/<windows username>/.ssh ), or any other directory, and pointing DBeaver to that copy, or using the path to the key stored inside your WSL machine. PSQL On WSL terminal, you can enter the following commands to install psql $ sudo apt-get update $ sudo apt-get install postgresql-client Try typing psql into the terminal, you should see the following output. $ psql psql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \"/var/run/postgresql/.s.PGSQL.5432\"?","title":"Database"},{"location":"curriculum/setup/software-setup/setup_windows/#congratulations-you-made-it","text":"Good news -- that's it in terms of software setup (for now)! Take some time to familiarize yourself with them before the summer, and check out the resources here for some helpful guides. On that page, you'll also find some good background information on machine learning concepts and causal inference which may be helpful as well. More info about the .bashrc file \u21a9","title":"Congratulations -- You Made It!"},{"location":"curriculum/skills/domain_understanding/","text":"","title":"Domain Understanding"},{"location":"curriculum/skills/ethics_bias_fairness/","text":"Resources # Ethics in Machine Learning for Public Policy - slides from class lecture Bias and model selection - slides from DSAPP Deep Dive by Pedro Saleiro Aequitas: A Bias and Fairness Audit Toolkit Demo on using Aequitas Bias and Fairness Overview slides - Pedro Saleiro & Rayid Ghani","title":"Ethics, Bias, Fairness"},{"location":"curriculum/skills/ethics_bias_fairness/#resources","text":"Ethics in Machine Learning for Public Policy - slides from class lecture Bias and model selection - slides from DSAPP Deep Dive by Pedro Saleiro Aequitas: A Bias and Fairness Audit Toolkit Demo on using Aequitas Bias and Fairness Overview slides - Pedro Saleiro & Rayid Ghani","title":"Resources"},{"location":"curriculum/software/","text":"","title":"Home"},{"location":"curriculum/software/basic_python/","text":"Basic Python # Python is the language of choice here at DSSG. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). Python Package & Environment Management # To properly create a python environment, we recommend you use Pyenv or Anaconda / Miniconda . Some e-books # Think Python An amazing book from Allen B. Downey. Also check all of his book series Think \u2026 ! Lectures in Quantitative Economics A Econometrics web course with python code A tutorial for Economists A short (34 pages!) intro to python Python for Informatics - Exploring Information A more traditional (CS) approach More Python resources # Writing efficient Python Tips for Idiomatic Python Introduction to Python debugging tools Jupyter Notebooks and Jupyter Lab Beginner's Guide to Jupyter Lab Example of how IPython notebooks are used in data science Tutorial for setting up and opening IPython notebook Amazing examples of IPython notebooks","title":"Python"},{"location":"curriculum/software/basic_python/#basic-python","text":"Python is the language of choice here at DSSG. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers).","title":"Basic Python"},{"location":"curriculum/software/basic_python/#python-package-environment-management","text":"To properly create a python environment, we recommend you use Pyenv or Anaconda / Miniconda .","title":"Python Package &amp; Environment Management"},{"location":"curriculum/software/basic_python/#some-e-books","text":"Think Python An amazing book from Allen B. Downey. Also check all of his book series Think \u2026 ! Lectures in Quantitative Economics A Econometrics web course with python code A tutorial for Economists A short (34 pages!) intro to python Python for Informatics - Exploring Information A more traditional (CS) approach","title":"Some e-books"},{"location":"curriculum/software/basic_python/#more-python-resources","text":"Writing efficient Python Tips for Idiomatic Python Introduction to Python debugging tools Jupyter Notebooks and Jupyter Lab Beginner's Guide to Jupyter Lab Example of how IPython notebooks are used in data science Tutorial for setting up and opening IPython notebook Amazing examples of IPython notebooks","title":"More Python resources"},{"location":"curriculum/software/basic_sql/","text":"","title":"Basic sql"},{"location":"curriculum/software/good_practices/","text":"","title":"Good practices"},{"location":"curriculum/software/python_pandas/","text":"","title":"Python pandas"},{"location":"curriculum/software/sql_data_analysis/","text":"Basic SQL Intermediate SQL Advanced SQL","title":"Analyzing data (SQL)"},{"location":"curriculum/software/basic_sql/","text":"Intro to SQL # You've already used databases. Excel spreadsheets are a simple example. But those databases have many problems, such as * size of data you can use is limited by RAM * cannot handle complex data (there are databases to handle more complex data types, e.g. documents) * difficult to use data from multiple tables/sheets * no data integrity guarantees (you can accidentally put a letter in a numeric column and the entire column will become a character column) * it's difficult for multiple people to use the spreadsheet at the same time. If one person updates sheet A and another person updates sheet B, integrating both updates gets ugly. Things to cover: select from limit (Postgres) where group by order by So, let's get into it, shall we!? First, we'll need to either: 1. use psql * ssh into the server * Run psql 2. use dbeaver Getting a sense of the tables and data: # A few things we can do to explore: Using PSQL: * Look at the schemas currently present; make sure yours is there: type \\dn * List databases: \\l * ( This doc has a list of some other quick exploratory commands.) * Find the count of the list of rows: SELECT COUNT(*) FROM tablename; Output the list of columns for this table: SELECT column_name FROM information_schema.columns WHERE table_name = 'mytablename'; If you also want to look at data types: SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'mytablename'; Ok, now that we've gotten a sense of the data, let's dial it back and get to the basics. :) SELECT and FROM # Now, let's look a bit more into SELECT. The SELECT statement is used to select data from a database. The data returned is stored in a result table, called the result-set. In SQL, data is usually organized in various tables. For example, a sports team database might have the tables teams, players, and games. A wedding database might have tables guests, vendors, and music_playlist. First off, let's select everything from the table to see what we get: SELECT * FROM mytablename; There's a lot to view at once here. Let's say we're not interested in all those comments and just want to look at the columns inspection_id , dba_name , aka_name , results , and inspection_date . We can edit the above command to only select those columns: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections; LIMIT # Often, tables contain millions of rows, and it can take a while to grab everything. If we just want to see a few examples of the data in a table, we can select the first few rows with the LIMIT keyword. (This might remind you of using .head in pandas.) SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections LIMIT 20; ORDER BY # The ORDER BY keyword is used to sort the result-set in ascending or descending order. The ORDER BY keyword sorts the records in ascending order by default. To sort the records in descending order, use the DESC keyword. Here's how you might order by dba_name in ascending order: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections ORDER BY dba_name; Now, try altering the above line so that it orders the list in descending order. How might we do that? Can look here for help. ORDER BY / LIMIT COMBO If you use ORDER BY and then LIMIT, you would get the first rows for that order. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections ORDER BY dba_name LIMIT 10; WHERE # The WHERE clause is used to filter records. That is, the WHERE class extracts only those records that fulfill a specified condition. Let's say we only want to look at records where the restaurant name is SUN WAH, so that we can check to see if it's a good time to go to Joe's favorite duck place. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections WHERE dba_name='SUN WAH'; We'll notice that this does not get us any results... Hmmmm. Let's try using the LIKE operator !! SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections WHERE dba_name LIKE '%SUN WAH%'; GROUPBY # The GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. For example, if we want to find the amount of times that each restaurant has been inspected over this time frame, we might run: SELECT dba_name, COUNT(*) FROM inspections GROUP BY dba_name; Let's say you are only concerned with the amount of times that the restaurant failed in this timeframe... SELECT dba_name, COUNT(*) FROM inspections WHERE results LIKE 'Fail%' GROUP BY dba_name; JOIN # A JOIN clause is used to combine rows from two or more tables, based on a related column between them. Let's first look here to look at some ven diagrams of the various types of joins. We have a table with zip code boundaries. To demonstrate how joins work, let's join the boundaries table to the inspections table! So, in order to select the dba_name from the inspections table ( mpettit_schema.mpettit_table ) and the objectid from the gis.boundaries table, we would do something like below. Let's discuss what's happening! SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM inspections LEFT JOIN gis.boundaries ON gis.boundaries.zip=inspections.zip:; So, something went wrong. Any idea what it was? Let's investigate... SELECT column_name, data_type FROM information_schema.columns WHERE table_name = inspections'; & SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'boundaries'; We see the problem is that the data types for zip code don't match up between the two tables. Let's fix that: SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM inspections LEFT JOIN gis.boundaries ON gis.boundaries.zip=mpettit_schema.mpettit_table.zip::varchar; Also, you might notice that this is extremely wordy... We can write a shorter query if we used aliases for those tables. Basically, we create a \"nickname\" for that table. If you want to use an alias for a table, you add AS *alias_name* after the table name. SELECT inspect.dba_name, bound.objectid FROM inspections AS inspect LEFT JOIN gis.boundaries AS bound ON bound.zip=inspect.zip::varchar; SQL order of execution: # The clauses of an SQL query are evaluated in a specific order. Here is a blog post that goes into a bit more detail. Joins: SQL's FROM clause selects and joins your tables and is the first executed part of a query. This means that in queries with joins, the join is the first thing to happen.","title":"SQL"},{"location":"curriculum/software/basic_sql/#intro-to-sql","text":"You've already used databases. Excel spreadsheets are a simple example. But those databases have many problems, such as * size of data you can use is limited by RAM * cannot handle complex data (there are databases to handle more complex data types, e.g. documents) * difficult to use data from multiple tables/sheets * no data integrity guarantees (you can accidentally put a letter in a numeric column and the entire column will become a character column) * it's difficult for multiple people to use the spreadsheet at the same time. If one person updates sheet A and another person updates sheet B, integrating both updates gets ugly. Things to cover: select from limit (Postgres) where group by order by So, let's get into it, shall we!? First, we'll need to either: 1. use psql * ssh into the server * Run psql 2. use dbeaver","title":"Intro to SQL"},{"location":"curriculum/software/basic_sql/#getting-a-sense-of-the-tables-and-data","text":"A few things we can do to explore: Using PSQL: * Look at the schemas currently present; make sure yours is there: type \\dn * List databases: \\l * ( This doc has a list of some other quick exploratory commands.) * Find the count of the list of rows: SELECT COUNT(*) FROM tablename; Output the list of columns for this table: SELECT column_name FROM information_schema.columns WHERE table_name = 'mytablename'; If you also want to look at data types: SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'mytablename'; Ok, now that we've gotten a sense of the data, let's dial it back and get to the basics. :)","title":"Getting a sense of the tables and data:"},{"location":"curriculum/software/basic_sql/#select-and-from","text":"Now, let's look a bit more into SELECT. The SELECT statement is used to select data from a database. The data returned is stored in a result table, called the result-set. In SQL, data is usually organized in various tables. For example, a sports team database might have the tables teams, players, and games. A wedding database might have tables guests, vendors, and music_playlist. First off, let's select everything from the table to see what we get: SELECT * FROM mytablename; There's a lot to view at once here. Let's say we're not interested in all those comments and just want to look at the columns inspection_id , dba_name , aka_name , results , and inspection_date . We can edit the above command to only select those columns: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections;","title":"SELECT and FROM"},{"location":"curriculum/software/basic_sql/#limit","text":"Often, tables contain millions of rows, and it can take a while to grab everything. If we just want to see a few examples of the data in a table, we can select the first few rows with the LIMIT keyword. (This might remind you of using .head in pandas.) SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections LIMIT 20;","title":"LIMIT"},{"location":"curriculum/software/basic_sql/#order-by","text":"The ORDER BY keyword is used to sort the result-set in ascending or descending order. The ORDER BY keyword sorts the records in ascending order by default. To sort the records in descending order, use the DESC keyword. Here's how you might order by dba_name in ascending order: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections ORDER BY dba_name; Now, try altering the above line so that it orders the list in descending order. How might we do that? Can look here for help. ORDER BY / LIMIT COMBO If you use ORDER BY and then LIMIT, you would get the first rows for that order. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections ORDER BY dba_name LIMIT 10;","title":"ORDER BY"},{"location":"curriculum/software/basic_sql/#where","text":"The WHERE clause is used to filter records. That is, the WHERE class extracts only those records that fulfill a specified condition. Let's say we only want to look at records where the restaurant name is SUN WAH, so that we can check to see if it's a good time to go to Joe's favorite duck place. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections WHERE dba_name='SUN WAH'; We'll notice that this does not get us any results... Hmmmm. Let's try using the LIKE operator !! SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM inspections WHERE dba_name LIKE '%SUN WAH%';","title":"WHERE"},{"location":"curriculum/software/basic_sql/#groupby","text":"The GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. For example, if we want to find the amount of times that each restaurant has been inspected over this time frame, we might run: SELECT dba_name, COUNT(*) FROM inspections GROUP BY dba_name; Let's say you are only concerned with the amount of times that the restaurant failed in this timeframe... SELECT dba_name, COUNT(*) FROM inspections WHERE results LIKE 'Fail%' GROUP BY dba_name;","title":"GROUPBY"},{"location":"curriculum/software/basic_sql/#join","text":"A JOIN clause is used to combine rows from two or more tables, based on a related column between them. Let's first look here to look at some ven diagrams of the various types of joins. We have a table with zip code boundaries. To demonstrate how joins work, let's join the boundaries table to the inspections table! So, in order to select the dba_name from the inspections table ( mpettit_schema.mpettit_table ) and the objectid from the gis.boundaries table, we would do something like below. Let's discuss what's happening! SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM inspections LEFT JOIN gis.boundaries ON gis.boundaries.zip=inspections.zip:; So, something went wrong. Any idea what it was? Let's investigate... SELECT column_name, data_type FROM information_schema.columns WHERE table_name = inspections'; & SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'boundaries'; We see the problem is that the data types for zip code don't match up between the two tables. Let's fix that: SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM inspections LEFT JOIN gis.boundaries ON gis.boundaries.zip=mpettit_schema.mpettit_table.zip::varchar; Also, you might notice that this is extremely wordy... We can write a shorter query if we used aliases for those tables. Basically, we create a \"nickname\" for that table. If you want to use an alias for a table, you add AS *alias_name* after the table name. SELECT inspect.dba_name, bound.objectid FROM inspections AS inspect LEFT JOIN gis.boundaries AS bound ON bound.zip=inspect.zip::varchar;","title":"JOIN"},{"location":"curriculum/software/basic_sql/#sql-order-of-execution","text":"The clauses of an SQL query are evaluated in a specific order. Here is a blog post that goes into a bit more detail. Joins: SQL's FROM clause selects and joins your tables and is the first executed part of a query. This means that in queries with joins, the join is the first thing to happen.","title":"SQL order of execution:"},{"location":"curriculum/store_data/etl/","text":"Reproducible ETL","title":"Etl"},{"location":"curriculum/tutorial-template/","text":"Tutorial Template Session: Making Meringues # Motivation # A very brief abstract. This tutorial teaches the basics of making meringues. These simple preparations from sugar and eggwhites are an integral part of many french-inspired desserts and surprisingly easy to make. Potential Teachouts # Always include a list of relevant, more advanced topics, and encourage fellows to teach them in the coming weeks. Jane is running the schedule for fellow teachouts. This tutorial only covers the very basics of making meringues. If you are an experienced baker, please share your knowledge and schedule a fellow teachout! Here are some topics that would be helpful: - flavored meringues - marbled meringues - recipes with meringues Content # This section can either contain the content itself, or link to a slide presentation. When linking to a Google Presentation, make sure to allow comments there, and put a PDF copy of your presentation into the tutorial folder itself (and make sure you're not excluding PDFs via your .gitignore ). Google Slides Presentation If you have worksheets for fellows to go through during the session, place them in a worksheets folder, and also include solutions for future reference! Cheat Sheet # This section is only necessary for tech sessions, but important! It should list all relevant commands with very brief explanations for quick future reference. - 1 cup granulated sugar - 1 cup fine sugar - 8 egg whites - ... Further Resources # Links to further tutorials, videos, blog posts... - Basic Meringue Recipe Discussion Notes # Optional. If interesting points come up during the discussion, add them here.","title":"Tutorial Template Session: Making Meringues"},{"location":"curriculum/tutorial-template/#tutorial-template-session-making-meringues","text":"","title":"Tutorial Template Session: Making Meringues"},{"location":"curriculum/tutorial-template/#motivation","text":"A very brief abstract. This tutorial teaches the basics of making meringues. These simple preparations from sugar and eggwhites are an integral part of many french-inspired desserts and surprisingly easy to make.","title":"Motivation"},{"location":"curriculum/tutorial-template/#potential-teachouts","text":"Always include a list of relevant, more advanced topics, and encourage fellows to teach them in the coming weeks. Jane is running the schedule for fellow teachouts. This tutorial only covers the very basics of making meringues. If you are an experienced baker, please share your knowledge and schedule a fellow teachout! Here are some topics that would be helpful: - flavored meringues - marbled meringues - recipes with meringues","title":"Potential Teachouts"},{"location":"curriculum/tutorial-template/#content","text":"This section can either contain the content itself, or link to a slide presentation. When linking to a Google Presentation, make sure to allow comments there, and put a PDF copy of your presentation into the tutorial folder itself (and make sure you're not excluding PDFs via your .gitignore ). Google Slides Presentation If you have worksheets for fellows to go through during the session, place them in a worksheets folder, and also include solutions for future reference!","title":"Content"},{"location":"curriculum/tutorial-template/#cheat-sheet","text":"This section is only necessary for tech sessions, but important! It should list all relevant commands with very brief explanations for quick future reference. - 1 cup granulated sugar - 1 cup fine sugar - 8 egg whites - ...","title":"Cheat Sheet"},{"location":"curriculum/tutorial-template/#further-resources","text":"Links to further tutorials, videos, blog posts... - Basic Meringue Recipe","title":"Further Resources"},{"location":"curriculum/tutorial-template/#discussion-notes","text":"Optional. If interesting points come up during the discussion, add them here.","title":"Discussion Notes"},{"location":"dssg-manual/","text":"We've put together this manual for fellows in the Data Science for Social Good program. Like everything else we create, we are also making it available publicly to anyone interested in doing data science for social good, including potential fellows, mentors, and project partners, as well as those interested in funding or replicating such a program. The DSSG Manual # Welcome to the Data Science for Social Good (DSSG) Fellowship program! We hope your experience this summer will help you grow your skills as a data scientist and learn how to apply those skills to solve real-world problems with social impact. This manual outlines our goals in running this fellowship program, our hopes for your experience, and our expectations of the participants. We\u2019ve also outlined how the summer is typically structured and what you can expect. Things To Do Before You Arrive: Installing Software You'll Need for the Summer Read the Code of Conduct, Culture and Communications Learn What to Expect by Reading the Summer Overview Credits # This manual was created collaboratively by the Data Science and Public Policy team , with lots of help from various sources including those listed below. Contributors include Bridgit Donnelly, Matt Gee, Rayid Ghani, Maya Grever, Lauren Haynes, Jen Helsby, Lindsay Knight, Benedict Kuester, Joe Walsh, and Jane Zanzig. This manual is licensed under the Creative Commons Zero license. Parts of this manual are based on several other policies, including The Recurse Center User's Manual AlterConf Code of Conduct Django Community Code of Conduct SRCCON Code of Conduct Citizen Code of Conduct","title":"What is in this manual"},{"location":"dssg-manual/#the-dssg-manual","text":"Welcome to the Data Science for Social Good (DSSG) Fellowship program! We hope your experience this summer will help you grow your skills as a data scientist and learn how to apply those skills to solve real-world problems with social impact. This manual outlines our goals in running this fellowship program, our hopes for your experience, and our expectations of the participants. We\u2019ve also outlined how the summer is typically structured and what you can expect. Things To Do Before You Arrive: Installing Software You'll Need for the Summer Read the Code of Conduct, Culture and Communications Learn What to Expect by Reading the Summer Overview","title":"The DSSG Manual"},{"location":"dssg-manual/#credits","text":"This manual was created collaboratively by the Data Science and Public Policy team , with lots of help from various sources including those listed below. Contributors include Bridgit Donnelly, Matt Gee, Rayid Ghani, Maya Grever, Lauren Haynes, Jen Helsby, Lindsay Knight, Benedict Kuester, Joe Walsh, and Jane Zanzig. This manual is licensed under the Creative Commons Zero license. Parts of this manual are based on several other policies, including The Recurse Center User's Manual AlterConf Code of Conduct Django Community Code of Conduct SRCCON Code of Conduct Citizen Code of Conduct","title":"Credits"},{"location":"dssg-manual/conduct-culture-and-communications/","text":"Our Code of Conduct applies to all participants in all of our events open to external partners and the public, as well as to the fellowship itself. We want to ensure that everyone who comes into the DSSG space feels welcome, and we want to foster a safe, productive environment for fellows, staff, and visitors. Important Our Anti-Harassment Policy explicitly outlines the behavior for which we have a zero tolerance policy . If you feel that you or anyone else is being harassed or treated unfairly, or have any other concerns related to this policy, please contact a staff member. All contact with the staff about harassment will be confidential. Data Science for Social Good is dedicated to providing a harassment-free experience in all event venues, including talks, parties, and online media, for everyone regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age or religion. We do not tolerate harassment of participants in any form. Harassment includes offensive verbal comments related to gender, sexual orientation, disability, gender identity, age, race, religion, the use or display of sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any such behavior are expected to comply immediately. Participants asked to stop any harassing behavior are expected to comply immediately and may be sanctioned or expelled from the fellowship and/or all related events at the discretion of the organizers. Organizers may take any lawful action we deem appropriate, including but not limited to warning the offender or asking the offender to leave the specific event or the fellowship program as a whole. If you feel anyone has violated this policy, please bring it up directly with the individual or with a DSSG staff member. If you feel you have been unfairly accused of violating this code of conduct, please contact the fellowship staff team with a concise description of your grievance; all grievances will be considered.","title":"Code of conduct"},{"location":"dssg-manual/conduct-culture-and-communications/environment/","text":"The Space # Each year we reserve a space in the host city . While the space changes from year to year, in general, the layout is open and teams sit at desks together. There is private conference space for team meetings and calls with partners. We also provide access to a kitchen and provide a limited supply of snacks, as well as catered meals for special events. Let us know if you have any special food restrictions. You will learn more about space logistics (location, key card access, etc.) by email as they are finalized in the weeks leading up to the fellowship. The People # The foundation of any good project is a good team. We\u2019ve worked hard to recruit and hand pick a passionate and skilled team of interdisciplinary folks that all bring unique skills to your cohort. The fellowship is comprised of teams of three or four fellows each. Your teammates will be fellow students and recent graduates. We aim to ensure that each team has a mix of backgrounds, from computer science, statistics, math, physical science and engineering, social sciences and public policy. Your team will be assigned a Data Science Mentor . Each Mentor will be working with several teams of fellows, supporting their growth and project. All mentors are experienced data scientists who serve as a resource for you through the project development process, both with hands-on technical problems and implementation questions, as well as through higher-level design decisions. In addition to Data Science Mentors, you will have a Project Manager . Each Project Manager oversees several teams and is responsible for managing your relationship with your project partner. They work with you and your partner to set goals and deadlines, ensure proper communication between your team and your partner, and help tackle issues that are blocking your progress. Your Project Manager is also a great resource for questions about organizing teamwork, improving presentation skills, and communicating with the public. If fellows have any kind of questions or concerns that they would like to discuss confidentially, please tell a member of the staff. The staff member can raise concerns or ask for help on behalf of fellows, should they feel uncomfortable doing so themselves. The fellowship has one or more Interns , who help with all organizational and administrative tasks that the summer brings, like setting up the space, helping organize and publicize events, and recording tutorials. In their remaining time, interns might also help teams with their projects or work on their own self-directed data science projects. The Fellowship Organizers have spent months planning and preparing the summer program. They select fellows, mentors, project managers, interns, and project partners; find a space; secure funding; prepare the summer\u2019s curriculum; and plan all fellowship events. Over the summer, they will lead some of the fellowship-wide activities (such as the weekly deep dives and stand-ups), and teach some of the workshops. They also supervise the mentors, project managers, interns, and the communications manager. The Curriculum # Our goal is for you to learn a LOT this summer. We want you to feel empowered to drive your education throughout the summer. We see learning opportunities falling into three main buckets: Self-Directed Learning : We will share specific resources and guidelines for topics that we find useful in order to kick-start this learning process. We then encourage you to dive in and learn the skills most applicable to you and your growth. Peer-Directed Learning : We aim to create an environment that facilitates learning among fellows. Whether it\u2019s an informal discussion over lunch or a more formal teaching session, we encourage you to take advantage of the diversity of experiences and skills in the room. Fellowship-Directed Learning : Lastly, we have developed a specific workshop curriculum to cover basic concepts that we believe are essential for the summer. It is up to you to make sure you are seeking the resources you need to learn the skills you want to learn. For example, in the past, groups of fellows have started reading groups to learn about similar topics, like deep learning, together. That being said, if you are lost, speak up. You shouldn't feel that you are unable to make a meaningful contribution to your project. Your data science mentor is available to help you tackle skill gaps. In all of this, we recognize that there is no definitive \u201cData Science for Social Good\u201d curriculum or roadmap. We are charting new territory and developing it together. Throughout the summer, there will be plenty of opportunities for feedback and brainstorming on how to improve learning; for example, in past summers we have held an informal \u201cDunkin\u2019 Discussion\u201d series where we discuss the future of data science for social good over donuts. The Tools # We typically use GitHub for storing our codebase, a cloud service provider like Amazon Web Services for our data storage and analysis, Slack for fellowship-wide communication, and Trello for project management. We also store team-wide and fellowship-wide documents on Google Drive , and we schedule meetings on Google Calendar . You will receive an email address from the host institution to use for the duration of the fellowship. You are expected to use this for all fellowship-related communication. Call to action! Be sure that you create and share your username to all of this services. Also ask for the service's URL addresses specific to your project! The Communication # Teams will work together to develop specific team norms, but each team will typically have a daily morning stand up meeting with their project manager and data science/technical mentors. In these meetings, each fellow will have the opportunity to discuss what they did the day before, what they\u2019re planning to do today, and what they\u2019re stuck on. In addition to that daily meeting, teams will have regular (typically at least weekly) conference calls with their project partner to provide updates, ask questions, and receive feedback on their progress. The Fun # While this is a job \u2014 and we expect you to treat it as such \u2014 we would hate for the summer to be all work and no play. We want to help foster a community among your cohort. We start the summer off with a host of orientation events , including a fellowship-wide picnic, a variety of icebreakers, and a scavenger hunt for you to get to know the city. We host \u201cUn-DSSG\u201d, a day for you to share your side passions (from fondue making to dance) with your new peers. Throughout the summer, we host happy hours every other week, and invite the larger data science, tech, startup, government, and non-profit communities. Nearly daily, we pull out the ping pong table after hours, often leading to intense rivalries tracked on Slack by Pongbot.","title":"The DSSG Environment"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-space","text":"Each year we reserve a space in the host city . While the space changes from year to year, in general, the layout is open and teams sit at desks together. There is private conference space for team meetings and calls with partners. We also provide access to a kitchen and provide a limited supply of snacks, as well as catered meals for special events. Let us know if you have any special food restrictions. You will learn more about space logistics (location, key card access, etc.) by email as they are finalized in the weeks leading up to the fellowship.","title":"The Space"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-people","text":"The foundation of any good project is a good team. We\u2019ve worked hard to recruit and hand pick a passionate and skilled team of interdisciplinary folks that all bring unique skills to your cohort. The fellowship is comprised of teams of three or four fellows each. Your teammates will be fellow students and recent graduates. We aim to ensure that each team has a mix of backgrounds, from computer science, statistics, math, physical science and engineering, social sciences and public policy. Your team will be assigned a Data Science Mentor . Each Mentor will be working with several teams of fellows, supporting their growth and project. All mentors are experienced data scientists who serve as a resource for you through the project development process, both with hands-on technical problems and implementation questions, as well as through higher-level design decisions. In addition to Data Science Mentors, you will have a Project Manager . Each Project Manager oversees several teams and is responsible for managing your relationship with your project partner. They work with you and your partner to set goals and deadlines, ensure proper communication between your team and your partner, and help tackle issues that are blocking your progress. Your Project Manager is also a great resource for questions about organizing teamwork, improving presentation skills, and communicating with the public. If fellows have any kind of questions or concerns that they would like to discuss confidentially, please tell a member of the staff. The staff member can raise concerns or ask for help on behalf of fellows, should they feel uncomfortable doing so themselves. The fellowship has one or more Interns , who help with all organizational and administrative tasks that the summer brings, like setting up the space, helping organize and publicize events, and recording tutorials. In their remaining time, interns might also help teams with their projects or work on their own self-directed data science projects. The Fellowship Organizers have spent months planning and preparing the summer program. They select fellows, mentors, project managers, interns, and project partners; find a space; secure funding; prepare the summer\u2019s curriculum; and plan all fellowship events. Over the summer, they will lead some of the fellowship-wide activities (such as the weekly deep dives and stand-ups), and teach some of the workshops. They also supervise the mentors, project managers, interns, and the communications manager.","title":"The People"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-curriculum","text":"Our goal is for you to learn a LOT this summer. We want you to feel empowered to drive your education throughout the summer. We see learning opportunities falling into three main buckets: Self-Directed Learning : We will share specific resources and guidelines for topics that we find useful in order to kick-start this learning process. We then encourage you to dive in and learn the skills most applicable to you and your growth. Peer-Directed Learning : We aim to create an environment that facilitates learning among fellows. Whether it\u2019s an informal discussion over lunch or a more formal teaching session, we encourage you to take advantage of the diversity of experiences and skills in the room. Fellowship-Directed Learning : Lastly, we have developed a specific workshop curriculum to cover basic concepts that we believe are essential for the summer. It is up to you to make sure you are seeking the resources you need to learn the skills you want to learn. For example, in the past, groups of fellows have started reading groups to learn about similar topics, like deep learning, together. That being said, if you are lost, speak up. You shouldn't feel that you are unable to make a meaningful contribution to your project. Your data science mentor is available to help you tackle skill gaps. In all of this, we recognize that there is no definitive \u201cData Science for Social Good\u201d curriculum or roadmap. We are charting new territory and developing it together. Throughout the summer, there will be plenty of opportunities for feedback and brainstorming on how to improve learning; for example, in past summers we have held an informal \u201cDunkin\u2019 Discussion\u201d series where we discuss the future of data science for social good over donuts.","title":"The Curriculum"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-tools","text":"We typically use GitHub for storing our codebase, a cloud service provider like Amazon Web Services for our data storage and analysis, Slack for fellowship-wide communication, and Trello for project management. We also store team-wide and fellowship-wide documents on Google Drive , and we schedule meetings on Google Calendar . You will receive an email address from the host institution to use for the duration of the fellowship. You are expected to use this for all fellowship-related communication. Call to action! Be sure that you create and share your username to all of this services. Also ask for the service's URL addresses specific to your project!","title":"The Tools"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-communication","text":"Teams will work together to develop specific team norms, but each team will typically have a daily morning stand up meeting with their project manager and data science/technical mentors. In these meetings, each fellow will have the opportunity to discuss what they did the day before, what they\u2019re planning to do today, and what they\u2019re stuck on. In addition to that daily meeting, teams will have regular (typically at least weekly) conference calls with their project partner to provide updates, ask questions, and receive feedback on their progress.","title":"The Communication"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-fun","text":"While this is a job \u2014 and we expect you to treat it as such \u2014 we would hate for the summer to be all work and no play. We want to help foster a community among your cohort. We start the summer off with a host of orientation events , including a fellowship-wide picnic, a variety of icebreakers, and a scavenger hunt for you to get to know the city. We host \u201cUn-DSSG\u201d, a day for you to share your side passions (from fondue making to dance) with your new peers. Throughout the summer, we host happy hours every other week, and invite the larger data science, tech, startup, government, and non-profit communities. Nearly daily, we pull out the ping pong table after hours, often leading to intense rivalries tracked on Slack by Pongbot.","title":"The Fun"},{"location":"dssg-manual/conduct-culture-and-communications/goals/","text":"Goals of the Fellowship # Our long-term, overarching, unifying goal is to see more of this type of work happen in the world, and for our fellows to leave the program better equipped and more likely to use their skills for social good. To achieve this, our program focuses on three guiding goals. Our goals are (in order of priority): Training Our Fellows Increasing the responsible use of Data Science/ML/AI in Social Good Organizations Building and Supporting a Community of Data Scientists for Social Good [Tackling and Making an Impact on the Problems We Work on] We know that these goals are lofty. We do not presume or pretend to know the optimal way to achieve them, but we believe it's worth trying. We want all of you to help us by actively contributing ideas to improve our program and achieve these goals more effectively. Training Our Fellows # First and foremost, DSSG is a training program for fellows. We believe that the best way to learn is by working on real projects and not toy examples, which is why fellows work on real projects with real partners , in teams with other real people . The program provides training in the form of hands-on technical data science experience, but that\u2019s not enough to do real data science for social good. The training aspect of the fellowship also includes working with project partners, understanding social issues, using social science methods, working collaboratively on a team, developing solutions in an agile way, and communicating effectively to technical and non-technical audiences. We start off with a week of intensive orientation, getting everyone acquainted with each other and the structure of the program, as well as making sure all the fellows are up to speed with the tools and methods that are fellowship-wide standards. Training continues throughout the summer with lectures and workshops by staff, guest speakers, and fellow teach-outs. Dedicated full-time data science mentors and project managers will support and guide fellows throughout the summer. In short, we want the summer to be a productive and collaborative experience for you, and will provide you with many resources; however, your biggest resource will be yourself and the other fellows in your cohort . Every fellow contributes their own wealth of experience and expertise, and we aim to foster a learning environment where everyone can share this knowledge and learn from one another. Increasing the use of Responsible Data Science in Social Good Organizations # We believe that data-driven decisionmaking isn\u2019t reserved for companies selling online advertisements or banks trying to detect fraud. We know that data science can help governments, non-profits, and social good organizations do their work more effectively and serve their constituents more equitably. All of our project partners collect data, and many are already using data in some way, whether to evaluate their programs and write reports for their funders. However, most social good organizations have not used data science to actively inform their ongoing decision processes. Through this program, we aim to increase awareness of the benefits and challenges of data-driven impact work, both among the partners we work with and among non-profits and governments in general. Our project partners are partners, not clients. This means that the fellows work with the partners, not for them. We believe that participating in this program helps both the partners and the fellows develop a common language and learn how to work together more effectively. Sometimes fellows and partners won't see eye to eye on every decision, or the need to complete work within a deadline will mean you have to adjust your expectations, put GNU/Emacs 1 on the back burner, or sacrifice doing the work exactly the way you had hoped. While fellows' learning is our primary priority, it is important to note that part of what fellows are learning is how to work with partners to produce work that is useful for the partners and delivered on time. Building and Supporting a Community of Data Scientists for Social Good # We hope -- and expect -- that your impact as a DSSG alum continues beyond your summer tenure. Throughout the summer, we will introduce you to other practitioners within the data science and social good spaces to help you understand these sectors, form relationships, and start to think about potential contributions you could make. You\u2019ll also have the opportunity to network with local data science, tech, government, non-profit, and startup communities through regular fellowship-sponsored happy hours and meetups. Whether it\u2019s continuing to collaborate with your DSSG colleagues on other social good projects, joining the data team at a government agency, or working to recruit other like-minded people to apply their in-demand data science skills to impactful problems, we hope that this is just the beginning of of a lifetime as a data scientist for social good. Our Hopes for Your Fellowship Experience # The fellowship provides you with the opportunity to learn; to work on important, challenging, and unique projects; and to meet a lot of people who share your interests and goals. It is up to you to take advantage of these opportunities. We hope you use the summer to: Meet a group of fellows, staff and project partners who have a wealth of skills and experiences; listen to and learn from them; and make new friends. Embrace gaps in your knowledge as learning opportunities, exploring your limitations with respect to technical skills, new methods from different disciplines, project management, social issues, and teamwork. Learn about the challenges of working on real projects that don\u2019t have clean data, guaranteed results, or elegant solutions. Navigate the triangle between learning technical skills, creating deliverables that are useful and actionable for your project partner, and putting the varied skills within your team to good use. Rise to the challenge of working on a team that will include strong personalities with diverse experiences and strengths Realize the ambiguity and uncertainty that comes with working in a traditionally less tech-savvy sector, and learn how to communicate effectively to bridge this gap. Explore the impact (intentional and unintentional) that working with data from real, often disadvantaged or marginalized, people might have on them. Think deeply about the scope and limitations of technology to improve social problems. Explore existing roles in the field of data science for social good, find one you are best suited for, or create your own. Note Throughout the summer, we encourage you to share your ideas about how to improve the fellowship experience for yourself and others -- and to put them into action. We are constantly trying to improve the fellowship every day throughout the summer, and over time as we learn from each cohort. Our Expectations of Fellows # The fellowship offers a lot of freedom; however, we expect all fellows to stick to the basic principles of conduct listed below at all times. These guidelines apply to everybody at the fellowship, including mentors, project managers, and the fellowship organizers. If you feel that anyone is not behaving in accordance with these guidelines, we invite you to bring it up constructively. If you are unsure who to address, or if you do not feel comfortable doing so directly, you can bring up your concerns with the fellow advocate. Be supportive, open-minded, and willing to compromise. DSSG brings together people from different backgrounds and with different skills. In fact, this might be the best resource the fellowship has to offer! Share your knowledge and your experience with each other. Be patient as you teach each other, and have an open ear for your peers. Be professional. You will be working with NGOs, non-profits, and government institutions as project partners. You will also be presenting your work at and attending events with the general public. In all of these capacities you are acting as a representative of the DSSG community. We expect you to be professional \u2014 that is, respectful, friendly, and on time \u2014 in your conduct with partners and the public alike. Be resourceful and pragmatic. Own your own learning. Seek out resources as necessary. Don't be shy to ask others for help, but be mindful of their time - tell them what what you do understand, where you're stuck, and what you\u2019ve already tried, so they know how they can help. When you notice problems or have ideas for improvements \u2014 be it in your team, your project, or the fellowship \u2014 don\u2019t rely on others to notice or fix them; offer your initiative. Deal with conflicts maturely. There are many potential sources of conflict throughout the fellowship. It is perfectly acceptable, and even expected, that you will run into conflicts with your team, your mentors, your project, or the fellowship organizers. In any case, we ask you to be productive, pragmatic, and mature when dealing with conflicts. Keep an open mind, listen and communicate with everybody involved. Neither your project, nor your team, nor the fellowship will be perfect. Remember that everyone involved has invested a lot in the fellowship and wants all participants to be happy. Show up. We have all committed to be here for the duration of the program. The fellowship runs from approximately from end of May through the end of August 2 . We expect you to be in the office five days a week, to attend all DSSG-wide sessions, occasional special events, and the final event. We recognize that you have a life outside of the fellowship, and if you have any known or potential absences, you must inform DSSG staff upon your acceptance. Any additional conflicts that arise during the summer must be discussed with and agreed between you, your team, and your project manager well ahead of time. Take care of the space. Offices, meeting rooms, and kitchen areas are shared spaces. It\u2019s everybody\u2019s job to keep the space clean and free of messes. This policy also applies when you are attending off-site events. Stay involved and act as a steward. As a member of the DSSG community, we expect a commitment from you to stay involved, even after the summer. We ask you to seek opportunities to present the work you did, whether it's at your university, company, or events in your area. We ask you to assist your team in writing publications about your project, both during and after the fellowship. We will also ask you to help us with the application process in the following years by reviewing applications and interviewing candidates. We ask that you do what you're able to contribute to the DSSG mission and community. DON'T PANIC Regardless of how much experience you have, we admitted you because we believe that you can make a valuable contribution to your cohort, and we think being a DSSG fellow will help prepare you to do data science for social good in the real world. We've made a commitment to you and want to do everything we can to help you succeed. This is really important, so we'll say it again, in bold: If you're reading this, you are here because we want you to be here and believe that you are ready to make an impact. For example, don't worry about how much more or less productive, knowledgable, or experienced other fellows in your cohort might appear to be. It's easy to only pay attention - and compare yourself - to those who seem to be doing particularly well. Know that everyone has their own struggles, and everyone has good and bad days. Or if you prefer learning VIM \u21a9 In US: from Memorial Day through Labor Day, specific dates change by year and location \u21a9","title":"Goals of the Fellowship"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#goals-of-the-fellowship","text":"Our long-term, overarching, unifying goal is to see more of this type of work happen in the world, and for our fellows to leave the program better equipped and more likely to use their skills for social good. To achieve this, our program focuses on three guiding goals. Our goals are (in order of priority): Training Our Fellows Increasing the responsible use of Data Science/ML/AI in Social Good Organizations Building and Supporting a Community of Data Scientists for Social Good [Tackling and Making an Impact on the Problems We Work on] We know that these goals are lofty. We do not presume or pretend to know the optimal way to achieve them, but we believe it's worth trying. We want all of you to help us by actively contributing ideas to improve our program and achieve these goals more effectively.","title":"Goals of the Fellowship"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#training-our-fellows","text":"First and foremost, DSSG is a training program for fellows. We believe that the best way to learn is by working on real projects and not toy examples, which is why fellows work on real projects with real partners , in teams with other real people . The program provides training in the form of hands-on technical data science experience, but that\u2019s not enough to do real data science for social good. The training aspect of the fellowship also includes working with project partners, understanding social issues, using social science methods, working collaboratively on a team, developing solutions in an agile way, and communicating effectively to technical and non-technical audiences. We start off with a week of intensive orientation, getting everyone acquainted with each other and the structure of the program, as well as making sure all the fellows are up to speed with the tools and methods that are fellowship-wide standards. Training continues throughout the summer with lectures and workshops by staff, guest speakers, and fellow teach-outs. Dedicated full-time data science mentors and project managers will support and guide fellows throughout the summer. In short, we want the summer to be a productive and collaborative experience for you, and will provide you with many resources; however, your biggest resource will be yourself and the other fellows in your cohort . Every fellow contributes their own wealth of experience and expertise, and we aim to foster a learning environment where everyone can share this knowledge and learn from one another.","title":"Training Our Fellows"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#increasing-the-use-of-responsible-data-science-in-social-good-organizations","text":"We believe that data-driven decisionmaking isn\u2019t reserved for companies selling online advertisements or banks trying to detect fraud. We know that data science can help governments, non-profits, and social good organizations do their work more effectively and serve their constituents more equitably. All of our project partners collect data, and many are already using data in some way, whether to evaluate their programs and write reports for their funders. However, most social good organizations have not used data science to actively inform their ongoing decision processes. Through this program, we aim to increase awareness of the benefits and challenges of data-driven impact work, both among the partners we work with and among non-profits and governments in general. Our project partners are partners, not clients. This means that the fellows work with the partners, not for them. We believe that participating in this program helps both the partners and the fellows develop a common language and learn how to work together more effectively. Sometimes fellows and partners won't see eye to eye on every decision, or the need to complete work within a deadline will mean you have to adjust your expectations, put GNU/Emacs 1 on the back burner, or sacrifice doing the work exactly the way you had hoped. While fellows' learning is our primary priority, it is important to note that part of what fellows are learning is how to work with partners to produce work that is useful for the partners and delivered on time.","title":"Increasing the use of Responsible Data Science in Social Good Organizations"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#building-and-supporting-a-community-of-data-scientists-for-social-good","text":"We hope -- and expect -- that your impact as a DSSG alum continues beyond your summer tenure. Throughout the summer, we will introduce you to other practitioners within the data science and social good spaces to help you understand these sectors, form relationships, and start to think about potential contributions you could make. You\u2019ll also have the opportunity to network with local data science, tech, government, non-profit, and startup communities through regular fellowship-sponsored happy hours and meetups. Whether it\u2019s continuing to collaborate with your DSSG colleagues on other social good projects, joining the data team at a government agency, or working to recruit other like-minded people to apply their in-demand data science skills to impactful problems, we hope that this is just the beginning of of a lifetime as a data scientist for social good.","title":"Building and Supporting a Community of Data Scientists for Social Good"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#our-hopes-for-your-fellowship-experience","text":"The fellowship provides you with the opportunity to learn; to work on important, challenging, and unique projects; and to meet a lot of people who share your interests and goals. It is up to you to take advantage of these opportunities. We hope you use the summer to: Meet a group of fellows, staff and project partners who have a wealth of skills and experiences; listen to and learn from them; and make new friends. Embrace gaps in your knowledge as learning opportunities, exploring your limitations with respect to technical skills, new methods from different disciplines, project management, social issues, and teamwork. Learn about the challenges of working on real projects that don\u2019t have clean data, guaranteed results, or elegant solutions. Navigate the triangle between learning technical skills, creating deliverables that are useful and actionable for your project partner, and putting the varied skills within your team to good use. Rise to the challenge of working on a team that will include strong personalities with diverse experiences and strengths Realize the ambiguity and uncertainty that comes with working in a traditionally less tech-savvy sector, and learn how to communicate effectively to bridge this gap. Explore the impact (intentional and unintentional) that working with data from real, often disadvantaged or marginalized, people might have on them. Think deeply about the scope and limitations of technology to improve social problems. Explore existing roles in the field of data science for social good, find one you are best suited for, or create your own. Note Throughout the summer, we encourage you to share your ideas about how to improve the fellowship experience for yourself and others -- and to put them into action. We are constantly trying to improve the fellowship every day throughout the summer, and over time as we learn from each cohort.","title":"Our Hopes for Your Fellowship Experience"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#our-expectations-of-fellows","text":"The fellowship offers a lot of freedom; however, we expect all fellows to stick to the basic principles of conduct listed below at all times. These guidelines apply to everybody at the fellowship, including mentors, project managers, and the fellowship organizers. If you feel that anyone is not behaving in accordance with these guidelines, we invite you to bring it up constructively. If you are unsure who to address, or if you do not feel comfortable doing so directly, you can bring up your concerns with the fellow advocate. Be supportive, open-minded, and willing to compromise. DSSG brings together people from different backgrounds and with different skills. In fact, this might be the best resource the fellowship has to offer! Share your knowledge and your experience with each other. Be patient as you teach each other, and have an open ear for your peers. Be professional. You will be working with NGOs, non-profits, and government institutions as project partners. You will also be presenting your work at and attending events with the general public. In all of these capacities you are acting as a representative of the DSSG community. We expect you to be professional \u2014 that is, respectful, friendly, and on time \u2014 in your conduct with partners and the public alike. Be resourceful and pragmatic. Own your own learning. Seek out resources as necessary. Don't be shy to ask others for help, but be mindful of their time - tell them what what you do understand, where you're stuck, and what you\u2019ve already tried, so they know how they can help. When you notice problems or have ideas for improvements \u2014 be it in your team, your project, or the fellowship \u2014 don\u2019t rely on others to notice or fix them; offer your initiative. Deal with conflicts maturely. There are many potential sources of conflict throughout the fellowship. It is perfectly acceptable, and even expected, that you will run into conflicts with your team, your mentors, your project, or the fellowship organizers. In any case, we ask you to be productive, pragmatic, and mature when dealing with conflicts. Keep an open mind, listen and communicate with everybody involved. Neither your project, nor your team, nor the fellowship will be perfect. Remember that everyone involved has invested a lot in the fellowship and wants all participants to be happy. Show up. We have all committed to be here for the duration of the program. The fellowship runs from approximately from end of May through the end of August 2 . We expect you to be in the office five days a week, to attend all DSSG-wide sessions, occasional special events, and the final event. We recognize that you have a life outside of the fellowship, and if you have any known or potential absences, you must inform DSSG staff upon your acceptance. Any additional conflicts that arise during the summer must be discussed with and agreed between you, your team, and your project manager well ahead of time. Take care of the space. Offices, meeting rooms, and kitchen areas are shared spaces. It\u2019s everybody\u2019s job to keep the space clean and free of messes. This policy also applies when you are attending off-site events. Stay involved and act as a steward. As a member of the DSSG community, we expect a commitment from you to stay involved, even after the summer. We ask you to seek opportunities to present the work you did, whether it's at your university, company, or events in your area. We ask you to assist your team in writing publications about your project, both during and after the fellowship. We will also ask you to help us with the application process in the following years by reviewing applications and interviewing candidates. We ask that you do what you're able to contribute to the DSSG mission and community. DON'T PANIC Regardless of how much experience you have, we admitted you because we believe that you can make a valuable contribution to your cohort, and we think being a DSSG fellow will help prepare you to do data science for social good in the real world. We've made a commitment to you and want to do everything we can to help you succeed. This is really important, so we'll say it again, in bold: If you're reading this, you are here because we want you to be here and believe that you are ready to make an impact. For example, don't worry about how much more or less productive, knowledgable, or experienced other fellows in your cohort might appear to be. It's easy to only pay attention - and compare yourself - to those who seem to be doing particularly well. Know that everyone has their own struggles, and everyone has good and bad days. Or if you prefer learning VIM \u21a9 In US: from Memorial Day through Labor Day, specific dates change by year and location \u21a9","title":"Our Expectations of Fellows"},{"location":"dssg-manual/summer-overview/","text":"What to Expect # While specific schedules will vary from project to project, the summer will follow roughly the structure below. See also high level summer plan for an outline of the flow of the summer, and general concepts that will inform the topics addressed by tutorials and speakers as well as what we'll ask fellows to present about in weekly updates and deep dives. Before the Summer # Prior to your arrival, we provide you with the prerequisites so you can familiarize yourself with the tools you\u2019ll use all summer and equip yourself with the knowledge to be able to follow along with the curriculum. You'll receive a list of software to install before the first day of orientation, programming languages you should brush up on, and tools we suggest you use to manage your data workflow. Learning about Projects and Partners # Staff at the Data Science and Public Policy team at CMU and DSSG alum volunteers have worked hard to recruit partners and scope projects. This is a lengthy, complicated process with plenty of logistical hurdles (think legal data sharing agreements and data transfer challenges), which means the list of project partners is usually not finalized until the fellowship begins. Based on your preferences, the requirements of each project, and the balance of disciplines within each team, we will assign teams of typically 4 fellows per project. You will learn which project you've been assigned during the second week of the fellowship. We want you to meet the people you\u2019re working with face to face. We ask all our project partners to come meet their team in the second of the summer. During partner visits, you\u2019ll spend a lot of time talking with them about the problem and the data, and they will give a presentation to the fellowship. This also gives all of the fellows a chance to hear about all of the projects and for the partners to meet other project partners and the other fellows. After orientation, you will spend the first part of the summer getting to know your project partner and their unique challenges. While the projects have already been scoped, you will almost certainly need to refine that scope throughout the summer. For example, we may know your partner\u2019s goal is to find violations of a particular law. Your team would then work with the partner to narrow that to: (1) locations at risk of violations in general; (2) locations at risk of multiple violations; or (3) locations with the most impactful violations. We believe it is important for you to thoroughly understand the problem and the process that gives rise to the data before getting too entrenched in the data itself. A deep understanding of your partner and the problems they face is crucial to knowing what your variables really mean. Defining your outcome and evaluation metrics will depend heavily upon this understanding. While we know you are eager to dig into the data, you will find that at least as much of your time is devoted to talking about the data as manipulating it. This is a good thing. You will also be working with real data, which is messy! You will encounter missing values and things that don\u2019t seem to make sense. Talking to your partners and telling them what you see from looking at the data they\u2019ve given you will help you evaluate your own understanding, reconcile inconsistencies (or carry on being aware of them), and identify whether the errors lie in the data itself or in whatever preconceived notions you had. Working on Your Project # Although we aim to have all the data from project partners ready well in advance of the fellowship, there are inevitably data transfer delays and partial data that will continue to be augmented throughout - and sometimes after - the fellowship. As you explore, you'll find holes in the provided information, or identify potential new useful sources of data, and will need to work with your partner to decide whether it\u2019s possible to acquire the data you need in the time that you have; that\u2019s the reality of working with real world partners and sensitive data! Fellows drive the work of every project, learning their subject matter in depth, writing code, and collaborating with their project partners to develop something useful and usable. Over the course of the summer, your team will: Explore the (real) data your partners collect Design your project workflow based on what tools you'll use and how your team works together Identify user stories to make sure what you're creating has a real purpose Develop a machine learning pipeline to turn raw data into analysis that can inform decisions Build relevant models that reflect the subject you're analyzing as closely as possible Add features to your model based on subject matter expertise, available data, and exploratory analysis Evaluate model performance using the metrics that make sense for your project Create an interface for your partner to use your results (API, dashboard applications, etc) Presentations # We believe that our work is only useful if we are able to communicate what we do and why it\u2019s important to our partners, peers, and the general public. As such, an important piece of this training program is learning to present the work you do . Each week, a member of your team will give a 3-5 minute update to the entire fellowship, outlining your recent progress and findings, giving shoutouts to other fellows or staff members who have helped you along the way, and things you're stuck on and are seeking help with. Two teams per week (so each team presents 2-3 times throughout the summer) will give a longer 20-minute \"deep dive\" presentation, outlining more technical components of your project and seeking feedback from other fellows and mentors. At the end of the summer\u2026 At the end of the summer, your team will develop two polished presentations : one 5-minute presentation for the DataFest final event, and one 20-minute technical presentation for use at a local tech meetup at the end of the summer and for future presentations at conferences or your home institution. In the last few weeks of the fellowship, each team will present at a local meetup. Each team will elect one team member to deliver the short final presentation at DataFest; however, all team members should feel comfortable delivering all presentations. Our communications staff will work with you on both of these presentations, brainstorming ways to present your work and providing feedback on your delivery. Wrap-Up and Handovers # To make sure the work you do this summer has real impact, a lot more work needs to be done after the official end of the fellowship; some of this will be done by your project partner, and some will be done by the Data Science and Public Policy team at Carnegie Mellon University. You will need to transition the work over to your project partners so they can validate, implement, and extend your work. To do this, you will have to document your work throughout the summer and wrap it up neatly at the end of the summer. We ask that you prepare a poster to be displayed at DSSG events and for potential conference poster sessions, a technical report, and an outline of a paper. This makes it easier to collaborate once you and your teammates are no longer working at the same desk every day. We also ask that all your code is tested to run on a new machine, and that there is sufficient documentation for someone else to replicate and understand your work. Curriculum # Our number one goal is to train the fellows to do data science for social good work. Here is some insight into how we accomplish this throughout the summer. To look through all of our curriculum materials, please see the curriculum section . Orientation # We expect that every incoming fellow has experience programming in Python, a basic working knowledge of statistics and social science, and an interest in doing social good work. However, we understand that everyone comes from a different background, so to ensure that everyone is able to contribute as a productive member of the team and the fellowship, we start the first few weeks off with an intensive orientation See sample from 2022 , getting everyone \"up to speed\" with the basic skills and tools they'll need. Typical schedule # Week One Software Setup Pipelines and Project Workflow Git and Github Making the Fellowship Skills You Need to do DSSG Command Line Tools Project Management, Partners, and Communications Data Exploration in Python Project Scoping Intro Week Two Usability and User Interfaces CSV to DB Legal Agreements Data Security Primer Legible, Good Code Conduct, Culture, and Communications Week Three Reproducible ETL The Work We Do Record Linkage Databases Quantitative Social Science Ongoing Curriculum # Training continues on throughout the summer in the form of \"lunch and learns\" - less formal lessons over lunch - and teachouts by staff or fellows who have relevant specializations. Sometimes we ask for volunteers to do a teachout on a topic we think is important, like data visualization or inference with observational data, and a few fellows will work together to put together a lesson. Sometimes a DSSGer will suggest a topic that they have a pet interest in, or that they think will be relevant to one or more of the summer projects. We have lunch and learns scheduled twice a week through the summer, and some fellows choose to offer optional teachouts at the end of the workday. Although we don't expect all teams to be working in unison, there is a general structure to the summer that guides how we pace the remaining curriculum - we try to schedule topics so that fellows know about them with enough time to incorporate them into their projects, but not so early that they've forgotten about what they learned by the time the knowledge would be useful. As we get nearer to the end of the summer, there are fewer required topics, so there are more open time slots for fellows to do teachouts. Example topics for the Rest of the Summer Educational Data and Testing Social Good Business Models Basic Web Scraping Pipelines and Evaluation Feature Generation Workshop Test, Test, Test Beyond the Deep Learning Hype Causal Inference with Observational Data Model Evaluation Spatial Analysis Tools Operations Research Theory and Theorizing in the Social Sciences Web Classification Presentation Skills Data Visualization Natural Language Processing Opening Closed Data","title":"Summer overview"},{"location":"dssg-manual/summer-overview/#what-to-expect","text":"While specific schedules will vary from project to project, the summer will follow roughly the structure below. See also high level summer plan for an outline of the flow of the summer, and general concepts that will inform the topics addressed by tutorials and speakers as well as what we'll ask fellows to present about in weekly updates and deep dives.","title":"What to Expect"},{"location":"dssg-manual/summer-overview/#before-the-summer","text":"Prior to your arrival, we provide you with the prerequisites so you can familiarize yourself with the tools you\u2019ll use all summer and equip yourself with the knowledge to be able to follow along with the curriculum. You'll receive a list of software to install before the first day of orientation, programming languages you should brush up on, and tools we suggest you use to manage your data workflow.","title":"Before the Summer"},{"location":"dssg-manual/summer-overview/#learning-about-projects-and-partners","text":"Staff at the Data Science and Public Policy team at CMU and DSSG alum volunteers have worked hard to recruit partners and scope projects. This is a lengthy, complicated process with plenty of logistical hurdles (think legal data sharing agreements and data transfer challenges), which means the list of project partners is usually not finalized until the fellowship begins. Based on your preferences, the requirements of each project, and the balance of disciplines within each team, we will assign teams of typically 4 fellows per project. You will learn which project you've been assigned during the second week of the fellowship. We want you to meet the people you\u2019re working with face to face. We ask all our project partners to come meet their team in the second of the summer. During partner visits, you\u2019ll spend a lot of time talking with them about the problem and the data, and they will give a presentation to the fellowship. This also gives all of the fellows a chance to hear about all of the projects and for the partners to meet other project partners and the other fellows. After orientation, you will spend the first part of the summer getting to know your project partner and their unique challenges. While the projects have already been scoped, you will almost certainly need to refine that scope throughout the summer. For example, we may know your partner\u2019s goal is to find violations of a particular law. Your team would then work with the partner to narrow that to: (1) locations at risk of violations in general; (2) locations at risk of multiple violations; or (3) locations with the most impactful violations. We believe it is important for you to thoroughly understand the problem and the process that gives rise to the data before getting too entrenched in the data itself. A deep understanding of your partner and the problems they face is crucial to knowing what your variables really mean. Defining your outcome and evaluation metrics will depend heavily upon this understanding. While we know you are eager to dig into the data, you will find that at least as much of your time is devoted to talking about the data as manipulating it. This is a good thing. You will also be working with real data, which is messy! You will encounter missing values and things that don\u2019t seem to make sense. Talking to your partners and telling them what you see from looking at the data they\u2019ve given you will help you evaluate your own understanding, reconcile inconsistencies (or carry on being aware of them), and identify whether the errors lie in the data itself or in whatever preconceived notions you had.","title":"Learning about Projects and Partners"},{"location":"dssg-manual/summer-overview/#working-on-your-project","text":"Although we aim to have all the data from project partners ready well in advance of the fellowship, there are inevitably data transfer delays and partial data that will continue to be augmented throughout - and sometimes after - the fellowship. As you explore, you'll find holes in the provided information, or identify potential new useful sources of data, and will need to work with your partner to decide whether it\u2019s possible to acquire the data you need in the time that you have; that\u2019s the reality of working with real world partners and sensitive data! Fellows drive the work of every project, learning their subject matter in depth, writing code, and collaborating with their project partners to develop something useful and usable. Over the course of the summer, your team will: Explore the (real) data your partners collect Design your project workflow based on what tools you'll use and how your team works together Identify user stories to make sure what you're creating has a real purpose Develop a machine learning pipeline to turn raw data into analysis that can inform decisions Build relevant models that reflect the subject you're analyzing as closely as possible Add features to your model based on subject matter expertise, available data, and exploratory analysis Evaluate model performance using the metrics that make sense for your project Create an interface for your partner to use your results (API, dashboard applications, etc)","title":"Working on Your Project"},{"location":"dssg-manual/summer-overview/#presentations","text":"We believe that our work is only useful if we are able to communicate what we do and why it\u2019s important to our partners, peers, and the general public. As such, an important piece of this training program is learning to present the work you do . Each week, a member of your team will give a 3-5 minute update to the entire fellowship, outlining your recent progress and findings, giving shoutouts to other fellows or staff members who have helped you along the way, and things you're stuck on and are seeking help with. Two teams per week (so each team presents 2-3 times throughout the summer) will give a longer 20-minute \"deep dive\" presentation, outlining more technical components of your project and seeking feedback from other fellows and mentors. At the end of the summer\u2026 At the end of the summer, your team will develop two polished presentations : one 5-minute presentation for the DataFest final event, and one 20-minute technical presentation for use at a local tech meetup at the end of the summer and for future presentations at conferences or your home institution. In the last few weeks of the fellowship, each team will present at a local meetup. Each team will elect one team member to deliver the short final presentation at DataFest; however, all team members should feel comfortable delivering all presentations. Our communications staff will work with you on both of these presentations, brainstorming ways to present your work and providing feedback on your delivery.","title":"Presentations"},{"location":"dssg-manual/summer-overview/#wrap-up-and-handovers","text":"To make sure the work you do this summer has real impact, a lot more work needs to be done after the official end of the fellowship; some of this will be done by your project partner, and some will be done by the Data Science and Public Policy team at Carnegie Mellon University. You will need to transition the work over to your project partners so they can validate, implement, and extend your work. To do this, you will have to document your work throughout the summer and wrap it up neatly at the end of the summer. We ask that you prepare a poster to be displayed at DSSG events and for potential conference poster sessions, a technical report, and an outline of a paper. This makes it easier to collaborate once you and your teammates are no longer working at the same desk every day. We also ask that all your code is tested to run on a new machine, and that there is sufficient documentation for someone else to replicate and understand your work.","title":"Wrap-Up and Handovers"},{"location":"dssg-manual/summer-overview/#curriculum","text":"Our number one goal is to train the fellows to do data science for social good work. Here is some insight into how we accomplish this throughout the summer. To look through all of our curriculum materials, please see the curriculum section .","title":"Curriculum"},{"location":"dssg-manual/summer-overview/#orientation","text":"We expect that every incoming fellow has experience programming in Python, a basic working knowledge of statistics and social science, and an interest in doing social good work. However, we understand that everyone comes from a different background, so to ensure that everyone is able to contribute as a productive member of the team and the fellowship, we start the first few weeks off with an intensive orientation See sample from 2022 , getting everyone \"up to speed\" with the basic skills and tools they'll need.","title":"Orientation"},{"location":"dssg-manual/summer-overview/#typical-schedule","text":"Week One Software Setup Pipelines and Project Workflow Git and Github Making the Fellowship Skills You Need to do DSSG Command Line Tools Project Management, Partners, and Communications Data Exploration in Python Project Scoping Intro Week Two Usability and User Interfaces CSV to DB Legal Agreements Data Security Primer Legible, Good Code Conduct, Culture, and Communications Week Three Reproducible ETL The Work We Do Record Linkage Databases Quantitative Social Science","title":"Typical schedule"},{"location":"dssg-manual/summer-overview/#ongoing-curriculum","text":"Training continues on throughout the summer in the form of \"lunch and learns\" - less formal lessons over lunch - and teachouts by staff or fellows who have relevant specializations. Sometimes we ask for volunteers to do a teachout on a topic we think is important, like data visualization or inference with observational data, and a few fellows will work together to put together a lesson. Sometimes a DSSGer will suggest a topic that they have a pet interest in, or that they think will be relevant to one or more of the summer projects. We have lunch and learns scheduled twice a week through the summer, and some fellows choose to offer optional teachouts at the end of the workday. Although we don't expect all teams to be working in unison, there is a general structure to the summer that guides how we pace the remaining curriculum - we try to schedule topics so that fellows know about them with enough time to incorporate them into their projects, but not so early that they've forgotten about what they learned by the time the knowledge would be useful. As we get nearer to the end of the summer, there are fewer required topics, so there are more open time slots for fellows to do teachouts. Example topics for the Rest of the Summer Educational Data and Testing Social Good Business Models Basic Web Scraping Pipelines and Evaluation Feature Generation Workshop Test, Test, Test Beyond the Deep Learning Hype Causal Inference with Observational Data Model Evaluation Spatial Analysis Tools Operations Research Theory and Theorizing in the Social Sciences Web Classification Presentation Skills Data Visualization Natural Language Processing Opening Closed Data","title":"Ongoing Curriculum"},{"location":"tech-tutorials/model_eval/","text":"Model Evaluation and Bias in Data Science Projects # We have 6 lessons - two sessions a day over three days. One session a day (or the latter half of the second session in a given day) should be hands-on. We can give homework, which could simply be the 'leftovers' of a given day's hands-on session. Day 1 introduction & brief outline of workshop examples of DSaPP projects explanation of common data structure (temporal data, entities, ...) examples of common features (spatiotemporal aggregations, prior behavior, 'static' features like demographics) examples of common outcome definitions ('binarizations') What do we mean by 'successful' predictions? Examples of deployments; explain resource constraints. accuracy, precision, ROC top-k precision, recall-precision curves list stability Detour into caveats: We are not working causally. We are not (most of the time) modeling inspection densities. We are not (most of the time) providing statistical inference, optimality guarantees, or anything like that. Delineate 'science' vs 'engineering' approach (here, we are sharing conceptual and technical tools, not scientific, or even empirical, knowledge). Encourage questions & suggestions throughout workshop. introduce toy/example dataset and have students connect to SQL DB ( maybe in next session? ) simple temporal cross-validation ('as-of' date, label window, feature window, metrics) train, test, evaluation sets (TODO: ensure universal terminology) caveats on temporal cross-validation: incorrect dates, overwritten or backfilled data (example: student dropouts), data updating frequencies (e.g. end-of-year reports), non-stationary derivatives in the data, changes in policy, changes in systems, bias in the labelling process (e.g. biased judges) Exercise: Implement temporal split generator, label generator, feature getter, label-feature join, some evaluation metrics, on toy data set with pre-built features Day 2 difference between incident and label dates, and how that influence label generation; caveat on correlates of the incident that are predictive of the label leakage, and examples of hard-to-find leakage prediction horizons (e.g. in education) prediction frequencies (depending on deployment) overlapping label windows (caveat: dependent 'samples') overlapping feature windows (generally no problem) label rank breaking (unknown k vs additional randomization) 'status list' of entities - finding labels for null entities in a given label window imputation - finding features for null entities in a given feature window; counts/mean/median caveat on leakage dummification, and the problems of knowing the set of all levels upfront \"train however you like, test however you deploy\" sub-sampling of training and/or testing data (?) making train and/or test sets more reprensetative (?) modeling/weighting by P(I) / inverse probability weighting (?) evaluating predictions in field trials / problems of randomized trials on ranked lists (?) novelty vs accuracy (Cincinnati?) stability across temporal splits Exercise: Implement label generator with incident-vs-decision date, in-split imputation, global dummification, plots for temporal stability Day 3 characterizing models via entities, not via labels (back ref: list overlap) cross-tabs on categorical features or discretized continuous features between high-risk/low-risk predictions scatter plots on continuous features simple significance tests (chi-square, KS-test); caveat: multiple testing, dependency between features protected class definitions not sufficient: removing protected features population parity disparate impact test equality extension from equal FNR/FPR to independence of score and class; removing dependency on thresholds (is there an equivalent formulation for equal precision?) caveat about curse of dimensionality - marginal bias is 'easy', but interactions of bias are very many (e.g. difficulties of testing not only for discrimination against age, or race, or gender, but also for discrimination against (age x race x gender) ) science thriller: ProPublica's COMPASS coverage, initial response articles, follow-up articles Exercise: calculating cross-tabs on toy data set; calculating condtioned cross-tabs and scatter plots for disparate impact and test equality; applying chi-square test","title":"Model Evaluation and Bias in Data Science Projects"},{"location":"tech-tutorials/model_eval/#model-evaluation-and-bias-in-data-science-projects","text":"We have 6 lessons - two sessions a day over three days. One session a day (or the latter half of the second session in a given day) should be hands-on. We can give homework, which could simply be the 'leftovers' of a given day's hands-on session. Day 1 introduction & brief outline of workshop examples of DSaPP projects explanation of common data structure (temporal data, entities, ...) examples of common features (spatiotemporal aggregations, prior behavior, 'static' features like demographics) examples of common outcome definitions ('binarizations') What do we mean by 'successful' predictions? Examples of deployments; explain resource constraints. accuracy, precision, ROC top-k precision, recall-precision curves list stability Detour into caveats: We are not working causally. We are not (most of the time) modeling inspection densities. We are not (most of the time) providing statistical inference, optimality guarantees, or anything like that. Delineate 'science' vs 'engineering' approach (here, we are sharing conceptual and technical tools, not scientific, or even empirical, knowledge). Encourage questions & suggestions throughout workshop. introduce toy/example dataset and have students connect to SQL DB ( maybe in next session? ) simple temporal cross-validation ('as-of' date, label window, feature window, metrics) train, test, evaluation sets (TODO: ensure universal terminology) caveats on temporal cross-validation: incorrect dates, overwritten or backfilled data (example: student dropouts), data updating frequencies (e.g. end-of-year reports), non-stationary derivatives in the data, changes in policy, changes in systems, bias in the labelling process (e.g. biased judges) Exercise: Implement temporal split generator, label generator, feature getter, label-feature join, some evaluation metrics, on toy data set with pre-built features Day 2 difference between incident and label dates, and how that influence label generation; caveat on correlates of the incident that are predictive of the label leakage, and examples of hard-to-find leakage prediction horizons (e.g. in education) prediction frequencies (depending on deployment) overlapping label windows (caveat: dependent 'samples') overlapping feature windows (generally no problem) label rank breaking (unknown k vs additional randomization) 'status list' of entities - finding labels for null entities in a given label window imputation - finding features for null entities in a given feature window; counts/mean/median caveat on leakage dummification, and the problems of knowing the set of all levels upfront \"train however you like, test however you deploy\" sub-sampling of training and/or testing data (?) making train and/or test sets more reprensetative (?) modeling/weighting by P(I) / inverse probability weighting (?) evaluating predictions in field trials / problems of randomized trials on ranked lists (?) novelty vs accuracy (Cincinnati?) stability across temporal splits Exercise: Implement label generator with incident-vs-decision date, in-split imputation, global dummification, plots for temporal stability Day 3 characterizing models via entities, not via labels (back ref: list overlap) cross-tabs on categorical features or discretized continuous features between high-risk/low-risk predictions scatter plots on continuous features simple significance tests (chi-square, KS-test); caveat: multiple testing, dependency between features protected class definitions not sufficient: removing protected features population parity disparate impact test equality extension from equal FNR/FPR to independence of score and class; removing dependency on thresholds (is there an equivalent formulation for equal precision?) caveat about curse of dimensionality - marginal bias is 'easy', but interactions of bias are very many (e.g. difficulties of testing not only for discrimination against age, or race, or gender, but also for discrimination against (age x race x gender) ) science thriller: ProPublica's COMPASS coverage, initial response articles, follow-up articles Exercise: calculating cross-tabs on toy data set; calculating condtioned cross-tabs and scatter plots for disparate impact and test equality; applying chi-square test","title":"Model Evaluation and Bias in Data Science Projects"}]}