{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Hitchhiker's Guide to Data Science for Social Good. # What is the Data Science for Social Good Fellowship? # The Data Science for Social Good Fellowship (DSSG) is a hands-on and project-based summer program that launched in 2013 at the University of Chicago and has now expanded to multiple locations globally. It brings data science fellows from across the world to work on machine learning, artificial intelligence, and data science projects that have a social impact. From a pool of typically over 800 applicants, 20-40 fellows are selected from diverse computational and quantitative disciplines including computer science, statistics, math, engineering, sociology, economics, and public policy backgrounds. The fellows work in small, cross-disciplinary teams on social good projects spanning education, health, energy, transportation, criminal justice, social services, economic development and international development in collaboration with global government agencies and non-profits. This work is done under close and hands-on mentorship from full-time, dedicated data science mentors as well as dedicated project managers, with industry experience. The result is highly trained fellows, improved data science capacity of the social good organization, and a high quality data science project that is ready for field trial and implementation, delivered at the end of the program. In addition to hands-on project-based training, the summer program also consists of workshops, tutorials, and discussion groups based on our data science for social good curriculum designed to train the fellows in doing practical data science and artificial intelligence for social impact. Who is this guide for? # Our number one priority at DSSG is to train fellows to do data science for social good work . This curriculum includes many things you'd find in a data science course or bootcamp, but with an emphasis on solving problems with social impact, integrating data science with the social sciences, discussing ethical implications of the work, as well as privacy, and confidentiality issues. If you are applying to the program or have been accepted as a fellow, check out the manual to see how you can prepare before arriving, what orientation and training will cover, and what to expect from the summer. If you are interested in learning at home, check out the tutorials and teach-outs developed by our staff and fellows throughout the summer, and to suggest or contribute additional resources. *Another one of our goals is to encourage collaborations. Anyone interested in doing this type of work, or starting a DSSG program, to build on what we've learned by using and contributing to these resources. What is in this guide? # We have spent many (sort of) early mornings waxing existential over Dunkin' Donuts while trying to define what makes a \"data scientist for social good,\" that enigmatic breed combining one part data scientist, one part consultant, one part educator, and one part bleeding heart idealist. We've come to a rough working definition in the form of the skills and knowledge one would need, which we categorize as follows: Programming, because you'll need to tell your computer what to do, usually by writing code. Computer science, because you'll need to understand how your data is - and should be - structured, as well as the algorithms you use to analyze it. Math and stats, because everything else in life is just applied math , and numerical results are meaningless without some measure of uncertainty. Machine learning, because you'll want to build predictive or descriptive models that can learn, evolve, and improve over time. Social science, because you'll need to know how to design experiments to validate your models in the field, and to understand when correlation can plausibly suggest causation, and sometimes even do causal inference. Problem and Project Scoping, because you'll need to be able to go from a vague and fuzzy project description to a problem you can solve, understand the goals of the project, the interventions you are informing, the data you have and need, and the analysis that needs to be done. Project management, to make progress as a team, to work effectively with your project partner, and work with a team to make that useful solution actually happen. Privacy and security, because data is people and needs to be kept secure and confidential. Ethics, fairness, bias, and transparency, because your work has the potential to be misused or have a negative impact on people's lives, so you have to consider the biases in your data and analyses, the ethical and fairness implications, and how to make your work interpretable and transparent to the users and to the people impacted by it. Communications, because you'll need to be able to tell the story of why what you're doing matters and the methods you're using to a broad audience. Social issues, because you're doing this work to help people, and you don't live or work in a vacuum, so you need to understand the context and history surrounding the people, places and issues you want to impact. All material is licensed under CC-BY 4.0","title":"Home"},{"location":"#welcome-to-the-hitchhikers-guide-to-data-science-for-social-good","text":"","title":"Welcome to the Hitchhiker's Guide to Data Science for Social Good."},{"location":"#what-is-the-data-science-for-social-good-fellowship","text":"The Data Science for Social Good Fellowship (DSSG) is a hands-on and project-based summer program that launched in 2013 at the University of Chicago and has now expanded to multiple locations globally. It brings data science fellows from across the world to work on machine learning, artificial intelligence, and data science projects that have a social impact. From a pool of typically over 800 applicants, 20-40 fellows are selected from diverse computational and quantitative disciplines including computer science, statistics, math, engineering, sociology, economics, and public policy backgrounds. The fellows work in small, cross-disciplinary teams on social good projects spanning education, health, energy, transportation, criminal justice, social services, economic development and international development in collaboration with global government agencies and non-profits. This work is done under close and hands-on mentorship from full-time, dedicated data science mentors as well as dedicated project managers, with industry experience. The result is highly trained fellows, improved data science capacity of the social good organization, and a high quality data science project that is ready for field trial and implementation, delivered at the end of the program. In addition to hands-on project-based training, the summer program also consists of workshops, tutorials, and discussion groups based on our data science for social good curriculum designed to train the fellows in doing practical data science and artificial intelligence for social impact.","title":"What is the Data Science for Social Good Fellowship?"},{"location":"#who-is-this-guide-for","text":"Our number one priority at DSSG is to train fellows to do data science for social good work . This curriculum includes many things you'd find in a data science course or bootcamp, but with an emphasis on solving problems with social impact, integrating data science with the social sciences, discussing ethical implications of the work, as well as privacy, and confidentiality issues. If you are applying to the program or have been accepted as a fellow, check out the manual to see how you can prepare before arriving, what orientation and training will cover, and what to expect from the summer. If you are interested in learning at home, check out the tutorials and teach-outs developed by our staff and fellows throughout the summer, and to suggest or contribute additional resources. *Another one of our goals is to encourage collaborations. Anyone interested in doing this type of work, or starting a DSSG program, to build on what we've learned by using and contributing to these resources.","title":"Who is this guide for?"},{"location":"#what-is-in-this-guide","text":"We have spent many (sort of) early mornings waxing existential over Dunkin' Donuts while trying to define what makes a \"data scientist for social good,\" that enigmatic breed combining one part data scientist, one part consultant, one part educator, and one part bleeding heart idealist. We've come to a rough working definition in the form of the skills and knowledge one would need, which we categorize as follows: Programming, because you'll need to tell your computer what to do, usually by writing code. Computer science, because you'll need to understand how your data is - and should be - structured, as well as the algorithms you use to analyze it. Math and stats, because everything else in life is just applied math , and numerical results are meaningless without some measure of uncertainty. Machine learning, because you'll want to build predictive or descriptive models that can learn, evolve, and improve over time. Social science, because you'll need to know how to design experiments to validate your models in the field, and to understand when correlation can plausibly suggest causation, and sometimes even do causal inference. Problem and Project Scoping, because you'll need to be able to go from a vague and fuzzy project description to a problem you can solve, understand the goals of the project, the interventions you are informing, the data you have and need, and the analysis that needs to be done. Project management, to make progress as a team, to work effectively with your project partner, and work with a team to make that useful solution actually happen. Privacy and security, because data is people and needs to be kept secure and confidential. Ethics, fairness, bias, and transparency, because your work has the potential to be misused or have a negative impact on people's lives, so you have to consider the biases in your data and analyses, the ethical and fairness implications, and how to make your work interpretable and transparent to the users and to the people impacted by it. Communications, because you'll need to be able to tell the story of why what you're doing matters and the methods you're using to a broad audience. Social issues, because you're doing this work to help people, and you don't live or work in a vacuum, so you need to understand the context and history surrounding the people, places and issues you want to impact. All material is licensed under CC-BY 4.0","title":"What is in this guide?"},{"location":"host-cities/","text":"Host cities # Year Host City Country 2013 Chicago USA 2014 Chicago USA 2015 Chicago USA 2016 Chicago USA 2017 Lisbon Portugal 2018 Lisbon Portugal 2018 Chicago USA 2019 Warwick UK 2019 London UK","title":"DSSG Locations"},{"location":"host-cities/#host-cities","text":"Year Host City Country 2013 Chicago USA 2014 Chicago USA 2015 Chicago USA 2016 Chicago USA 2017 Lisbon Portugal 2018 Lisbon Portugal 2018 Chicago USA 2019 Warwick UK 2019 London UK","title":"Host cities"},{"location":"what-is-dssg/","text":"We have spent many (sort of) early mornings waxing existential over Dunkin' Donuts while trying to define what makes a \"data scientist for social good,\" that enigmatic breed combining one part data scientist, one part consultant, one part educator, and one part bleeding heart idealist. We've come to a rough working definition in the form of the skills and knowledge one would need, which we categorize as follows: Programming, because you'll need to tell your computer what to do, usually by writing code. Computer science, because you'll need to understand how your data is - and should be - structured, as well as the algorithms you use to analyze it. Math and stats, because everything else in life is just applied math , and numerical results are meaningless without some measure of uncertainty. Machine learning, because you'll want to build predictive or descriptive models that can learn, evolve, and improve over time. Social science, because you'll need to know how to design experiments to validate your models in the field, and to understand when correlation can plausibly suggest causation, and sometimes even do causal inference. Problem and Project Scoping, because you'll need to be able to go from a vague and fuzzy project description to a problem you can solve, understand the goals of the project, the interventions you are informing, the data you have and need, and the analysis that needs to be done. Project management, to make progress as a team, to work effectively with your project partner, and work with a team to make that useful solution actually happen. Privacy and security, because data is people and needs to be kept secure and confidential. Ethics, fairness, bias, and transparency, because your work has the potential to be misused or have a negative impact on people's lives, so you have to consider the biases in your data and analyses, the ethical and fairness implications, and how to make your work interpretable and transparent to the users and to the people impacted by it. Communications, because you'll need to be able to tell the story of why what you're doing matters and the methods you're using to a broad audience. Social issues, because you're doing this work to help people, and you don't live or work in a vacuum, so you need to understand the context and history surrounding the people, places and issues you want to impact.","title":"What is in this curriculum?"},{"location":"curriculum/","text":"Before You Start # So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and Software Setup and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you decide on a project, or refine a project you already have in mind. Next, check out Pipelines and Project Workflow for an overview of how the steps of your project (and, therefore, your code) will be organized. Getting and Keeping Data # Data comes in many forms, from many sources - you may get a database dump or CSV files directly from a project partner , or you may need to scrape data from the web . Either way, once you've got your hands on some data, you'll need to bring it into a database , and start cleaning and \"wrangling\" it. You'll definitely want to keep track of the steps to take your data from its original, raw form to being model-ready, so check out Reproducible ETL . Command line tools will start to come in handy here. Finally, data science for social good projects often involve sensitive data about real humans, which is what makes this work both interesting and important, but also makes it extra important to keep security in mind, so make sure to check out the Data Security Primer . Data Exploration and Analysis # Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data . Modeling and Machine Learning # Now you're ready to make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through. Programming Best Practices # As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in. Presentations and Communications # Remember that there's no point to doing data science for social good work if no one understands what you did and can put it to good use. You'll need to be able to communicate your work: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable. How to Contribute # We welcome contributions and collaboration! If you find a broken link, or have a request or suggestion, please submit an issue. If you have materials to contribute, please submit a pull request. New tutorials should follow our tutorial template , and keep in mind the teaching philosophy outlined below. Teaching Philosophy # Our guiding teaching philosophy is as follows: - You get out what you put in. Fellows are encouraged to take an active role in teaching and shaping the curriculum, as well as learning from it. Learning also takes initiative and participation on the student side. - Clearly motivate topics and tools. For more technical topics: what actual task that a data scientist does will require this tool? What other options are there to do similar tasks? What are pitfalls, and what will it look like when something goes wrong? What are common mistakes or bugs? For conceptual topics: Why do we feel the need to communicate this topic? What are some concrete examples of where it's been done well or poorly in the past? - Lessons should be user friendly. Lectures should be concise - 45 minutes at the outside - and materials should be practical. Slides should be accompanied by a worksheet or exercises so fellows can follow along and learn by doing, and a cheat sheet with relevant commands or code snippets should be included where possible.","title":"What is in this curriculum"},{"location":"curriculum/#before-you-start","text":"So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and Software Setup and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you decide on a project, or refine a project you already have in mind. Next, check out Pipelines and Project Workflow for an overview of how the steps of your project (and, therefore, your code) will be organized.","title":"Before You Start"},{"location":"curriculum/#getting-and-keeping-data","text":"Data comes in many forms, from many sources - you may get a database dump or CSV files directly from a project partner , or you may need to scrape data from the web . Either way, once you've got your hands on some data, you'll need to bring it into a database , and start cleaning and \"wrangling\" it. You'll definitely want to keep track of the steps to take your data from its original, raw form to being model-ready, so check out Reproducible ETL . Command line tools will start to come in handy here. Finally, data science for social good projects often involve sensitive data about real humans, which is what makes this work both interesting and important, but also makes it extra important to keep security in mind, so make sure to check out the Data Security Primer .","title":"Getting and Keeping Data"},{"location":"curriculum/#data-exploration-and-analysis","text":"Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data .","title":"Data Exploration and Analysis"},{"location":"curriculum/#modeling-and-machine-learning","text":"Now you're ready to make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through.","title":"Modeling and Machine Learning"},{"location":"curriculum/#programming-best-practices","text":"As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in.","title":"Programming Best Practices"},{"location":"curriculum/#presentations-and-communications","text":"Remember that there's no point to doing data science for social good work if no one understands what you did and can put it to good use. You'll need to be able to communicate your work: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable.","title":"Presentations and Communications"},{"location":"curriculum/#how-to-contribute","text":"We welcome contributions and collaboration! If you find a broken link, or have a request or suggestion, please submit an issue. If you have materials to contribute, please submit a pull request. New tutorials should follow our tutorial template , and keep in mind the teaching philosophy outlined below.","title":"How to Contribute"},{"location":"curriculum/#teaching-philosophy","text":"Our guiding teaching philosophy is as follows: - You get out what you put in. Fellows are encouraged to take an active role in teaching and shaping the curriculum, as well as learning from it. Learning also takes initiative and participation on the student side. - Clearly motivate topics and tools. For more technical topics: what actual task that a data scientist does will require this tool? What other options are there to do similar tasks? What are pitfalls, and what will it look like when something goes wrong? What are common mistakes or bugs? For conceptual topics: Why do we feel the need to communicate this topic? What are some concrete examples of where it's been done well or poorly in the past? - Lessons should be user friendly. Lectures should be concise - 45 minutes at the outside - and materials should be practical. Slides should be accompanied by a worksheet or exercises so fellows can follow along and learn by doing, and a cheat sheet with relevant commands or code snippets should be included where possible.","title":"Teaching Philosophy"},{"location":"curriculum/sample/","text":"monday tuesday wednesday thursday friday every week team updates deep dives code review external talk ethics discussions week 1 software installation and check logins scoping how does a project go over the summer technical environment setup, cmd line, workflow git dealing with partners python for data analysis pipelines communications for the summer dbs and sql week 2 data maturity project deliverables ethics overview more dbs and ETL working in a team good software practices no deep dive - partner session data exploration two sessions - viz, pandas, sql, spatial week 3 policy problem templates ml overview - formulation and validation intro to social sciences sql record linkage ML overview - validation case study from previous dssg week 4 ml overview - methods non-technical session causal inference - observational Features ml overview - methods user interfaces and usability week 5 text analysis ML pipelines deeper Features workshop optimization week 6 validation model interpretation communications - speaking audition and overview of postmodeling bias and fairness week 7 post-modeling causal inference - experiments case study post-modeling week 8 recap of what needs to be done experimental design TBD technical session - TBD week 9 recap of what needs to be done social science methods TBD technical session - TBD week 10 bias and fairness communications - writing image/video analysis network analysis week 11 week 12","title":"Sample curriculum for Summer 2019"},{"location":"curriculum/0_before_you_start/","text":"Before You Start # So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and Software Setup and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you decide on a project or refine a project you already have in mind. Then, check out Pipelines and Project Workflow for an overview of how the steps of your project (and, therefore, your code) will be organized. Don\u2019t forget to keep close to your hearth the best practices for your day to day workflow .","title":"Before you start"},{"location":"curriculum/0_before_you_start/#before-you-start","text":"So you want to start a DSSG project! First, please make sure you have gone through the Prerequisites and Software Setup and are equipped with the basic knowledge and tools you'll need. Then use our Project Scoping Intro to help you decide on a project or refine a project you already have in mind. Then, check out Pipelines and Project Workflow for an overview of how the steps of your project (and, therefore, your code) will be organized. Don\u2019t forget to keep close to your hearth the best practices for your day to day workflow .","title":"Before You Start"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/","text":"Technical Workflow and Best Practices # This tutorial is designed to help you understand how to get started with setting up your computing environment, how to decide what to use your local laptop/desktop for, what to do on the server (and how), and how to go back and forth between different environments and tools on your laptop, the server, and your remote database (an other data resources). We assume a GNU/linux (Ubuntu) server that's been set up for you, and access to a database (PostgreSQL). 1. What should you have on your laptop? # You should have the following tools installed on your local machine (whether it's a MacOS, windows, or GNU/Linux) that you will use primarily locally: ssh (to connect to the server) psql (to connect to the database through command line) dbeaver (or dbvisualizer ) to connect to the database through a GUI git client (to work with github repositories) Tableau GNU/Emacs, Vi, sublime or atom (text editor to edit code locally) python , jupyter and other coding tools are helpful but you will be primarily using them on the server and not on your laptop 2. What should you set up on the server? # Decide which shell you're using. You have bash by default, but many of us like zsh . Set up dotfiles. you can clone this repo with Adolfo's dotfiles Danger You should never blindly copy lines to your dotfiles that you don't understand. Check the files in dotfiles repository and adapt/adopt what suits your needs and tastes Configure git Decide on your editor (vim or GNU/Emacs). For vim users Get a good .vimrc file to make life easier for yourself if you choose vim. See for example this If you prefer GNU/Emacs There are several options and depends in your taste, but Emacs prelude is a good start Create a file with your database credentials ( sample file ) or ( recommended ) setup a .pg_service.conf Learn about virtual environments and set one up (if it hasn't been set up for you). Learn how to install new python packages through pip install 3. Workflow: How should you work day to day with your laptop and the remote server? # screen / tmux : When you log in to your remote machine, run screen or tmux and work from a screen/tmux session (Optional) When using the database for any reason from your laptop (to connect with tableau or dbeaver or for any other application), open an ssh tunnel from your local machine to the remote server . Windows See here for instructions MacOS, GNU/Linux As a reminder of another section : ssh -N -L localhost:8888:localhost:8888 username@[projectname].dssg.io Writing and Running Code If you're using your laptop (sublime, atom, or some other editor) to edit code, use git to commit nad push to the repo and then do a git pull on the server to get your code there. If you're writing code on the server directly, you should use vim or GNU/Emacs. git commit often. Every time you finish a chunk of work, do a git commit. git push when you've tested it and it is doing what you intended for it to do. Do not push code to master if it breaks. You will annoy your teammates :) Later in the summer, we'll talk more about how to create git branches. Every time you resume working, do a git pull to get the latest version of the code. If you need to copy files from your laptop to server, use scp . Danger Other way around, i.e. from the server to your laptop , DON'T! All the data needs to stay on the remote server. If you're writing (or running) your code in jupyter notebooks, then you should: create a no-browser jupyter session on the server jupyter notebook --no-browser --port=8889 You may need to chage the port number to avoid conflicts with other teammates using the same port. On your local machine, create an SSH tunnel that forwards the port for Jupyter Notebook ( 8889 in the above command) on the remote machine to a port on the local machine (also 8888 above) so that we can access it using our local browser. ssh -N -L localhost:8888:localhost:8889 username@projectname.dssg.io Access the remote jupyter server via your local browser. Open your browser and go to http://0.0.0.0:8888 you may need to copy and paste the longer URL with a token that is generated when you run the command in step 1) that looks like http://localhost:8889/?token=343vdfvdfggdfgfdt345&token=fdsfdf345353vc See More detailed instructions 4. Other Workflow Considerations # When should you use Jupyter notebooks, versus when you should use .py files to write code When to use psql versus DBeaver When to use SQL versus when to use Python and/or Pandas 5. Other Tips # Tunneling to the DB for Tableau (or another app like QGIS): ssh -L 5433:databaseservername:5432 username@projectservername","title":"Technical Workflow and Best Practices"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#technical-workflow-and-best-practices","text":"This tutorial is designed to help you understand how to get started with setting up your computing environment, how to decide what to use your local laptop/desktop for, what to do on the server (and how), and how to go back and forth between different environments and tools on your laptop, the server, and your remote database (an other data resources). We assume a GNU/linux (Ubuntu) server that's been set up for you, and access to a database (PostgreSQL).","title":"Technical Workflow and Best Practices"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#1-what-should-you-have-on-your-laptop","text":"You should have the following tools installed on your local machine (whether it's a MacOS, windows, or GNU/Linux) that you will use primarily locally: ssh (to connect to the server) psql (to connect to the database through command line) dbeaver (or dbvisualizer ) to connect to the database through a GUI git client (to work with github repositories) Tableau GNU/Emacs, Vi, sublime or atom (text editor to edit code locally) python , jupyter and other coding tools are helpful but you will be primarily using them on the server and not on your laptop","title":"1. What should you have on your laptop?"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#2-what-should-you-set-up-on-the-server","text":"Decide which shell you're using. You have bash by default, but many of us like zsh . Set up dotfiles. you can clone this repo with Adolfo's dotfiles Danger You should never blindly copy lines to your dotfiles that you don't understand. Check the files in dotfiles repository and adapt/adopt what suits your needs and tastes Configure git Decide on your editor (vim or GNU/Emacs). For vim users Get a good .vimrc file to make life easier for yourself if you choose vim. See for example this If you prefer GNU/Emacs There are several options and depends in your taste, but Emacs prelude is a good start Create a file with your database credentials ( sample file ) or ( recommended ) setup a .pg_service.conf Learn about virtual environments and set one up (if it hasn't been set up for you). Learn how to install new python packages through pip install","title":"2. What should you set up on the server?"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#3-workflow-how-should-you-work-day-to-day-with-your-laptop-and-the-remote-server","text":"screen / tmux : When you log in to your remote machine, run screen or tmux and work from a screen/tmux session (Optional) When using the database for any reason from your laptop (to connect with tableau or dbeaver or for any other application), open an ssh tunnel from your local machine to the remote server . Windows See here for instructions MacOS, GNU/Linux As a reminder of another section : ssh -N -L localhost:8888:localhost:8888 username@[projectname].dssg.io Writing and Running Code If you're using your laptop (sublime, atom, or some other editor) to edit code, use git to commit nad push to the repo and then do a git pull on the server to get your code there. If you're writing code on the server directly, you should use vim or GNU/Emacs. git commit often. Every time you finish a chunk of work, do a git commit. git push when you've tested it and it is doing what you intended for it to do. Do not push code to master if it breaks. You will annoy your teammates :) Later in the summer, we'll talk more about how to create git branches. Every time you resume working, do a git pull to get the latest version of the code. If you need to copy files from your laptop to server, use scp . Danger Other way around, i.e. from the server to your laptop , DON'T! All the data needs to stay on the remote server. If you're writing (or running) your code in jupyter notebooks, then you should: create a no-browser jupyter session on the server jupyter notebook --no-browser --port=8889 You may need to chage the port number to avoid conflicts with other teammates using the same port. On your local machine, create an SSH tunnel that forwards the port for Jupyter Notebook ( 8889 in the above command) on the remote machine to a port on the local machine (also 8888 above) so that we can access it using our local browser. ssh -N -L localhost:8888:localhost:8889 username@projectname.dssg.io Access the remote jupyter server via your local browser. Open your browser and go to http://0.0.0.0:8888 you may need to copy and paste the longer URL with a token that is generated when you run the command in step 1) that looks like http://localhost:8889/?token=343vdfvdfggdfgfdt345&token=fdsfdf345353vc See More detailed instructions","title":"3. Workflow: How should you work day to day with your laptop and the remote server?"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#4-other-workflow-considerations","text":"When should you use Jupyter notebooks, versus when you should use .py files to write code When to use psql versus DBeaver When to use SQL versus when to use Python and/or Pandas","title":"4. Other Workflow Considerations"},{"location":"curriculum/0_before_you_start/TechnicalWorkflowAndBestPractices/#5-other-tips","text":"Tunneling to the DB for Tableau (or another app like QGIS): ssh -L 5433:databaseservername:5432 username@projectservername","title":"5. Other Tips"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/","text":"Pipelines and Project Workflow # Motivation # Most data-science projects have the same set of tasks: ETL : extracting data from its source, transforming it, then loading it into a database. Pre-process data : This might include imputing missing values and choosing the training and testing sets. Train the model(s) : You can try different algorithms, features, and so on. Assess performance on the test set: Using an appropriate metric (e.g. Precision@k Precision@k , Recall Recall , AUC AUC ), examine the performance of your model \"out of sample.\" Think of new things to try. Repeat steps 1 through 4 as appropriate. Often, by the time you build a couple dozen models, you're struggling to remember the details of each. What features did you use for each? What training and testing split? What hyperparameters? Your code might be getting messy too. Did you overwrite the code for the previous model? Maybe you copied, pasted, and edited code from an earlier model. Can you still read what's there? It can quickly become a hodgepodge that requires heroics to decipher. In this session, we will introduce the data pipeline, an approach that helps you simplify the modeling process. Concepts # A data pipeline is a set of code that handles all the computational tasks your project needs from beginning to end. The typical data pipeline is a set of functions strung together. Here's a simple example using scikit-learn 's boston dataset: This pipeline has two steps. The first, which I call \"preprocessing,\" prepares the data for modeling by creating training and testing splits. The second, which I call \"models, predictions, and metrics,\" uses the preprocessed data to train models, make predictions, and print R^2 R^2 on the test set. The pipeline takes inputs (e.g. data, training/testing proportions, and model types) at one end and produces outputs ( accuracy ) at the other end. Obviously, this analysis is incomplete, but the pipeline is a good start. Because we use the same code and data, we can run the pipeline from beginning to end and get the same results. And because we split the pipeline into functions, we can identify where the pipeline goes wrong and improve the pipeline one function at a time. (Each function just needs to use the same inputs and outputs as before.) Also note the function and loops in the second part of the pipeline. We're somewhat agnostic about the methods we use. If it works, great! This structure lets us loop through many types of models using the same preprocessed data and the same predictions and metrics. It makes adding new methods and comparing the results easier, and it helps us focus on other parts of the pipeline, such as feature generation. Aren't pipelines super duper? Our projects are far more complex than this Boston example, and our pipelines reflect that. Here's what a typical DSSG pipeline looks like: The police pipeline , started at DSSG 2015 , is an example of a relatively well developed pipeline. It lets us specify the pipeline options we want in a yaml file, from preprocessing on. (The code in this repository does not include ETL.) It gives us many modeling options, and it makes comparisons easy. Remember ETL stands for Extract , Transform and Load Your Pipeline and the DSSG Schedule # Much of your work will revolve around and within your pipeline, but we have identified specific aspects for you to focus on each week : Resources # Our lead pipeline , started at DSSG 2014 Our Cincinnati pipeline , started at DSSG 2015 Triage (a generalized DSSG pipeline)","title":"Pipelines and Project Workflow"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#pipelines-and-project-workflow","text":"","title":"Pipelines and Project Workflow"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#motivation","text":"Most data-science projects have the same set of tasks: ETL : extracting data from its source, transforming it, then loading it into a database. Pre-process data : This might include imputing missing values and choosing the training and testing sets. Train the model(s) : You can try different algorithms, features, and so on. Assess performance on the test set: Using an appropriate metric (e.g. Precision@k Precision@k , Recall Recall , AUC AUC ), examine the performance of your model \"out of sample.\" Think of new things to try. Repeat steps 1 through 4 as appropriate. Often, by the time you build a couple dozen models, you're struggling to remember the details of each. What features did you use for each? What training and testing split? What hyperparameters? Your code might be getting messy too. Did you overwrite the code for the previous model? Maybe you copied, pasted, and edited code from an earlier model. Can you still read what's there? It can quickly become a hodgepodge that requires heroics to decipher. In this session, we will introduce the data pipeline, an approach that helps you simplify the modeling process.","title":"Motivation"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#concepts","text":"A data pipeline is a set of code that handles all the computational tasks your project needs from beginning to end. The typical data pipeline is a set of functions strung together. Here's a simple example using scikit-learn 's boston dataset: This pipeline has two steps. The first, which I call \"preprocessing,\" prepares the data for modeling by creating training and testing splits. The second, which I call \"models, predictions, and metrics,\" uses the preprocessed data to train models, make predictions, and print R^2 R^2 on the test set. The pipeline takes inputs (e.g. data, training/testing proportions, and model types) at one end and produces outputs ( accuracy ) at the other end. Obviously, this analysis is incomplete, but the pipeline is a good start. Because we use the same code and data, we can run the pipeline from beginning to end and get the same results. And because we split the pipeline into functions, we can identify where the pipeline goes wrong and improve the pipeline one function at a time. (Each function just needs to use the same inputs and outputs as before.) Also note the function and loops in the second part of the pipeline. We're somewhat agnostic about the methods we use. If it works, great! This structure lets us loop through many types of models using the same preprocessed data and the same predictions and metrics. It makes adding new methods and comparing the results easier, and it helps us focus on other parts of the pipeline, such as feature generation. Aren't pipelines super duper? Our projects are far more complex than this Boston example, and our pipelines reflect that. Here's what a typical DSSG pipeline looks like: The police pipeline , started at DSSG 2015 , is an example of a relatively well developed pipeline. It lets us specify the pipeline options we want in a yaml file, from preprocessing on. (The code in this repository does not include ETL.) It gives us many modeling options, and it makes comparisons easy. Remember ETL stands for Extract , Transform and Load","title":"Concepts"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#your-pipeline-and-the-dssg-schedule","text":"Much of your work will revolve around and within your pipeline, but we have identified specific aspects for you to focus on each week :","title":"Your Pipeline and the DSSG Schedule"},{"location":"curriculum/0_before_you_start/pipelines-and-project-workflow/#resources","text":"Our lead pipeline , started at DSSG 2014 Our Cincinnati pipeline , started at DSSG 2015 Triage (a generalized DSSG pipeline)","title":"Resources"},{"location":"curriculum/0_before_you_start/prerequisites/","text":"Before the Summer: Pre-requisites # All of our fellows come in with at least an intermediate programming ability (usually in Python), some statistics knowledge, and some social science knowledge. We bring together people of all backgrounds, so we know that some will be stronger in some areas, and some might never have seen some of this material. To get everyone on the same page at the beginning, we send out a welcome email about a month before the fellowship begins with recommended background reading and what they'll need to install. We also spend a significant part of the first two weeks of the summer getting everyone up to speed with the tools they'll be using in their projects. In order to be ready for the summer, you need to install some packages on your computer: Required # SSH (PuTTY for Windows) Git (for version control) psql (PostgreSQL command line interface) Python tools Python 3.6 Anaconda/Miniconda or pyenv + virtualenv Python Packages pandas matplotlib scikit-learn psycopg2 ipython jupyter Highly Recommended # DBeaver (GUI to access various databases) Tableau (students can request a free education license) GNU/Emacs, VIM, Sublime Text (text editor for coding) How to install pre-requisites? # OS X users - Follow these instructions Linux users - You probably know how to do it, but still check this for information on Python tools Windows users - We don't have a guide yet (any volunteers?) We will also hold a software setup session during the first week with technical mentors there to help anyone still having difficulty. Try it out! # You should give all installed software a quick spin to check that it did install. For your python packages, try to import them. Type python in your shell, and then once you are in your python session, try for example import pandas , import matplotlib , and so on. (You can quit with exit() .) Also try ipython and jupyter notebook in your terminal, and see if you get any errors. Similarly, try psql in your terminal; it should reply psql: could not connect to server: No such file or directory ssh should print a 'helpful' message, and R should drop you into an R session that you can quit with q() . SSH Key Setup # You need to generate a SSH key pair. To do this, follow the instructions on GitHub , namely 'Generating a new SSH key' and 'Adding your SSH key to ssh-agent'. Windows users probably want to use git bash or PuTTYgen (if you're on Linux or OS X, your standard terminal should be the bash shell you need). The steps in 'Generating a new SSH key' create two new files (by default in ~/.ssh/ : One without a file extension (by default, it's called id_rsa ), and one with the extension .pub . The latter one is your _pub_lic key, which you will share with your project server, so that it can recognize you; the former is your private key, which you must not share with anybody, as it will let you access your project server. After having generated the key pair, you should set the correct file permissions for your private key: SSH requires that only you, the owner, are able to read/write it, and will give you an error otherwise. You can set the right permissions with this command: chmod 600 ~/.ssh/nameofyourprivatekey (where you'll have to substitute in the path and name of your private key that you chose during key generation). Asking for help # We just started this repo but we want the issues section to be a knowledge base for common problems. If you have any trouble installing anything check closed issues. If you don't find the answer, feel free to open an issue and someone will help you. To open issues, you need to create a Github account (you'll need it for the summer anyway).","title":"Before the Summer: Pre-requisites"},{"location":"curriculum/0_before_you_start/prerequisites/#before-the-summer-pre-requisites","text":"All of our fellows come in with at least an intermediate programming ability (usually in Python), some statistics knowledge, and some social science knowledge. We bring together people of all backgrounds, so we know that some will be stronger in some areas, and some might never have seen some of this material. To get everyone on the same page at the beginning, we send out a welcome email about a month before the fellowship begins with recommended background reading and what they'll need to install. We also spend a significant part of the first two weeks of the summer getting everyone up to speed with the tools they'll be using in their projects. In order to be ready for the summer, you need to install some packages on your computer:","title":"Before the Summer: Pre-requisites"},{"location":"curriculum/0_before_you_start/prerequisites/#required","text":"SSH (PuTTY for Windows) Git (for version control) psql (PostgreSQL command line interface) Python tools Python 3.6 Anaconda/Miniconda or pyenv + virtualenv Python Packages pandas matplotlib scikit-learn psycopg2 ipython jupyter","title":"Required"},{"location":"curriculum/0_before_you_start/prerequisites/#highly-recommended","text":"DBeaver (GUI to access various databases) Tableau (students can request a free education license) GNU/Emacs, VIM, Sublime Text (text editor for coding)","title":"Highly Recommended"},{"location":"curriculum/0_before_you_start/prerequisites/#how-to-install-pre-requisites","text":"OS X users - Follow these instructions Linux users - You probably know how to do it, but still check this for information on Python tools Windows users - We don't have a guide yet (any volunteers?) We will also hold a software setup session during the first week with technical mentors there to help anyone still having difficulty.","title":"How to install pre-requisites?"},{"location":"curriculum/0_before_you_start/prerequisites/#try-it-out","text":"You should give all installed software a quick spin to check that it did install. For your python packages, try to import them. Type python in your shell, and then once you are in your python session, try for example import pandas , import matplotlib , and so on. (You can quit with exit() .) Also try ipython and jupyter notebook in your terminal, and see if you get any errors. Similarly, try psql in your terminal; it should reply psql: could not connect to server: No such file or directory ssh should print a 'helpful' message, and R should drop you into an R session that you can quit with q() .","title":"Try it out!"},{"location":"curriculum/0_before_you_start/prerequisites/#ssh-key-setup","text":"You need to generate a SSH key pair. To do this, follow the instructions on GitHub , namely 'Generating a new SSH key' and 'Adding your SSH key to ssh-agent'. Windows users probably want to use git bash or PuTTYgen (if you're on Linux or OS X, your standard terminal should be the bash shell you need). The steps in 'Generating a new SSH key' create two new files (by default in ~/.ssh/ : One without a file extension (by default, it's called id_rsa ), and one with the extension .pub . The latter one is your _pub_lic key, which you will share with your project server, so that it can recognize you; the former is your private key, which you must not share with anybody, as it will let you access your project server. After having generated the key pair, you should set the correct file permissions for your private key: SSH requires that only you, the owner, are able to read/write it, and will give you an error otherwise. You can set the right permissions with this command: chmod 600 ~/.ssh/nameofyourprivatekey (where you'll have to substitute in the path and name of your private key that you chose during key generation).","title":"SSH Key Setup"},{"location":"curriculum/0_before_you_start/prerequisites/#asking-for-help","text":"We just started this repo but we want the issues section to be a knowledge base for common problems. If you have any trouble installing anything check closed issues. If you don't find the answer, feel free to open an issue and someone will help you. To open issues, you need to create a Github account (you'll need it for the summer anyway).","title":"Asking for help"},{"location":"curriculum/0_before_you_start/prerequisites/2016_welcome_email/","text":"Hey DSSG 2016 Fellows! Jane Zanzig, Benedict Kuester, and Eduardo Blancas Reyes reporting. We\u2019re SUPER stoked that DSSG 2016 is less than a month away. A lot of you have expressed both excitement about the fellowship and trepidation about being prepared. We are here to help! Here is a guide of everything you need to install before showing up in Chicago, on your machine as well as in your brain. We expect everyone should at least be familiar with Python (or R) to an intermediate level, and have some knowledge of data analysis, computers, and stats. However, everyone comes from different backgrounds. Make sure you have everything on the Things to Install list installed, look through the Explanations & Tutorials section if you don't know why/how to use any of these tools, and check out the Background Reading section to brush up on quantitative social science or machine learning (or both!) and you\u2019ll be ready to roll. Cry out for help on Slack if you are having trouble! And get used to it, you\u2019ll be doing a lot of it over the summer (the Slacking, not the crying). Things to Install # SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pip + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter R DBeaver (to query databases) Tableau (get the free license for students) Sublime Text (if you need a decent editor) If you are an OS X user, we highly recommend to install Homebrew to make software installation easier. For more information on requirements, see this . A guide to install pre-requirements on OS X is available here . Explanations & Tutorials # Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). - General command line navigation - Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . - grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at the Fellowship. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). - Python Package & Environment Management: To properly create a Python environment, we recommend you use Anaconda or Miniconda . If you feel adventurous, feel free to use pip + virtualenv. - Python Programming Resources: - Writing efficient Python - Tips for Idiomatic Python - Introduction to Python debugging tools - Jupyter (formerly known as IPython) - Example of how IPython notebooks are used in data science - Tutorial for setting up and opening IPython notebook - Amazing examples of IPython notebooks R R is an open-source programming language for statistical analysis, with lots of great packages for modeling and visualization. - RStudio (IDE made for R) - Shiny - Shiny is a web framework for R, so that you can build interactive visualizations and web widgets. You can use this to prototype tools for project partners to visualize and understand your analyses. Databases We typically use postgres for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. - Installing Git - Complete Beginner\u2019s Guide to Git and Github - 10-minute Hello World Tutorial to using Git and Github - Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: Unless you prefer to program using vim/emacs, we suggest you install a general purpose text editor, such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license. Background Reading # General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science","title":"2016 welcome email"},{"location":"curriculum/0_before_you_start/prerequisites/2016_welcome_email/#things-to-install","text":"SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pip + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter R DBeaver (to query databases) Tableau (get the free license for students) Sublime Text (if you need a decent editor) If you are an OS X user, we highly recommend to install Homebrew to make software installation easier. For more information on requirements, see this . A guide to install pre-requirements on OS X is available here .","title":"Things to Install"},{"location":"curriculum/0_before_you_start/prerequisites/2016_welcome_email/#explanations-tutorials","text":"Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). - General command line navigation - Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . - grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at the Fellowship. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). - Python Package & Environment Management: To properly create a Python environment, we recommend you use Anaconda or Miniconda . If you feel adventurous, feel free to use pip + virtualenv. - Python Programming Resources: - Writing efficient Python - Tips for Idiomatic Python - Introduction to Python debugging tools - Jupyter (formerly known as IPython) - Example of how IPython notebooks are used in data science - Tutorial for setting up and opening IPython notebook - Amazing examples of IPython notebooks R R is an open-source programming language for statistical analysis, with lots of great packages for modeling and visualization. - RStudio (IDE made for R) - Shiny - Shiny is a web framework for R, so that you can build interactive visualizations and web widgets. You can use this to prototype tools for project partners to visualize and understand your analyses. Databases We typically use postgres for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. - Installing Git - Complete Beginner\u2019s Guide to Git and Github - 10-minute Hello World Tutorial to using Git and Github - Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: Unless you prefer to program using vim/emacs, we suggest you install a general purpose text editor, such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license.","title":"Explanations &amp; Tutorials"},{"location":"curriculum/0_before_you_start/prerequisites/2016_welcome_email/#background-reading","text":"General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science","title":"Background Reading"},{"location":"curriculum/0_before_you_start/prerequisites/email/","text":"Hey DSSG 2019 Fellows! We\u2019re excited that DSSG 2019 is comning up soon! A lot of you have expressed both excitement about the fellowship and trepidation about being prepared. We are here to help! Here is a guide of everything you need to install before showing up in Chicago, on your machine as well as in your brain. We expect everyone should at least be familiar with Python to an intermediate level, and have some knowledge of data analysis, computers, and stats. However, everyone comes from different backgrounds. Make sure you have everything on the Things to Install list installed, look through the Explanations & Tutorials section if you don't know why/how to use any of these tools, and check out the Background Reading section to brush up on quantitative social science or machine learning (or both!) and you\u2019ll be ready to roll. Cry out for help on Slack if you are having trouble! And get used to it, you\u2019ll be doing a lot of it over the summer (the Slacking, not the crying). Things to Install # SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pyenv + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter Seaborn R (you won't need it but it may make some of you feel better) DBeaver (to query databases) Tableau (get the free license for students) GNU/Emacs, VIM or \u2026 Sublime Text (if you need a decent editor) If you are an OS X user, we recommend to install Homebrew to make software installation easier. A guide to install pre-requirements on OS X is available here . Explanations & Tutorials # Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). General command line navigation Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at DSSG. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). Python Package & Environment Management: To properly create a Python environment, we recommend you use [Anaconda] 1 or [Miniconda] 2 . If you feel adventurous, feel free to use pip + virtualenv. Python Programming Resources: [Writing efficient Python] 3 [Tips for Idiomatic Python] 4 [Introduction to Python debugging tools] 5 Jupyter Notebooks and Jupyter Lab [Beginner's Guide to Jupyter Lab] 9 [Example of how IPython notebooks are used in data science] 6 [Tutorial for setting up and opening IPython notebook] 7 [Amazing examples of IPython notebooks] 8 Databases We typically use Postgres (full name PostgreSQL) for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. Installing Git Complete Beginner\u2019s Guide to Git and Github 10-minute Hello World Tutorial to using Git and Github Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: If for some weird reason 10 you don\u2019t use GNU/Emacs or VIM , we suggest you install text editor such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license. Background Reading # General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Machine Learning book chapter from Big Data and Social Science Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science https://www.continuum.io/downloads \u21a9 http://conda.pydata.org/miniconda.html \u21a9 https://www.memonic.com/user/pneff/folder/python/id/1bufp \u21a9 https://web.archive.org/web/20180411011411/http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html \u21a9 https://web.archive.org/web/20141209082719/https://blog.safaribooksonline.com/2014/11/18/intro-python-debugger/ \u21a9 http://nbviewer.ipython.org/github/jvns/talks/blob/master/pydatanyc2013/PyData%20NYC%202013%20tutorial.ipynb \u21a9 http://opentechschool.github.io/python-data-intro/core/notebook.html \u21a9 https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-and-IPython-Notebooks \u21a9 https://medium.com/@brianray_7981/jupyterlab-first-impressions-e6d70d8a175d \u21a9 Why do you do that to yourself? \u21a9","title":"Email"},{"location":"curriculum/0_before_you_start/prerequisites/email/#things-to-install","text":"SSH (PuTTY or cygwin if you\u2019re Windows) Git psql (PostgreSQL CLI) Python tools Python 3.6 Anaconda/Miniconda or pyenv + virtualenv Packages Pandas Matpotlib Scikit-learn Psycopg2 Ipython Jupyter Seaborn R (you won't need it but it may make some of you feel better) DBeaver (to query databases) Tableau (get the free license for students) GNU/Emacs, VIM or \u2026 Sublime Text (if you need a decent editor) If you are an OS X user, we recommend to install Homebrew to make software installation easier. A guide to install pre-requirements on OS X is available here .","title":"Things to Install"},{"location":"curriculum/0_before_you_start/prerequisites/email/#explanations-tutorials","text":"Command Line You will be running code and storing project data on Amazon Web Services (AWS) machines that run Linux, requiring the use of the command line. If you are on Mac or Windows, you can open up Terminal (on Mac) or download Cygwin or Putty (on Windows). General command line navigation Secure Shell (ssh) : You\u2019ll need this to connect to AWS/cloud computers. If you\u2019re on Windows, you\u2019ll need putty . grep/awk/sed : Quickly find and manipulate files, without ever leaving the command line. Python Python is the language of choice here at DSSG. If you\u2019re only going to learn one programming language, learn Python! It\u2019s powerful, expressive, and easy to read (even by non-programmers). Python Package & Environment Management: To properly create a Python environment, we recommend you use [Anaconda] 1 or [Miniconda] 2 . If you feel adventurous, feel free to use pip + virtualenv. Python Programming Resources: [Writing efficient Python] 3 [Tips for Idiomatic Python] 4 [Introduction to Python debugging tools] 5 Jupyter Notebooks and Jupyter Lab [Beginner's Guide to Jupyter Lab] 9 [Example of how IPython notebooks are used in data science] 6 [Tutorial for setting up and opening IPython notebook] 7 [Amazing examples of IPython notebooks] 8 Databases We typically use Postgres (full name PostgreSQL) for our projects (and Redshift or mongodb when necessary). That will be installed on the server but you need a client to connect to it: Dbeaver is a free database access tool that allows you to easily query different types of databases. psql is a powerful command line tool to interact with Postgres databases SQL introduction and SQL Cheatsheet Git and Github Working on code together is almost impossible without using a version control system. This summer, we\u2019ll be using Git. Our code will be stored on Github. These are fantastic tools for any software project. Installing Git Complete Beginner\u2019s Guide to Git and Github 10-minute Hello World Tutorial to using Git and Github Github\u2019s interactive web tutorial Other Useful Tools Text Editors and/or IDEs: If for some weird reason 10 you don\u2019t use GNU/Emacs or VIM , we suggest you install text editor such as Sublime Text . Tableau is a good tool to explore and visualize data without using any programming. If you\u2019re a student, you can request a free license.","title":"Explanations &amp; Tutorials"},{"location":"curriculum/0_before_you_start/prerequisites/email/#background-reading","text":"General Concepts in Machine Learning A few useful things to know about machine learning Survey of machine learning tools for social scientists Machine Learning book chapter from Big Data and Social Science Quantitative Social Science Methods Intro to Causal Inference Causal Inference in Social Science https://www.continuum.io/downloads \u21a9 http://conda.pydata.org/miniconda.html \u21a9 https://www.memonic.com/user/pneff/folder/python/id/1bufp \u21a9 https://web.archive.org/web/20180411011411/http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html \u21a9 https://web.archive.org/web/20141209082719/https://blog.safaribooksonline.com/2014/11/18/intro-python-debugger/ \u21a9 http://nbviewer.ipython.org/github/jvns/talks/blob/master/pydatanyc2013/PyData%20NYC%202013%20tutorial.ipynb \u21a9 http://opentechschool.github.io/python-data-intro/core/notebook.html \u21a9 https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-and-IPython-Notebooks \u21a9 https://medium.com/@brianray_7981/jupyterlab-first-impressions-e6d70d8a175d \u21a9 Why do you do that to yourself? \u21a9","title":"Background Reading"},{"location":"curriculum/0_before_you_start/prerequisites/osx/","text":"Installing DSSG pre-requisites on OS X # This guide will help you install all pre-requisites to be ready for the summer. Step 1. Install Homebrew # Homebrew is a package manager for OS X that will make your life a lot easier... To install, open Terminal.app and execute the following: /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" Now you can do: brew install anything Awesome! If that doesn't work, you need to add the following to your PATH: export PATH = /usr/local/bin:/usr/local/sbin: $PATH If you don't know how to do that, run this: echo 'export PATH=/usr/local/bin:/usr/local/sbin:$PATH' >> ~/.profile If still not working. Ask for help . Step 2. Install tools # Minimum pre-requisites SSH - this comes already installed with OS X Git - this comes already installed with OS X psql (PostgreSQL CLI) - brew install postgresql R - brew install r Other useful tools # Sublime Text - brew cask install sublime-text Atom - brew cask install atom Rstudio - brew cask install rstudio DBeaver - brew tap caskroom/versions; brew cask install dbeaver-community Note: applications installed via brew cask install will be available in ~/Applications Step 3. Install Python tools # For managing your python environments and packages, we recommend to use conda , but you can also use virtualenv and pip . Option A - Install Anaconda (recommended for beginners) Anaconda includes Python, conda (a package and environment manager) and a bunch of Python packages . After installing Anaconda you'll have everything to get started, but it requires ~3GB of disk space. Installation guide (Use Anaconda with Python 3.6) Documentation Once Anaconda is installed, run the following to install DSSG required Python packages: curl https://raw.githubusercontent.com/dssg/hitchhikers-guide/master/curriculum/0_before_you_start/prerequisites/requirements.txt -o dssg-requirements.txt conda install --file dssg-requirements.txt Note for advanced users: You can install anaconda using brew . Run brew cask info anaconda for details. Option B - Install Miniconda (recommended for beginnners without much space disk left) Miniconda is a light-weight version of Anaconda, it only includes Python and conda , you can later install only the Python packages that you'll need. Installation guide (Use Miniconda with Python 3.6) Documentation Once Miniconda is installed, run the following to install DSSG required Python packages: curl https://raw.githubusercontent.com/dssg/hitchhikers-guide/master/curriculum/0_before_you_start/prerequisites/requirements.txt -o dssg-requirements.txt conda install --file dssg-requirements.txt Note for advanced users: You can install miniconda using brew . Run brew cask info miniconda for details. Option C - Python manual installation + pip + virtualenv (only if you like to live dangerously) If you don't want to install Anaconda/Miniconda, you can install Python directly from homebrew and manage your packages with pip and virtual environments with virtualenv . #To install Python 3 and pip brew install python3 #To install virtual env pip install virtualenv #To install dssg required python packages curl https://raw.githubusercontent.com/dssg/hitchhikers-guide/master/curriculum/0_before_you_start/prerequisites/requirements.txt -o dssg-requirements.txt pip install -r dssg-requirements.txt Nice guide to use virtualenv . Note: The caveat of using pip directly instead of conda , is that conda will take care of external dependencies for you (some Python packages depend on non-Python packages to work). But you can install those with brew .","title":"Installing DSSG pre-requisites on OS X"},{"location":"curriculum/0_before_you_start/prerequisites/osx/#installing-dssg-pre-requisites-on-os-x","text":"This guide will help you install all pre-requisites to be ready for the summer.","title":"Installing DSSG pre-requisites on OS X"},{"location":"curriculum/0_before_you_start/prerequisites/osx/#step-1-install-homebrew","text":"Homebrew is a package manager for OS X that will make your life a lot easier... To install, open Terminal.app and execute the following: /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" Now you can do: brew install anything Awesome! If that doesn't work, you need to add the following to your PATH: export PATH = /usr/local/bin:/usr/local/sbin: $PATH If you don't know how to do that, run this: echo 'export PATH=/usr/local/bin:/usr/local/sbin:$PATH' >> ~/.profile If still not working. Ask for help .","title":"Step 1. Install Homebrew"},{"location":"curriculum/0_before_you_start/prerequisites/osx/#step-2-install-tools","text":"","title":"Step 2. Install tools"},{"location":"curriculum/0_before_you_start/prerequisites/osx/#other-useful-tools","text":"Sublime Text - brew cask install sublime-text Atom - brew cask install atom Rstudio - brew cask install rstudio DBeaver - brew tap caskroom/versions; brew cask install dbeaver-community Note: applications installed via brew cask install will be available in ~/Applications","title":"Other useful tools"},{"location":"curriculum/0_before_you_start/prerequisites/osx/#step-3-install-python-tools","text":"For managing your python environments and packages, we recommend to use conda , but you can also use virtualenv and pip .","title":"Step 3. Install Python tools"},{"location":"curriculum/0_before_you_start/software-setup/","text":"Software Setup Session # Motivation # Every team will settle on a specific setup with their tech mentors. This setup will determine, for example: which Python version to use versioning via virtual environments maintaining package dependencies continuous testing tools your version control workflow. Today, we're not doing that. We're making sure that everybody has some basic tools they will need for the tutorials and the beginning of the fellowship, and that you can log into the server and database. Getting help Work through the prerequisites below, making sure that all the software you installed works. Affix three kinds of post-it notes to your laptop: one with your operating system, e.g. Ubuntu 18.04 if you get an app working (e.g. ssh ), write its name on a green post-it and stick it to your screen if you tried - but the app failed - write its name on a red post-it If you're stuck with a step, ask a person with a corresponding green post-it (and preferrably your operating system) for help. The tech mentors will hover around and help with red stickers. You will need a few credentials for training accounts. We'll post them up front. Important The notes below aren't self-explanatory and will not cover all (or even the majority) of errors you might encounter. Make good use of the people in the room! Getting a terminal environment on a Windows Computer You're going to want to access the terminal. Unfortunately, windows computers don't have this by default (yet). Fortunately, there are a couple options for obtaining a terminal-like environment, such as the following: git-bash Cygwin If you go with cygwin, make sure to choose all git packages when you're in the package menu portion of the setup If you're a windows user, make sure to download one of these. Let's get this over with! Package Manager # A package manager will make your life easier. on Mac, install Brew Testing : To check that it installed, run the command which brew in the terminal. If it returns: /usr/local/bin/brew , it means that homebrew is installed; if it returns brew not found , it means homebrew is not installed. on GNU/Linux, you probably already have yum (RedHat based distros) or apt (Debian based distros) Testing : run yum help or apt help ask Windows users around you for their preferred way to manage packages. And tell us, so we can add them here! Git and GitHub Account # If you don't have a GitHub account, make one ! Go to this site , input your username, and click \"Add me to organization\". Your username will be automatically added to the DSSG organization on GitHub. Install git using the appropriate OS\u2019s package manager MacOS In the terminal, type: brew update brew install git GNU/Linux sudo apt update sudo apt install git Windows On Windows, you should already have git. (Either you installed git-bash , which is part of git, or you should have downloaded git in the cygwin package menu.) Test your installation. For example, create a directory, and make it a git repo: mkdir mytestdir cd mytestdir/ git init > Initialized empty Git repository in [...]/mytestdir/.git/ You can un-git the directory by deleting the .git folder: rm -r .git (or simply delete mytestdir entirely with command rmdir mytestdir ). Python # As said, your team will decide on which Python version (and versioning) to install. Thus, if you have any working setup already, don't break it (for now)! Just make sure you have the packages listed below installed. pyenv vs anaconda This is a contentious topic! For some people the way to go, because apparently is easier is Anaconda ( conda or mini-conda ), for other, for consistency and flexibility is pyenv . In reality, python library\u2019s system is a mess and in constant evolution. We will favor pyenv here, since we think is the one that allows you more flexibility and teaches you about how python works. If you are in GNU/Linux or in MacOS you have python installed. But that python is the one use by your operative system for doing stuff, probably you don\u2019t want to mess with it. So we will install a different python , for you exclusive use. First we will install some libraries 1 : MacOS # optional, but recommended: brew install openssl readline sqlite3 xz zlib GNU/Linux For Debian based distros: sudo apt-get update; sudo apt-get install --no-install-recommends make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev For Red hat based distros: dnf install make gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel libffi-devel We will install pyenv $ curl https://pyenv.run | bash And follow the instructions at the end (Mostly about adding some lines to your .bash_profile or .zsh_profile or similar.) Restart your terminal. Then, we will install a python version: $ pyenv install 3.7.3 This will take several minutes. Virtual environment # As a last step, we will create a virtual environment . $ pyenv virtualenv 3.7.3 dssg Info A virtual environment isolates the python libraries that you install in an specific project. And then assign it as the virtual environment to use in your directory of choice with $ echo dssg > .python-version Depending on your command shell ( bash , zsh , csh , etc) configuration you should get some info that the environment is in use, if not you can check it with $ pyenv version dssg (set by /home/user/projects/.python-version) Then we will install a lot of libraries. Copy the file requirements.txt to your directory and execute: $ pip install -r requirements.txt It's time to test! In order to test that both jupyter and the python packages installed appropriately, you should do the following: Download the file SoftwareSetup.ipynb into your directory. And type in the terminal: $ jupyter notebook Your browser will open a new tab and you will see something like the following: Click on SoftwareSetup.ipynb to open the notebook Follow the instructions in the notebook to run each cell. SSH / Putty # Use your username and server's address to ssh into the server: ssh yourusername@serverurl Once you enter your password, you should be dropped into a shell on the server: yourusername@servername: ~$ PRO tip Your life will be easier if you set up a .ssh/config file PSQL # The database server runs Postgres 9.5.10. For Windows users Windows users should skip the steps below, and instead use DBeaver . When setting up the connection in DBeaver, you will need to specify the SSH tunnel; the database credentials are the ones we shared with you, and the SSH tunnel credentials are the ones you used in the previous step to SSH into the training server. Alternatively, everybody can access psql from the training server: SSH into the training server as in the step before, then, on the server's shell, call psql -h POSTGRESURL -U USERNAME -d DBNAME , where you need to substitute POSTGRESURL with the postgres server's address, USERNAME with your database username, and DBNAME with the name of the database. For all non-Windows users, also do these steps to access the PostgreSQL server from your local machine. First we need to install the database client , psql MacOS Make sure you have the psql client installed; on Mac, this would be $ brew tap-pin dbcli/tap $ brew install pgcli Note, we are installing pgcli instead of psql , but apparently there is no way of install just the client without installing the whole database server. If you still want to give it a shot: $ brew postgres GNU/Linux On Debian based distros: sudo apt install postgresql-client libpq-dev Once you have the postgres client installed, you can access the training database with it. However, the database server only allows access from the training server. Thus, you need to set up an SSH tunnel through the training server to the Postgres server: $ ssh -NL localhost:8888:POSTGRESURL:5432 ec2username@EC2URL where you need to substitute POSTGRESURL , ec2username , and EC2URL with the postgres server's URL, your username on the training server, and the training server's URL respectively. Also, you should substitute 8888 with a random number in the 8000-65000 range of your choice (port 8888 might be in use already). This command forwards your laptop's port 8888 through your account on the EC2 (EC2URL) to the Postgres server port 5432. So if you access your local port 8888 in the next step, you get forwarded to the Postgres server's port 5432 - but from the Postgres server's view, the traffic is now coming from the training server (instead of your laptop), and the training server is the only IP address that is allowed to access the postgres server. Figure. A graphical representation of a ssh tunnel. Not quite our situation -they are using a MySQL db who knows why-, but it is close enough. Courtesy from this Medium post . Connect to the Postgres database on the forwarded port $ psql -h localhost -p 8888 -U USERNAME -d DBNAME where you need to replace USERNAME with the postgres [!] username, DBNAME with the name of your database, and the 8888 with the number you chose in the previous step. You then get prompted for a password. This is now the postgres server asking, so you need to reply with the corresponding password! This should drop you into a SQL shell on the database server. Note In some configurations, you'll need to explicitly assume a role to do anything beyond connecting to the database. To make changes to the training database, use the training_write role. Let's test it by creating and dropping a schema: set role training_write; create schema jsmith; drop schema jsmith; PRO tip You could save a lot of keystrokes if you setup a .pgservice.conf file and a .pgpass file in your $HOME folder. Then you could simply type $ psql service=mydb # mydb is the name of the dbservice I really prefer a GUI if you want a graphical interface to databases - you might want to use DBeaver . for an updated version of this instructions see here \u21a9","title":"Software Setup Session"},{"location":"curriculum/0_before_you_start/software-setup/#software-setup-session","text":"","title":"Software Setup Session"},{"location":"curriculum/0_before_you_start/software-setup/#motivation","text":"Every team will settle on a specific setup with their tech mentors. This setup will determine, for example: which Python version to use versioning via virtual environments maintaining package dependencies continuous testing tools your version control workflow. Today, we're not doing that. We're making sure that everybody has some basic tools they will need for the tutorials and the beginning of the fellowship, and that you can log into the server and database. Getting help Work through the prerequisites below, making sure that all the software you installed works. Affix three kinds of post-it notes to your laptop: one with your operating system, e.g. Ubuntu 18.04 if you get an app working (e.g. ssh ), write its name on a green post-it and stick it to your screen if you tried - but the app failed - write its name on a red post-it If you're stuck with a step, ask a person with a corresponding green post-it (and preferrably your operating system) for help. The tech mentors will hover around and help with red stickers. You will need a few credentials for training accounts. We'll post them up front. Important The notes below aren't self-explanatory and will not cover all (or even the majority) of errors you might encounter. Make good use of the people in the room! Getting a terminal environment on a Windows Computer You're going to want to access the terminal. Unfortunately, windows computers don't have this by default (yet). Fortunately, there are a couple options for obtaining a terminal-like environment, such as the following: git-bash Cygwin If you go with cygwin, make sure to choose all git packages when you're in the package menu portion of the setup If you're a windows user, make sure to download one of these. Let's get this over with!","title":"Motivation"},{"location":"curriculum/0_before_you_start/software-setup/#package-manager","text":"A package manager will make your life easier. on Mac, install Brew Testing : To check that it installed, run the command which brew in the terminal. If it returns: /usr/local/bin/brew , it means that homebrew is installed; if it returns brew not found , it means homebrew is not installed. on GNU/Linux, you probably already have yum (RedHat based distros) or apt (Debian based distros) Testing : run yum help or apt help ask Windows users around you for their preferred way to manage packages. And tell us, so we can add them here!","title":"Package Manager"},{"location":"curriculum/0_before_you_start/software-setup/#git-and-github-account","text":"If you don't have a GitHub account, make one ! Go to this site , input your username, and click \"Add me to organization\". Your username will be automatically added to the DSSG organization on GitHub. Install git using the appropriate OS\u2019s package manager MacOS In the terminal, type: brew update brew install git GNU/Linux sudo apt update sudo apt install git Windows On Windows, you should already have git. (Either you installed git-bash , which is part of git, or you should have downloaded git in the cygwin package menu.) Test your installation. For example, create a directory, and make it a git repo: mkdir mytestdir cd mytestdir/ git init > Initialized empty Git repository in [...]/mytestdir/.git/ You can un-git the directory by deleting the .git folder: rm -r .git (or simply delete mytestdir entirely with command rmdir mytestdir ).","title":"Git and GitHub Account"},{"location":"curriculum/0_before_you_start/software-setup/#python","text":"As said, your team will decide on which Python version (and versioning) to install. Thus, if you have any working setup already, don't break it (for now)! Just make sure you have the packages listed below installed. pyenv vs anaconda This is a contentious topic! For some people the way to go, because apparently is easier is Anaconda ( conda or mini-conda ), for other, for consistency and flexibility is pyenv . In reality, python library\u2019s system is a mess and in constant evolution. We will favor pyenv here, since we think is the one that allows you more flexibility and teaches you about how python works. If you are in GNU/Linux or in MacOS you have python installed. But that python is the one use by your operative system for doing stuff, probably you don\u2019t want to mess with it. So we will install a different python , for you exclusive use. First we will install some libraries 1 : MacOS # optional, but recommended: brew install openssl readline sqlite3 xz zlib GNU/Linux For Debian based distros: sudo apt-get update; sudo apt-get install --no-install-recommends make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev For Red hat based distros: dnf install make gcc zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel tk-devel libffi-devel We will install pyenv $ curl https://pyenv.run | bash And follow the instructions at the end (Mostly about adding some lines to your .bash_profile or .zsh_profile or similar.) Restart your terminal. Then, we will install a python version: $ pyenv install 3.7.3 This will take several minutes.","title":"Python"},{"location":"curriculum/0_before_you_start/software-setup/#virtual-environment","text":"As a last step, we will create a virtual environment . $ pyenv virtualenv 3.7.3 dssg Info A virtual environment isolates the python libraries that you install in an specific project. And then assign it as the virtual environment to use in your directory of choice with $ echo dssg > .python-version Depending on your command shell ( bash , zsh , csh , etc) configuration you should get some info that the environment is in use, if not you can check it with $ pyenv version dssg (set by /home/user/projects/.python-version) Then we will install a lot of libraries. Copy the file requirements.txt to your directory and execute: $ pip install -r requirements.txt It's time to test! In order to test that both jupyter and the python packages installed appropriately, you should do the following: Download the file SoftwareSetup.ipynb into your directory. And type in the terminal: $ jupyter notebook Your browser will open a new tab and you will see something like the following: Click on SoftwareSetup.ipynb to open the notebook Follow the instructions in the notebook to run each cell.","title":"Virtual environment"},{"location":"curriculum/0_before_you_start/software-setup/#ssh-putty","text":"Use your username and server's address to ssh into the server: ssh yourusername@serverurl Once you enter your password, you should be dropped into a shell on the server: yourusername@servername: ~$ PRO tip Your life will be easier if you set up a .ssh/config file","title":"SSH / Putty"},{"location":"curriculum/0_before_you_start/software-setup/#psql","text":"The database server runs Postgres 9.5.10. For Windows users Windows users should skip the steps below, and instead use DBeaver . When setting up the connection in DBeaver, you will need to specify the SSH tunnel; the database credentials are the ones we shared with you, and the SSH tunnel credentials are the ones you used in the previous step to SSH into the training server. Alternatively, everybody can access psql from the training server: SSH into the training server as in the step before, then, on the server's shell, call psql -h POSTGRESURL -U USERNAME -d DBNAME , where you need to substitute POSTGRESURL with the postgres server's address, USERNAME with your database username, and DBNAME with the name of the database. For all non-Windows users, also do these steps to access the PostgreSQL server from your local machine. First we need to install the database client , psql MacOS Make sure you have the psql client installed; on Mac, this would be $ brew tap-pin dbcli/tap $ brew install pgcli Note, we are installing pgcli instead of psql , but apparently there is no way of install just the client without installing the whole database server. If you still want to give it a shot: $ brew postgres GNU/Linux On Debian based distros: sudo apt install postgresql-client libpq-dev Once you have the postgres client installed, you can access the training database with it. However, the database server only allows access from the training server. Thus, you need to set up an SSH tunnel through the training server to the Postgres server: $ ssh -NL localhost:8888:POSTGRESURL:5432 ec2username@EC2URL where you need to substitute POSTGRESURL , ec2username , and EC2URL with the postgres server's URL, your username on the training server, and the training server's URL respectively. Also, you should substitute 8888 with a random number in the 8000-65000 range of your choice (port 8888 might be in use already). This command forwards your laptop's port 8888 through your account on the EC2 (EC2URL) to the Postgres server port 5432. So if you access your local port 8888 in the next step, you get forwarded to the Postgres server's port 5432 - but from the Postgres server's view, the traffic is now coming from the training server (instead of your laptop), and the training server is the only IP address that is allowed to access the postgres server. Figure. A graphical representation of a ssh tunnel. Not quite our situation -they are using a MySQL db who knows why-, but it is close enough. Courtesy from this Medium post . Connect to the Postgres database on the forwarded port $ psql -h localhost -p 8888 -U USERNAME -d DBNAME where you need to replace USERNAME with the postgres [!] username, DBNAME with the name of your database, and the 8888 with the number you chose in the previous step. You then get prompted for a password. This is now the postgres server asking, so you need to reply with the corresponding password! This should drop you into a SQL shell on the database server. Note In some configurations, you'll need to explicitly assume a role to do anything beyond connecting to the database. To make changes to the training database, use the training_write role. Let's test it by creating and dropping a schema: set role training_write; create schema jsmith; drop schema jsmith; PRO tip You could save a lot of keystrokes if you setup a .pgservice.conf file and a .pgpass file in your $HOME folder. Then you could simply type $ psql service=mydb # mydb is the name of the dbservice I really prefer a GUI if you want a graphical interface to databases - you might want to use DBeaver . for an updated version of this instructions see here \u21a9","title":"PSQL"},{"location":"curriculum/1_getting_and_keeping_data/","text":"Getting and Keeping Data # Data comes in many forms, from many sources - you may get a database dump directly from a project partner, or you may need to scrape data from the web (see Basic Web Scraping ). Either way, once you've got your hands on some data, you'll need to bring it into a database , and start formatting it in such a way that you can use it for analysis. Command Line Tools will start to come in handy here. If your data is in a format that resembles CSV this instructions will be helpful . You'll definitely want to keep track of the steps you took to go from raw data to model-ready data ( Reproducible ETL ). Often data science for social good projects will involve sensitive data, so it's important to be aware of some basic principles of data security: Data Security Primer .","title":"Intro"},{"location":"curriculum/1_getting_and_keeping_data/#getting-and-keeping-data","text":"Data comes in many forms, from many sources - you may get a database dump directly from a project partner, or you may need to scrape data from the web (see Basic Web Scraping ). Either way, once you've got your hands on some data, you'll need to bring it into a database , and start formatting it in such a way that you can use it for analysis. Command Line Tools will start to come in handy here. If your data is in a format that resembles CSV this instructions will be helpful . You'll definitely want to keep track of the steps you took to go from raw data to model-ready data ( Reproducible ETL ). Often data science for social good projects will involve sensitive data, so it's important to be aware of some basic principles of data security: Data Security Primer .","title":"Getting and Keeping Data"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/","text":"Web Scraping 101 # Background and Motivation # With the vast amount of data on the internet, an important question is how to to access it. Normally, we just use a web browser to go to a website and view it manually. Other times a site may have a data portal we can use to download curated data or they an API. In the case where there is useful data stored in web pages we can create programs for the automatic and systematic gathering and parsing of data from the web. Go to the notebook Further Resources # Stanford Tutorial Web Scraping with Python","title":"Web Scraping 101"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/#web-scraping-101","text":"","title":"Web Scraping 101"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/#background-and-motivation","text":"With the vast amount of data on the internet, an important question is how to to access it. Normally, we just use a web browser to go to a website and view it manually. Other times a site may have a data portal we can use to download curated data or they an API. In the case where there is useful data stored in web pages we can create programs for the automatic and systematic gathering and parsing of data from the web. Go to the notebook","title":"Background and Motivation"},{"location":"curriculum/1_getting_and_keeping_data/basic-web-scraping/#further-resources","text":"Stanford Tutorial Web Scraping with Python","title":"Further Resources"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/","text":"CSVs to the Database # Motivation # This summer, you will use a database to store and analyze data. Databases have several advantages over using text files such as CSVs: Databases can store information about relationships between tables. We're collecting more and more data -- often too much to fit in memory. Most databases can handle this. Databases can provide integrity checks and guarantees. If you have a column of numbers in a spreadsheet, Excel will let you change a random cell to text. In contrast, you can tell your database to only accept input that meets your conditions (e.g. type, uniqueness). This is especially important for ongoing projects, where you have new data coming in. Databases allow you to store data in one place. That makes updates easy and reliable. Databases are more secure. You can more carefully control who has which types of access to what data better in a database than with a CSV. Databases can handle multiple users. Concurrent edits to a CSV can get messy. Some file systems won't even let multiple users access a CSV at the same time. Databases are designed to help you do analysis. SQL will probably become your best friend. You'll likely have to load CSVs into your database (e.g. from the open data portal), even if your partner gave a database dump ( which is ideal ). This session builds on what you learned last week in the pipeline and command line sessions. We will focus on ETL. Tools # psql (command line) dBeaver csvkit Notice that we're not using pandas. DO NOT COPY DATA INTO THE DATABASE USING PANDAS. We strongly recommend using psql , which is orders of magnitude faster. Basic Database Lingo # Database server or host : the computer on which the database is running. We will use Amazon RDS. Database : a self-contained set of tables and schema. A server can run many databases. This summer, we will operate databases for almost all projects from the same Amazon server. Schema : similar to a folder. A database can contain many schema, each containing many tables. Tables : tables are like spreadsheets. They have rows and columns and values in each cell. Views : views are virtual tables created by a query but only instantiated when the query is run. They can be used as tables but are generated \"on-demand\" when they're used. An advantage is that they always contain the most current data but take time to compute. Queries : Queries are analysis that you run on a database, often in SQL. Let's Rock Some Data! # Connecting to the database # Some unique aspects of the setup at DSSG: You cannot access the database server directly; you have to connect to the University's secure network and tunnel go through one of the EC2 instances. The data are far safer that way: you have to access the University's secure network, then one of our EC2s, and then the database. There are two ways to connect to the database (once you're on the University network): Connect from your laptop : Use an SSH tunnel to pass data between your laptop and the database. You have a database program running locally. If you're using dBeaver, you're connecting from your laptop. Connect from the EC2 : SSH into the EC2 and run everything from there. Your laptop only sends your commands to the EC2; the EC2 does the work. You don't use an SSH tunnel because everything stays on the EC2. You can use option 1 (especially dBeaver) to explore the data, but you should use option 2 to load the data. First, downloading the datasets to your laptop may violate our contracts. Second, the internet connections will be better. The connections within Amazon are pretty fast; the connections from our office to Amazon might not be. Option 2 keeps the heavy transfers on Amazon's infrastructure. Getting data into a Database # There are three steps to get a CSV into an existing database: 1. Create table : This involves figuring out the structure of the table (how many fields, what should they be called, and what data types they are). Once you figure out the structure, you can create a sql \"CREATE TABLE\" statement and run that to generate an empty table* 2. Copy CSV to the table : Every database has a \"bulk\" copy command that is much more efficient than using pandas. Please do not use pandas to copy large csvs to a database. Postgres has a COPY command that can now copy your csv to the table you just created. 3. Check if it copied successfully : You want to check if your table now has the same number of rows and columns as the CSV (as well as other consistency checks). If it did not copy successfully, you may need to modify the table structure, clean the csv to remove characters, replace nulls, and try steps 1 and 2 again. Step 1: Let's get the structure of the data # In this session, we will put the City of Chicago's food-inspection data into the DSSG training database. Start by SSHing into the training server: ssh your_username@the_training_EC2_address Create a folder for yourself in the EC2 training directory and download the data: cd /mnt/data/projects/training/ mkdir jwalsh cd jwalsh wget -O inspections.csv https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD This gives you a file called inspections.csv . You can explore the data using head , tail , csvlook , and other command-line tools you've learned in previous sessions. Here's the output from csvlook : csvsql generates create table statements for you. Because it uses Python, it will load all the data and then do its thing. That can be really inefficient for large datasets: you have to wait to read the entire dataset, and you need lots of memory to do it. To limit the resources csvsql needs, I'll only use the first 1000 rows. We're using a PostgreSQL (\"Postgres\") database: head -n1000 inspections.csv | csvsql -i postgresql Here's the output: CREATE TABLE stdin ( \"Inspection ID\" DECIMAL NOT NULL, \"DBA Name\" VARCHAR NOT NULL, \"AKA Name\" VARCHAR, \"License #\" DECIMAL NOT NULL, \"Facility Type\" VARCHAR, \"Risk\" VARCHAR, \"Address\" VARCHAR NOT NULL, \"City\" VARCHAR NOT NULL, \"State\" VARCHAR, \"Zip\" DECIMAL NOT NULL, \"Inspection Date\" DATE NOT NULL, \"Inspection Type\" VARCHAR NOT NULL, \"Results\" VARCHAR NOT NULL, \"Violations\" VARCHAR, \"Latitude\" DECIMAL, \"Longitude\" DECIMAL, \"Location\" VARCHAR ); A few things to note: * Inspection ID, DBA Name, AKA Name, etc. are column names. * VARCHAR and INTEGER are column types. VARCHAR(11) means variable character length column up to 11 characters. If you try to give a character column a number, an integer column a decimal, and so on, Postgres will prevent the entire transfer. * NOT NULL means you have to provide a value for that column. * Postgres hates uppercase and spaces in column names. If you have either, you need to wrap the column name in quotation marks. Yuck. * We need to replace stdin with the table name ( jwalsh.jwalsh ). Let's give it another shot: head -n 1000 inspections.csv | tr [:upper:] [:lower:] | tr ' ' '_' | sed 's/#/num/' | csvsql -i postgresql --db-schema jwalsh --tables jwalsh tr [:upper:] [:lower:] converts all uppercase to all lowercase. tr ' ' '_' converts all spaces to underscores. sed 's/#/num/' replaces the pound sign with \"num\". csvsql -i postgresql --db-schema jwalsh --tables jwalsh generates the postgres create table statement. Here's the output: CREATE TABLE jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); csvsql ain't perfect. We could still make changes if we wanted, e.g. changing the license_num column type. But DECIMAL is good enough for this exercise. Let's create the schema and table # Remember, the schema is like a folder. You can use schema to categorize your tables. Let's use a script, which I'll call inspections.sql , to create the schema and table. Here's what it looks like: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh; CREATE TABLE IF NOT EXISTS jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); A few things to note: * The first row sets the role. You need to assume a role that has write permissions to create schemas and tables and to copy data. * I added IF NOT EXISTS conditions for create schema and create table . You don't need those if you run the script once, but if you run the script multiple times, you'll get errors if those already exist. * The create table statement is from above. Step 2: Let's copy the data # All you've given the database to this point is a schema and an empty table. Use psql's \\copy command to get data into the table: \\copy [db table] from '[source CSV]' with csv header . I'll add it to the inspections.sql script: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; To run the script securely, follow these data security guidelines by storing the database credentials in a file. Postgres looks for four environment variables: PGHOST, PGUSER, PGPASSWORD, and PGDATABASE. To set the environment variables using default_profile (copy and modify from default_profile.example ): eval $(cat default_profile) Then psql -f inspections.sql Uh oh, we got an error: Password for user jwalsh: SET psql:copy_example.sql:2: ERROR: null value in column \"city\" violates not-null constraint DETAIL: Failing row contains (2145008, INTERURBAN, INTERURBAN, 2492070, Restaurant, Risk 1 (High), 1438 W CORTLAND ST , null, null, 60642, 2018-02-15, License, Pass, null, 41.916996072966775, -87.6645967198223, (41.916996072966775, -87.6645967198223)). CONTEXT: COPY jwalsh_table, line 7960: \"2145008,INTERURBAN,INTERURBAN,2492070,Restaurant,Risk 1 (High),1438 W CORTLAND ST ,,,60642,02/15/201...\" With Postgres copy , either the entire copy is successful or none of it is. Check your table: nothing is there. I'll modify inspections.sql to allow missing values and try again: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; DROP TABLE IF EXISTS jwalsh_schema.jwalsh_table; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL, dba_name VARCHAR, aka_name VARCHAR, license_num DECIMAL, facility_type VARCHAR, risk VARCHAR, address VARCHAR, city VARCHAR, state VARCHAR, zip DECIMAL, inspection_date DATE, inspection_type VARCHAR, results VARCHAR, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; Run the script: psql -f inspections.sql Step 3: Let's look at the data and make sure everything is there # Check if the data are there. Here's what it looks like in dBeaver: Further Resources # Software Carpentry: Databases and SQL Discussion Notes #","title":"CSVs to the Database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#csvs-to-the-database","text":"","title":"CSVs to the Database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#motivation","text":"This summer, you will use a database to store and analyze data. Databases have several advantages over using text files such as CSVs: Databases can store information about relationships between tables. We're collecting more and more data -- often too much to fit in memory. Most databases can handle this. Databases can provide integrity checks and guarantees. If you have a column of numbers in a spreadsheet, Excel will let you change a random cell to text. In contrast, you can tell your database to only accept input that meets your conditions (e.g. type, uniqueness). This is especially important for ongoing projects, where you have new data coming in. Databases allow you to store data in one place. That makes updates easy and reliable. Databases are more secure. You can more carefully control who has which types of access to what data better in a database than with a CSV. Databases can handle multiple users. Concurrent edits to a CSV can get messy. Some file systems won't even let multiple users access a CSV at the same time. Databases are designed to help you do analysis. SQL will probably become your best friend. You'll likely have to load CSVs into your database (e.g. from the open data portal), even if your partner gave a database dump ( which is ideal ). This session builds on what you learned last week in the pipeline and command line sessions. We will focus on ETL.","title":"Motivation"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#tools","text":"psql (command line) dBeaver csvkit Notice that we're not using pandas. DO NOT COPY DATA INTO THE DATABASE USING PANDAS. We strongly recommend using psql , which is orders of magnitude faster.","title":"Tools"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#basic-database-lingo","text":"Database server or host : the computer on which the database is running. We will use Amazon RDS. Database : a self-contained set of tables and schema. A server can run many databases. This summer, we will operate databases for almost all projects from the same Amazon server. Schema : similar to a folder. A database can contain many schema, each containing many tables. Tables : tables are like spreadsheets. They have rows and columns and values in each cell. Views : views are virtual tables created by a query but only instantiated when the query is run. They can be used as tables but are generated \"on-demand\" when they're used. An advantage is that they always contain the most current data but take time to compute. Queries : Queries are analysis that you run on a database, often in SQL.","title":"Basic Database Lingo"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#lets-rock-some-data","text":"","title":"Let's Rock Some Data!"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#connecting-to-the-database","text":"Some unique aspects of the setup at DSSG: You cannot access the database server directly; you have to connect to the University's secure network and tunnel go through one of the EC2 instances. The data are far safer that way: you have to access the University's secure network, then one of our EC2s, and then the database. There are two ways to connect to the database (once you're on the University network): Connect from your laptop : Use an SSH tunnel to pass data between your laptop and the database. You have a database program running locally. If you're using dBeaver, you're connecting from your laptop. Connect from the EC2 : SSH into the EC2 and run everything from there. Your laptop only sends your commands to the EC2; the EC2 does the work. You don't use an SSH tunnel because everything stays on the EC2. You can use option 1 (especially dBeaver) to explore the data, but you should use option 2 to load the data. First, downloading the datasets to your laptop may violate our contracts. Second, the internet connections will be better. The connections within Amazon are pretty fast; the connections from our office to Amazon might not be. Option 2 keeps the heavy transfers on Amazon's infrastructure.","title":"Connecting to the database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#getting-data-into-a-database","text":"There are three steps to get a CSV into an existing database: 1. Create table : This involves figuring out the structure of the table (how many fields, what should they be called, and what data types they are). Once you figure out the structure, you can create a sql \"CREATE TABLE\" statement and run that to generate an empty table* 2. Copy CSV to the table : Every database has a \"bulk\" copy command that is much more efficient than using pandas. Please do not use pandas to copy large csvs to a database. Postgres has a COPY command that can now copy your csv to the table you just created. 3. Check if it copied successfully : You want to check if your table now has the same number of rows and columns as the CSV (as well as other consistency checks). If it did not copy successfully, you may need to modify the table structure, clean the csv to remove characters, replace nulls, and try steps 1 and 2 again.","title":"Getting data into a Database"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#step-1-lets-get-the-structure-of-the-data","text":"In this session, we will put the City of Chicago's food-inspection data into the DSSG training database. Start by SSHing into the training server: ssh your_username@the_training_EC2_address Create a folder for yourself in the EC2 training directory and download the data: cd /mnt/data/projects/training/ mkdir jwalsh cd jwalsh wget -O inspections.csv https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD This gives you a file called inspections.csv . You can explore the data using head , tail , csvlook , and other command-line tools you've learned in previous sessions. Here's the output from csvlook : csvsql generates create table statements for you. Because it uses Python, it will load all the data and then do its thing. That can be really inefficient for large datasets: you have to wait to read the entire dataset, and you need lots of memory to do it. To limit the resources csvsql needs, I'll only use the first 1000 rows. We're using a PostgreSQL (\"Postgres\") database: head -n1000 inspections.csv | csvsql -i postgresql Here's the output: CREATE TABLE stdin ( \"Inspection ID\" DECIMAL NOT NULL, \"DBA Name\" VARCHAR NOT NULL, \"AKA Name\" VARCHAR, \"License #\" DECIMAL NOT NULL, \"Facility Type\" VARCHAR, \"Risk\" VARCHAR, \"Address\" VARCHAR NOT NULL, \"City\" VARCHAR NOT NULL, \"State\" VARCHAR, \"Zip\" DECIMAL NOT NULL, \"Inspection Date\" DATE NOT NULL, \"Inspection Type\" VARCHAR NOT NULL, \"Results\" VARCHAR NOT NULL, \"Violations\" VARCHAR, \"Latitude\" DECIMAL, \"Longitude\" DECIMAL, \"Location\" VARCHAR ); A few things to note: * Inspection ID, DBA Name, AKA Name, etc. are column names. * VARCHAR and INTEGER are column types. VARCHAR(11) means variable character length column up to 11 characters. If you try to give a character column a number, an integer column a decimal, and so on, Postgres will prevent the entire transfer. * NOT NULL means you have to provide a value for that column. * Postgres hates uppercase and spaces in column names. If you have either, you need to wrap the column name in quotation marks. Yuck. * We need to replace stdin with the table name ( jwalsh.jwalsh ). Let's give it another shot: head -n 1000 inspections.csv | tr [:upper:] [:lower:] | tr ' ' '_' | sed 's/#/num/' | csvsql -i postgresql --db-schema jwalsh --tables jwalsh tr [:upper:] [:lower:] converts all uppercase to all lowercase. tr ' ' '_' converts all spaces to underscores. sed 's/#/num/' replaces the pound sign with \"num\". csvsql -i postgresql --db-schema jwalsh --tables jwalsh generates the postgres create table statement. Here's the output: CREATE TABLE jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); csvsql ain't perfect. We could still make changes if we wanted, e.g. changing the license_num column type. But DECIMAL is good enough for this exercise.","title":"Step 1: Let's get the structure of the data"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#lets-create-the-schema-and-table","text":"Remember, the schema is like a folder. You can use schema to categorize your tables. Let's use a script, which I'll call inspections.sql , to create the schema and table. Here's what it looks like: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh; CREATE TABLE IF NOT EXISTS jwalsh.jwalsh ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); A few things to note: * The first row sets the role. You need to assume a role that has write permissions to create schemas and tables and to copy data. * I added IF NOT EXISTS conditions for create schema and create table . You don't need those if you run the script once, but if you run the script multiple times, you'll get errors if those already exist. * The create table statement is from above.","title":"Let's create the schema and table"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#step-2-lets-copy-the-data","text":"All you've given the database to this point is a schema and an empty table. Use psql's \\copy command to get data into the table: \\copy [db table] from '[source CSV]' with csv header . I'll add it to the inspections.sql script: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL NOT NULL, dba_name VARCHAR NOT NULL, aka_name VARCHAR, license_num DECIMAL NOT NULL, facility_type VARCHAR, risk VARCHAR, address VARCHAR NOT NULL, city VARCHAR NOT NULL, state VARCHAR, zip DECIMAL NOT NULL, inspection_date DATE NOT NULL, inspection_type VARCHAR NOT NULL, results VARCHAR NOT NULL, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; To run the script securely, follow these data security guidelines by storing the database credentials in a file. Postgres looks for four environment variables: PGHOST, PGUSER, PGPASSWORD, and PGDATABASE. To set the environment variables using default_profile (copy and modify from default_profile.example ): eval $(cat default_profile) Then psql -f inspections.sql Uh oh, we got an error: Password for user jwalsh: SET psql:copy_example.sql:2: ERROR: null value in column \"city\" violates not-null constraint DETAIL: Failing row contains (2145008, INTERURBAN, INTERURBAN, 2492070, Restaurant, Risk 1 (High), 1438 W CORTLAND ST , null, null, 60642, 2018-02-15, License, Pass, null, 41.916996072966775, -87.6645967198223, (41.916996072966775, -87.6645967198223)). CONTEXT: COPY jwalsh_table, line 7960: \"2145008,INTERURBAN,INTERURBAN,2492070,Restaurant,Risk 1 (High),1438 W CORTLAND ST ,,,60642,02/15/201...\" With Postgres copy , either the entire copy is successful or none of it is. Check your table: nothing is there. I'll modify inspections.sql to allow missing values and try again: SET ROLE training_write; CREATE SCHEMA IF NOT EXISTS jwalsh_schema; DROP TABLE IF EXISTS jwalsh_schema.jwalsh_table; CREATE TABLE jwalsh_schema.jwalsh_table ( inspection_id DECIMAL, dba_name VARCHAR, aka_name VARCHAR, license_num DECIMAL, facility_type VARCHAR, risk VARCHAR, address VARCHAR, city VARCHAR, state VARCHAR, zip DECIMAL, inspection_date DATE, inspection_type VARCHAR, results VARCHAR, violations VARCHAR, latitude DECIMAL, longitude DECIMAL, location VARCHAR ); \\COPY jwalsh_schema.jwalsh_table from 'inspections.csv' WITH CSV HEADER; Run the script: psql -f inspections.sql","title":"Step 2: Let's copy the data"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#step-3-lets-look-at-the-data-and-make-sure-everything-is-there","text":"Check if the data are there. Here's what it looks like in dBeaver:","title":"Step 3: Let's look at the data and make sure everything is there"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#further-resources","text":"Software Carpentry: Databases and SQL","title":"Further Resources"},{"location":"curriculum/1_getting_and_keeping_data/csv-to-db/#discussion-notes","text":"","title":"Discussion Notes"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/","text":"Data Security Primer # Why this is important # We have a lot of sensitive information Much of it is private data about individuals Legal agreements in place with partners to keep data safe Security 101 # No such thing as absolute security Consider your home Can a dedicated attacker break in to your home? Do you lock your door? Goal: Reduce risk of disclosure What We Care About # Confidentiality of project data Login credentials to the servers and databases (and places where these credentials are stored) Common DSSG Challenges # Avoid: Committing database credentials, API keys, SSH keys, etc. to Github repos Maintain awareness: IPython notebooks with exploratory data analysis with confidential data in them (talk with your team about this) Commit with Confidence! # Use git add filename to stage files individually Before you commit, git diff --cached to verify what you have staged is what you expect If you have files that you want to make sure that you do not commit, add them to your [.gitignore]{.title-ref} Authentication # Use unique, strong passwords Use a password manager e.g. KeePass, LastPass, 1Password Use two factor authentication when available (e.g. on Github) Database: Don\\'t # Don\\'t commit the following: from sqlalchemy import create_engine engine = create_engine ( 'postgresql://dbpro:ayylmao@dssg.example.com:5432/mydatabase' ) Database: Do # Store these credentials in a separate file dbcreds.py : host = 'dssg.example.com' user = 'dbpro' database = 'mydatabase' password = 'ayylmao' Add this file to your [.gitignore]{.title-ref} to ensure that you don\\'t commit it ( https://help.github.com/articles/ignoring-files/ ) You can commit an example file to your repo dbcreds.example : host = '' user = '' database = '' password = '' Database: Do # import dbcreds engine = sqlalchemy . create_engine (( 'postgresql://{conf.user}:' '{conf.password}@{conf.host}:5432/{conf.database}' ) . format ( conf = dbcreds )) Database: Do # Commit an even simpler config file `dbcreds.py`: config = { 'sqlalchemy.url' : 'postgres://dbpro:ayylmao@dssg.example.com/mydatabase' } And then connect: import sqlalchemy from dbcreds import config engine = sqlalchemy . engine_from_config ( config ) Beyond Content # Consider whether your project partner would want the names of tables disclosed Example: https://github.com/dssg/police-eis/blob/master/example_officer_config.yaml Cleaning Repos # Search for passwords/data leaks in a folder: https://github.com/dssg/repo-scraper Instead of git-filter-branch to remove secret things from your git repository: https://github.com/rtyley/bfg-repo-cleaner Mistakes Happen # Avoid cleaning by not putting sensitive data in your repos Web Applications # If you end up creating a web application, be aware of security best practices: OWASP Secure Coding Practices: https://www.owasp.org/images/0/08/OWASP_SCP_Quick_Reference_Guide_v2.pdf","title":"Home"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#data-security-primer","text":"","title":"Data Security Primer"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#why-this-is-important","text":"We have a lot of sensitive information Much of it is private data about individuals Legal agreements in place with partners to keep data safe","title":"Why this is important"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#security-101","text":"No such thing as absolute security Consider your home Can a dedicated attacker break in to your home? Do you lock your door? Goal: Reduce risk of disclosure","title":"Security 101"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#what-we-care-about","text":"Confidentiality of project data Login credentials to the servers and databases (and places where these credentials are stored)","title":"What We Care About"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#common-dssg-challenges","text":"Avoid: Committing database credentials, API keys, SSH keys, etc. to Github repos Maintain awareness: IPython notebooks with exploratory data analysis with confidential data in them (talk with your team about this)","title":"Common DSSG Challenges"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#commit-with-confidence","text":"Use git add filename to stage files individually Before you commit, git diff --cached to verify what you have staged is what you expect If you have files that you want to make sure that you do not commit, add them to your [.gitignore]{.title-ref}","title":"Commit with Confidence!"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#authentication","text":"Use unique, strong passwords Use a password manager e.g. KeePass, LastPass, 1Password Use two factor authentication when available (e.g. on Github)","title":"Authentication"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#database-dont","text":"Don\\'t commit the following: from sqlalchemy import create_engine engine = create_engine ( 'postgresql://dbpro:ayylmao@dssg.example.com:5432/mydatabase' )","title":"Database: Don\\'t"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#database-do","text":"Store these credentials in a separate file dbcreds.py : host = 'dssg.example.com' user = 'dbpro' database = 'mydatabase' password = 'ayylmao' Add this file to your [.gitignore]{.title-ref} to ensure that you don\\'t commit it ( https://help.github.com/articles/ignoring-files/ ) You can commit an example file to your repo dbcreds.example : host = '' user = '' database = '' password = ''","title":"Database: Do"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#database-do_1","text":"import dbcreds engine = sqlalchemy . create_engine (( 'postgresql://{conf.user}:' '{conf.password}@{conf.host}:5432/{conf.database}' ) . format ( conf = dbcreds ))","title":"Database: Do"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#database-do_2","text":"Commit an even simpler config file `dbcreds.py`: config = { 'sqlalchemy.url' : 'postgres://dbpro:ayylmao@dssg.example.com/mydatabase' } And then connect: import sqlalchemy from dbcreds import config engine = sqlalchemy . engine_from_config ( config )","title":"Database: Do"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#beyond-content","text":"Consider whether your project partner would want the names of tables disclosed Example: https://github.com/dssg/police-eis/blob/master/example_officer_config.yaml","title":"Beyond Content"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#cleaning-repos","text":"Search for passwords/data leaks in a folder: https://github.com/dssg/repo-scraper Instead of git-filter-branch to remove secret things from your git repository: https://github.com/rtyley/bfg-repo-cleaner","title":"Cleaning Repos"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#mistakes-happen","text":"Avoid cleaning by not putting sensitive data in your repos","title":"Mistakes Happen"},{"location":"curriculum/1_getting_and_keeping_data/data-security-primer/#web-applications","text":"If you end up creating a web application, be aware of security best practices: OWASP Secure Coding Practices: https://www.owasp.org/images/0/08/OWASP_SCP_Quick_Reference_Guide_v2.pdf","title":"Web Applications"},{"location":"curriculum/1_getting_and_keeping_data/databases/","text":"Databases 101 # Background and Motivation # In the case of small data (you can load it all into memory), simple analysis (maps well to your statistical package of choice), and no plans for you or anyone else to repeat your analysis (nor receive updated data), then keeping your data in text files, and using a scripting language like Python or R to work with it, is fine. In the case that you have a large amount of diverse data (cannot all be loaded into memory) that may be updated, or if you want to share your data with others and let others easily reproduce your analysis, then use a DBMS ( Database Management System ). DBMS are important for storing, organizing, managing and analyzing data. They mitigate the scaling and complexity problem of increasing data in volume and diversity. DBMS facilitate a data model that allows data to be stored, queried, and updated efficiently and concurrently by multiple users. In general, as a data scientist your toolkit will involve using SQL (Structured Query Language) with a database and something else-- python , R , SAS, Stata, SPSS. This tutorial covers the basics of relational databases and NoSQL databases, the pros and cons of each type of database, and when to use which one. Materials # Slides","title":"Databases 101"},{"location":"curriculum/1_getting_and_keeping_data/databases/#databases-101","text":"","title":"Databases 101"},{"location":"curriculum/1_getting_and_keeping_data/databases/#background-and-motivation","text":"In the case of small data (you can load it all into memory), simple analysis (maps well to your statistical package of choice), and no plans for you or anyone else to repeat your analysis (nor receive updated data), then keeping your data in text files, and using a scripting language like Python or R to work with it, is fine. In the case that you have a large amount of diverse data (cannot all be loaded into memory) that may be updated, or if you want to share your data with others and let others easily reproduce your analysis, then use a DBMS ( Database Management System ). DBMS are important for storing, organizing, managing and analyzing data. They mitigate the scaling and complexity problem of increasing data in volume and diversity. DBMS facilitate a data model that allows data to be stored, queried, and updated efficiently and concurrently by multiple users. In general, as a data scientist your toolkit will involve using SQL (Structured Query Language) with a database and something else-- python , R , SAS, Stata, SPSS. This tutorial covers the basics of relational databases and NoSQL databases, the pros and cons of each type of database, and when to use which one.","title":"Background and Motivation"},{"location":"curriculum/1_getting_and_keeping_data/databases/#materials","text":"Slides","title":"Materials"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/","text":"Reproducible ETL # Motivation # Understanding what you did : Save all the steps you took so you can tell how you got here. Using what you did : Use and re-use code for this project or for others. Fix errors easily. Import new data with confidence. GET IT IMPLEMENTED! This session builds on what you learned last week in the CSV to DB session. Potential Teachouts # Proprietary-database transfers (e.g. SQL Server, Oracle) Concepts # Many people associate data science with fancy machine-learning algorithms, but ETL is arguably more important. ETL: Extract : Get data from the source, e.g. a CSV the partner gave you. Transform : Get the data into the format you want/need, e.g. standardize missing values. Load : Get the data into the database. There are two reasons why ETL matters so much: The rest of your analysis depends on your ETL. For example, you might ignore some of the most important cases if you drop rows with missing values . Better data can help more than better methods. So you should do ETL well: * Reliably * Understandably * Preferably automatically Tools: * Code is typically better than GUIs. Code can be automated. * All else being equal, command-line tools are good choices. They are time tested and efficient. * make (command-line tool written to compile software efficiently) * drake * If you can't save the code, save the notes, e.g. record how you used Pentaho to transfer an Oracle database to Postgres. Examples # Hitchhiker's Guide Weather Example # Remember the weather example ? Let's make sure it's reproducible. I stored the code in two files: * jwalsh_table.sql drops jwalsh_schema.jwalsh_table if it exists, creates the table using our statement from the CSV-to-DB session, then copies the data. * Drakefile downloads the weather data, unzips it, then calls jwalsh_table.sql To run it, make sure you specify the Postgres environment variables in default_profile , then type drake while in this directory. I've run this code many times without error, and I feel pretty confident that it will continue to run without error for a while. Because we wrote some decent ETL code, we don't need to start from scratch. We can borrow the code for this project. (Some of this code originated with the lead project .) Let's say NOAA changes the format of the weather file. This code will throw an error when we try to run it. We don't need to start from scratch. We can simply modify jwalsh_table.sql to match the new format, re-run the code without error, and enjoy the up-to-date data. ETL Tests # You run the risk of losing or corrupting data with each step. To ensure that you extracted, transformed, and loaded the data correctly, you can run simple checks. Here are a few ways: * If you're copying a file, check the hash before and after. They should match. * Count rows. If they should match before and after, do they? * Check aggregate statistics, such as the sum of a column. If they should match before and after, do they? * If you receive a database dump, count the number of schemas/tables/sequences/etc in the origin and destination databases. Do they match? For example, we request partners who use Oracle to run the count_rows_oracle.sql script, which gives the number of rows and columns in the origin database. It's one (necessary but not sufficient) way to check that we got all the data. Lead Project # Well-developed ETL in input/ . Sanergy Project # input/Drakefile calls an R script. The repository is here . What If You Have to Use Points and Clicks? # See last year's police team Oracle directions . Discussion #","title":"Reproducible ETL"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#reproducible-etl","text":"","title":"Reproducible ETL"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#motivation","text":"Understanding what you did : Save all the steps you took so you can tell how you got here. Using what you did : Use and re-use code for this project or for others. Fix errors easily. Import new data with confidence. GET IT IMPLEMENTED! This session builds on what you learned last week in the CSV to DB session.","title":"Motivation"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#potential-teachouts","text":"Proprietary-database transfers (e.g. SQL Server, Oracle)","title":"Potential Teachouts"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#concepts","text":"Many people associate data science with fancy machine-learning algorithms, but ETL is arguably more important. ETL: Extract : Get data from the source, e.g. a CSV the partner gave you. Transform : Get the data into the format you want/need, e.g. standardize missing values. Load : Get the data into the database. There are two reasons why ETL matters so much: The rest of your analysis depends on your ETL. For example, you might ignore some of the most important cases if you drop rows with missing values . Better data can help more than better methods. So you should do ETL well: * Reliably * Understandably * Preferably automatically Tools: * Code is typically better than GUIs. Code can be automated. * All else being equal, command-line tools are good choices. They are time tested and efficient. * make (command-line tool written to compile software efficiently) * drake * If you can't save the code, save the notes, e.g. record how you used Pentaho to transfer an Oracle database to Postgres.","title":"Concepts"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#examples","text":"","title":"Examples"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#hitchhikers-guide-weather-example","text":"Remember the weather example ? Let's make sure it's reproducible. I stored the code in two files: * jwalsh_table.sql drops jwalsh_schema.jwalsh_table if it exists, creates the table using our statement from the CSV-to-DB session, then copies the data. * Drakefile downloads the weather data, unzips it, then calls jwalsh_table.sql To run it, make sure you specify the Postgres environment variables in default_profile , then type drake while in this directory. I've run this code many times without error, and I feel pretty confident that it will continue to run without error for a while. Because we wrote some decent ETL code, we don't need to start from scratch. We can borrow the code for this project. (Some of this code originated with the lead project .) Let's say NOAA changes the format of the weather file. This code will throw an error when we try to run it. We don't need to start from scratch. We can simply modify jwalsh_table.sql to match the new format, re-run the code without error, and enjoy the up-to-date data.","title":"Hitchhiker's Guide Weather Example"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#etl-tests","text":"You run the risk of losing or corrupting data with each step. To ensure that you extracted, transformed, and loaded the data correctly, you can run simple checks. Here are a few ways: * If you're copying a file, check the hash before and after. They should match. * Count rows. If they should match before and after, do they? * Check aggregate statistics, such as the sum of a column. If they should match before and after, do they? * If you receive a database dump, count the number of schemas/tables/sequences/etc in the origin and destination databases. Do they match? For example, we request partners who use Oracle to run the count_rows_oracle.sql script, which gives the number of rows and columns in the origin database. It's one (necessary but not sufficient) way to check that we got all the data.","title":"ETL Tests"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#lead-project","text":"Well-developed ETL in input/ .","title":"Lead Project"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#sanergy-project","text":"input/Drakefile calls an R script. The repository is here .","title":"Sanergy Project"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#what-if-you-have-to-use-points-and-clicks","text":"See last year's police team Oracle directions .","title":"What If You Have to Use Points and Clicks?"},{"location":"curriculum/1_getting_and_keeping_data/reproducible_ETL/#discussion","text":"","title":"Discussion"},{"location":"curriculum/2_data_exploration_and_analysis/","text":"Data Exploration and Analysis # Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data . Data Exploration Tips # Here are some things you want to do during data exploration: distributions of different variables - historgrams, boxplots distinct values for a categorical variable correlations between variables - you can do a correlation matrix and turn it into a heatmap changes and trends over time - how does the data and the entities in the data change over time. Distributions over time. missing values: are there lots of missing values? is there any pattern there? looking at outliers - this can be done using clustering but also using other methods by plotting distributions. cross-tabs (if you're looking at multiple classes/labels), describing how the positive and negative classes are different without doing any machine learning.","title":"Intro"},{"location":"curriculum/2_data_exploration_and_analysis/#data-exploration-and-analysis","text":"Once you've got some data, you're going to be eager to dig into it! Our tool of choice for data analysis is Python. Start off with Intro to Git and Python , then move onto Data Exploration in Python . If you're combining data from multiple sources, you'll have to do record linkage to match entities across datasets. Depending on your particular project, you may need special methods and tools; at this time, we have resources for working with text data , spatial data and network data .","title":"Data Exploration and Analysis"},{"location":"curriculum/2_data_exploration_and_analysis/#data-exploration-tips","text":"Here are some things you want to do during data exploration: distributions of different variables - historgrams, boxplots distinct values for a categorical variable correlations between variables - you can do a correlation matrix and turn it into a heatmap changes and trends over time - how does the data and the entities in the data change over time. Distributions over time. missing values: are there lots of missing values? is there any pattern there? looking at outliers - this can be done using clustering but also using other methods by plotting distributions. cross-tabs (if you're looking at multiple classes/labels), describing how the positive and negative classes are different without doing any machine learning.","title":"Data Exploration Tips"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/","text":"Intro to SQL # You've already used databases. Excel spreadsheets are a simple example. But those databases have many problems, such as size of data you can use is limited by RAM cannot handle complex data (there are databases to handle more complex data types, e.g. documents) difficult to use data from multiple tables/sheets no data integrity guarantees (you can accidentally put a letter in a numeric column and the entire column will become a character column) it's difficult for multiple people to use the spreadsheet at the same time. If one person updates sheet A and another person updates sheet B, integrating both updates gets ugly. Things to cover: select from limit (Postgres) where group by order by So, let's get into it, shall we!? First, we'll need to either: 1. use psql * ssh into the server * Run psql * Run SET ROLE training_write so that we have the appropriate permissions 2. use dbeaver * Run SET ROLE training_write so that we have the appropriate permissions Getting a sense of the tables and data: # A few things we can do to explore: Look at the schemas currently present; make sure yours is there: \\dn List databases: \\l ( This doc has a list of some other quick exploratory commands.) Find the count of the list of rows: SELECT COUNT(*) FROM mpettit_schema.mpettit_table; Output the list of columns for this table: SELECT column_name FROM information_schema.columns WHERE table_name = 'mpettit_table'; If you also want to look at datatype: SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'mpettit_table'; Ok, now that we've gotten a sense of the data, let's dial it back and get to the basics. :) SELECT and FROM # Now, let's look a bit more into SELECT. The SELECT statement is used to select data from a database. The data returned is stored in a result table, called the result-set. In SQL, data is usually organized in various tables. For example, a sports team database might have the tables teams, players, and games. A wedding database might have tables guests, vendors, and music_playlist. First off, let's select everything from the table to see what we get: SELECT * FROM mpettit_schema.mpettit_table; There's a lot to view at once here. Let's say we're not interested in all those comments and just want to look at the columns inspection_id , dba_name , aka_name , results , and inspection_date . We can edit the above command to only select those columns: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table; LIMIT # Often, tables contain millions of rows, and it can take a while to grab everything. If we just want to see a few examples of the data in a table, we can select the first few rows with the LIMIT keyword. (This might remind you of using .head in pandas.) SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table LIMIT 20; ORDER BY # The ORDER BY keyword is used to sort the result-set in ascending or descending order. The ORDER BY keyword sorts the records in ascending order by default. To sort the records in descending order, use the DESC keyword. Here's how you might order by dba_name in ascending order: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table ORDER BY dba_name; Now, try altering the above line so that it orders the list in descending order. How might we do that? Can look here for help. ORDER BY / LIMIT COMBO If you use ORDER BY and then LIMIT, you would get the first rows for that order. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table ORDER BY dba_name LIMIT 10; WHERE # The WHERE clause is used to filter records. That is, the WHERE class extracts only those records that fulfill a specified condition. Let's say we only want to look at records where the restaurant name is SUN WAH, so that we can check to see if it's a good time to go to Joe's favorite duck place. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table WHERE dba_name='SUN WAH'; We'll notice that this does not get us any results... Hmmmm. Let's try using the LIKE operator !! SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table WHERE dba_name LIKE '%SUN WAH%'; GROUPBY # The GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. For example, if we want to find the amount of times that each restaurant has been inspected over this time frame, we might run: SELECT dba_name, COUNT(*) FROM mpettit_schema.mpettit_table GROUP BY dba_name; Let's say you are only concerned with the amount of times that the restaurant failed in this timeframe... SELECT dba_name, COUNT(*) FROM mpettit_schema.mpettit_table WHERE results LIKE 'Fail%' GROUP BY dba_name; JOIN # A JOIN clause is used to combine rows from two or more tables, based on a related column between them. Let's first look here to look at some ven diagrams of the various types of joins. We have a table with zip code boundaries. To demonstrate how joins work, let's join the boundaries table to the inspections table! So, in order to select the dba_name from the inspections table ( mpettit_schema.mpettit_table ) and the objectid from the gis.boundaries table, we would do something like below. Let's discuss what's happening! SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM mpettit_schema.mpettit_table LEFT JOIN gis.boundaries ON gis.boundaries.zip=mpettit_schema.mpettit_table.zip:; So, something went wrong. Any idea what it was? Let's investigate... SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'mpettit_table'; & SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'boundaries'; We see the problem is that the data types for zip code don't match up between the two tables. Let's fix that: SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM mpettit_schema.mpettit_table LEFT JOIN gis.boundaries ON gis.boundaries.zip=mpettit_schema.mpettit_table.zip::varchar; Also, you might notice that this is extremely wordy... We can write a shorter query if we used aliases for those tables. Basically, we create a \"nickname\" for that table. If you want to use an alias for a table, you add AS *alias_name* after the table name. SELECT inspect.dba_name, bound.objectid FROM mpettit_schema.mpettit_table AS inspect LEFT JOIN gis.boundaries AS bound ON bound.zip=inspect.zip::varchar; SQL order of execution: # The clauses of an SQL query are evaluated in a specific order. Here is a blog post that goes into a bit more detail. Joins: SQL's FROM clause selects and joins your tables and is the first executed part of a query. This means that in queries with joins, the join is the first thing to happen.","title":"Intro to SQL"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#intro-to-sql","text":"You've already used databases. Excel spreadsheets are a simple example. But those databases have many problems, such as size of data you can use is limited by RAM cannot handle complex data (there are databases to handle more complex data types, e.g. documents) difficult to use data from multiple tables/sheets no data integrity guarantees (you can accidentally put a letter in a numeric column and the entire column will become a character column) it's difficult for multiple people to use the spreadsheet at the same time. If one person updates sheet A and another person updates sheet B, integrating both updates gets ugly. Things to cover: select from limit (Postgres) where group by order by So, let's get into it, shall we!? First, we'll need to either: 1. use psql * ssh into the server * Run psql * Run SET ROLE training_write so that we have the appropriate permissions 2. use dbeaver * Run SET ROLE training_write so that we have the appropriate permissions","title":"Intro to SQL"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#getting-a-sense-of-the-tables-and-data","text":"A few things we can do to explore: Look at the schemas currently present; make sure yours is there: \\dn List databases: \\l ( This doc has a list of some other quick exploratory commands.) Find the count of the list of rows: SELECT COUNT(*) FROM mpettit_schema.mpettit_table; Output the list of columns for this table: SELECT column_name FROM information_schema.columns WHERE table_name = 'mpettit_table'; If you also want to look at datatype: SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'mpettit_table'; Ok, now that we've gotten a sense of the data, let's dial it back and get to the basics. :)","title":"Getting a sense of the tables and data:"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#select-and-from","text":"Now, let's look a bit more into SELECT. The SELECT statement is used to select data from a database. The data returned is stored in a result table, called the result-set. In SQL, data is usually organized in various tables. For example, a sports team database might have the tables teams, players, and games. A wedding database might have tables guests, vendors, and music_playlist. First off, let's select everything from the table to see what we get: SELECT * FROM mpettit_schema.mpettit_table; There's a lot to view at once here. Let's say we're not interested in all those comments and just want to look at the columns inspection_id , dba_name , aka_name , results , and inspection_date . We can edit the above command to only select those columns: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table;","title":"SELECT and FROM"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#limit","text":"Often, tables contain millions of rows, and it can take a while to grab everything. If we just want to see a few examples of the data in a table, we can select the first few rows with the LIMIT keyword. (This might remind you of using .head in pandas.) SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table LIMIT 20;","title":"LIMIT"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#order-by","text":"The ORDER BY keyword is used to sort the result-set in ascending or descending order. The ORDER BY keyword sorts the records in ascending order by default. To sort the records in descending order, use the DESC keyword. Here's how you might order by dba_name in ascending order: SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table ORDER BY dba_name; Now, try altering the above line so that it orders the list in descending order. How might we do that? Can look here for help. ORDER BY / LIMIT COMBO If you use ORDER BY and then LIMIT, you would get the first rows for that order. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table ORDER BY dba_name LIMIT 10;","title":"ORDER BY"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#where","text":"The WHERE clause is used to filter records. That is, the WHERE class extracts only those records that fulfill a specified condition. Let's say we only want to look at records where the restaurant name is SUN WAH, so that we can check to see if it's a good time to go to Joe's favorite duck place. SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table WHERE dba_name='SUN WAH'; We'll notice that this does not get us any results... Hmmmm. Let's try using the LIKE operator !! SELECT inspection_id, dba_name, aka_name, results, inspection_date FROM mpettit_schema.mpettit_table WHERE dba_name LIKE '%SUN WAH%';","title":"WHERE"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#groupby","text":"The GROUP BY statement is often used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. For example, if we want to find the amount of times that each restaurant has been inspected over this time frame, we might run: SELECT dba_name, COUNT(*) FROM mpettit_schema.mpettit_table GROUP BY dba_name; Let's say you are only concerned with the amount of times that the restaurant failed in this timeframe... SELECT dba_name, COUNT(*) FROM mpettit_schema.mpettit_table WHERE results LIKE 'Fail%' GROUP BY dba_name;","title":"GROUPBY"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#join","text":"A JOIN clause is used to combine rows from two or more tables, based on a related column between them. Let's first look here to look at some ven diagrams of the various types of joins. We have a table with zip code boundaries. To demonstrate how joins work, let's join the boundaries table to the inspections table! So, in order to select the dba_name from the inspections table ( mpettit_schema.mpettit_table ) and the objectid from the gis.boundaries table, we would do something like below. Let's discuss what's happening! SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM mpettit_schema.mpettit_table LEFT JOIN gis.boundaries ON gis.boundaries.zip=mpettit_schema.mpettit_table.zip:; So, something went wrong. Any idea what it was? Let's investigate... SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'mpettit_table'; & SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'boundaries'; We see the problem is that the data types for zip code don't match up between the two tables. Let's fix that: SELECT mpettit_schema.mpettit_table.dba_name, gis.boundaries.objectid FROM mpettit_schema.mpettit_table LEFT JOIN gis.boundaries ON gis.boundaries.zip=mpettit_schema.mpettit_table.zip::varchar; Also, you might notice that this is extremely wordy... We can write a shorter query if we used aliases for those tables. Basically, we create a \"nickname\" for that table. If you want to use an alias for a table, you add AS *alias_name* after the table name. SELECT inspect.dba_name, bound.objectid FROM mpettit_schema.mpettit_table AS inspect LEFT JOIN gis.boundaries AS bound ON bound.zip=inspect.zip::varchar;","title":"JOIN"},{"location":"curriculum/2_data_exploration_and_analysis/basic_sql/#sql-order-of-execution","text":"The clauses of an SQL query are evaluated in a specific order. Here is a blog post that goes into a bit more detail. Joins: SQL's FROM clause selects and joins your tables and is the first executed part of a query. This means that in queries with joins, the join is the first thing to happen.","title":"SQL order of execution:"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/","text":"Introduction to Data Analysis in Python # Background and Motivation # Python is a high-level interpreted general purpose programming language named after a British Comedy Troupe, created by Guido van Rossum (Python's benevolent dictator for life), and maintained by a international group of python enthusiasts. In a python interpreter type import this to read Python's guiding principles. As of the time of this writing (10/2016) python is currently the fifth most popular programming language. It is popular for data science due to being powerful, fast, playing well with others, runs everywhere, easy to learn, highly-readable, open-source, and its fast development time compared to other languages. Because of its general-purpose and ability to call compiled languages like FORTRAN or C it can be used for full-stack development. There is a growing and every improving list of open-source libraries for scientific programming, data manipulation, and data analysis (e.g. NumPy, SciPy, Pandas, Scikit-Learn, Statsmodels Matplotlib, Seaborn, PyTables, etc.) IPython is an enhanced, interactive python interpreter that started as a grad school project by Fernando Perez. The IPython project then evolved into the IPython notebook that would allow users to archive their code, figures, and analysis in a single document, making doing reproducible computational research and sharing said research much easier. The creators of the IPython notebook quickly realized that the \"notebook\" aspects were agnostic to a specific programming language and ported the notebook to other languages, including but not limited to Julia, Python and R. This then led to a rebranding known as the Jupyter Project. The Pandas library, created by Wes McKinney, introduced the R-like dataframe object in Python, making working with data in Python much easier. This tutorial will go over over the basics of Data Analysis in Python using the PyData stack. Getting started with Jupyter Notebooks # To start up a Jupyter notebook server, simply navigate to the directory where you want the notebooks to be saved and run the command jupyter notebook A browser should open with a notebook navigator. Click the \"New\" button and select \"Python 3\". A beautiful blank notebook should open in a new tab Name the notebook by clicking on \"Untitled\" at the top of the page. Notebooks are sequences of cells. Cells can be markdown, code, or raw text. Change the first cell to markdown and briefly describe what you are going to do in the notebook. Running a remote Jupyter Server # You can also run a Jupyter server from a remote machine (e.g, EC2 Instance) and forward the notebook to your local machine. This is preferable if you need to access a large amount of data on a database. Your work will also be saved on a server that is periodically backed-up which will make your work immune to computer-crashes/someone-stealing-your-computer/(some)acts-of-god. Basic port forwarding: ssh -L 44444:localhost:44444 <username>@<instance-name>.dssg.io Now go to the location in the file system you would like to launch a Jupyter server and type jupyter notebook --no-browser --port 44444 You should now be able to go to your browser and go to the address localhost:44444 . Note :The port 44444 is an arbitrary port. If another user is running a server with the same port forwarding then you will not be able to run a Jupyter Server using that port. You will then need to switch to another port number. Anything between 30000-70000 should work The notebooks # The problem notebook , the complete solution notebook","title":"Introduction to Data Analysis in Python"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#introduction-to-data-analysis-in-python","text":"","title":"Introduction to Data Analysis in Python"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#background-and-motivation","text":"Python is a high-level interpreted general purpose programming language named after a British Comedy Troupe, created by Guido van Rossum (Python's benevolent dictator for life), and maintained by a international group of python enthusiasts. In a python interpreter type import this to read Python's guiding principles. As of the time of this writing (10/2016) python is currently the fifth most popular programming language. It is popular for data science due to being powerful, fast, playing well with others, runs everywhere, easy to learn, highly-readable, open-source, and its fast development time compared to other languages. Because of its general-purpose and ability to call compiled languages like FORTRAN or C it can be used for full-stack development. There is a growing and every improving list of open-source libraries for scientific programming, data manipulation, and data analysis (e.g. NumPy, SciPy, Pandas, Scikit-Learn, Statsmodels Matplotlib, Seaborn, PyTables, etc.) IPython is an enhanced, interactive python interpreter that started as a grad school project by Fernando Perez. The IPython project then evolved into the IPython notebook that would allow users to archive their code, figures, and analysis in a single document, making doing reproducible computational research and sharing said research much easier. The creators of the IPython notebook quickly realized that the \"notebook\" aspects were agnostic to a specific programming language and ported the notebook to other languages, including but not limited to Julia, Python and R. This then led to a rebranding known as the Jupyter Project. The Pandas library, created by Wes McKinney, introduced the R-like dataframe object in Python, making working with data in Python much easier. This tutorial will go over over the basics of Data Analysis in Python using the PyData stack.","title":"Background and Motivation"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#getting-started-with-jupyter-notebooks","text":"To start up a Jupyter notebook server, simply navigate to the directory where you want the notebooks to be saved and run the command jupyter notebook A browser should open with a notebook navigator. Click the \"New\" button and select \"Python 3\". A beautiful blank notebook should open in a new tab Name the notebook by clicking on \"Untitled\" at the top of the page. Notebooks are sequences of cells. Cells can be markdown, code, or raw text. Change the first cell to markdown and briefly describe what you are going to do in the notebook.","title":"Getting started with Jupyter Notebooks"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#running-a-remote-jupyter-server","text":"You can also run a Jupyter server from a remote machine (e.g, EC2 Instance) and forward the notebook to your local machine. This is preferable if you need to access a large amount of data on a database. Your work will also be saved on a server that is periodically backed-up which will make your work immune to computer-crashes/someone-stealing-your-computer/(some)acts-of-god. Basic port forwarding: ssh -L 44444:localhost:44444 <username>@<instance-name>.dssg.io Now go to the location in the file system you would like to launch a Jupyter server and type jupyter notebook --no-browser --port 44444 You should now be able to go to your browser and go to the address localhost:44444 . Note :The port 44444 is an arbitrary port. If another user is running a server with the same port forwarding then you will not be able to run a Jupyter Server using that port. You will then need to switch to another port number. Anything between 30000-70000 should work","title":"Running a remote Jupyter Server"},{"location":"curriculum/2_data_exploration_and_analysis/data-exploration-in-python/#the-notebooks","text":"The problem notebook , the complete solution notebook","title":"The notebooks"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/","text":"","title":"Home"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/","text":"A little of theory # Spatial database # Storage of spatial data Analysis of geographic data Manipulation of spatial objects just like the other database objects Creation of subsets of data Fixing geographic data \\ldots \\ldots Database backend for apps Spatial data # Data which describes or represents either a location or a shape Points, lines, polygons Besides the geometrical properties, the spatial data has attributes. Spatial data # Examples: Geocodable address Crime patterns EMS / patient location Weather information City planning Hazard detection Relationships # Proximity Adjacency (touching, connectivity) Containment Operations # Area Length Intersection Union Buffer Why a db instead of a file? # Spatial data is usually related to other types of data. How load data to the db? # shp2pgsql imports standard esri shapefiles and dbf ogr2ogr imports 20 different vector and flat files The spatial data that is not spatial data # longitude latitude disease date 26.870436 -31.909519 mumps 13/12/2008 26.868682 -31.909259 mumps 24/12/2008 26.867707 -31.910494 mumps 22/01/2009 26.854908 -31.920759 measles 11/01/2009 26.855817 -31.921929 measles 26/01/2009 26.852764 -31.921929 measles 10/02/2009 26.854778 -31.925112 measles 22/02/2009 26.869072 -31.911988 mumps 02/02/2009 (the disease and date columns are the attributes of this data) shape files # Stored in files on the computer The most common one is probably the 'shape file' It consists of at least three different files that work together to store vector data extension description `.shp` the geometry file `.dbf` the attributes file `.shx` index file Vector data # Is stored as a series of x,y coordinate pairs inside the computer's memory. Vector data is used to represent points (1 vertex) , lines (polyline) (2 or more vertices, but the first and the last one are different) and areas (polygons). A vector feature has its shape represented using geometry . The geometry is made up of one or more interconnected vertices. A vertex describes a position in space using an x, y and optionally z axis. The x and y values will depend on the coordinate reference system ( CRS ) being used. Problems with vector data # Raster data # Stored as a grid of values Each cell or pixel represents a geographical region, and the value of the pixel represents some attribute of the region Use it when you want to represent a continuous information across an area Multi-band images, each band contains different information Problems with raster data # High resolution raster data requires a huge amount of computer storage. Demo / exercise # Connect to the db # host: gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com port: 5432 username: dssg_gis password: dssg-gis db name:gis_tutorial SSH Tunneling ssh -fNT -L \\ 8889:gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com:5432 \\ -i ~/.ssh/your-dssh-key ec2-instance.dssg.io ## ssh tunneling Command line client psql -h localhost -p 8889 -U dssg_gis gis_tutorial Setup # create an schema using your github account (mine is nanounanue ) create schema your-github-username; Upload the first shapefiles # There are several shapefiles in the data directory First, we can see some information from the files ogrinfo -al roads.shp Observe that the projection is ... projcs[\"nad83_massachusetts_mainland\", geogcs[\"gcs_north_american_1983\", datum[\"north_american_datum_1983\", spheroid[\"grs_1980\",6378137,298.257222101]], primem[\"greenwich\",0], unit[\"degree\",0.017453292519943295]], projection[\"lambert_conformal_conic_2sp\"], parameter[\"standard_parallel_1\",42.68333333333333], parameter[\"standard_parallel_2\",41.71666666666667], parameter[\"latitude_of_origin\",41], parameter[\"central_meridian\",-71.5], parameter[\"false_easting\",200000], parameter[\"false_northing\",750000], unit[\"meter\",1]] ... This projection measures the area in meters. but Using shp2psql tool upload the following files: roads , land , hydrology shp2psql --host=localhost --port=8889 --username=dssg_gis \\ -f roads.shp gis your-github-username.roads \\ | psql -h localhost -p 8889 -u dssg_gis gis_tutorial if you want to change the projection to wgs 1984 (the one used in google maps) # you need to add the flag -s 26986:4326 before the name of the database (gis) # If you open QGIS you should see something like the following: and after some customization: note that we have lands over the roads and over the water . Spatial predicates for cleaning # We will use st_intersects() and st_dwithin() for removing the land which is touch with roads and water, and if it is too far of roads and water, respectively See the file for the sql statements. NOTE: For use of the EXISTS(subquery) look here and here St_intersects(a,b) returns true if exists at least one point in common between the geometrical objects a and b . St_dwithin(a,b,distance) returns true if the geometries a and b are within the specified distance of one another. Other functions: st_equals , st_disjoint , st_touches , st_crosses , st_overlaps , st_contains . Add more data: buildings and residents # Upload to the database the shapefiles buildings and residents . ## This time I will use ogr2ogr, but this is for demostration purpose only ## It is easier use shp2pgsql ogr2ogr -f \"PostgreSQL\" \\ PG:\"host=localhost user=dssg_gis dbname=gis_tutorial password=dssg-gis port=8889\" \\ buildings.shp -nln your-github-username.buildings Spatial joins: creating new views # As you can see, is not a spatial data. It is a regular psv file. But it contains the pid of the land in which the resident lives. csvhead -d '|' ./data/my_town/residents.psv | head How can I convert this data in spatial data? select r.* -- All the attributes of resident , st_centroid(l.the_geom) -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r.pid = l.pid; Ok, very well. But, How can I see this new \"data\" in QGIS ? You need to create a view create or replace view residents_loc as select row_number() over() as rl_id -- We need an unique identifier , r.* -- All the attributes of resident , st_centroid(l.the_geom) as the_geom -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r.pid = l.pid; Spatial operations: Legal issues in our town # How much real state area do we have? select sum(st_area(the_geom))/1000000 as total_sq_km , st_area(st_union(the_geom))/1000000 as no_overlap_total_sq_km -- st_union dissolves the overlaps! from land; Oh, oh. And buildings? select sum(st_area(the_geom))/1000000 as total_sq_km , st_area(st_union(the_geom))/1000000 as no_overlap_total_sq_km from buildings; We have buildings inside buildings, and some lands overlaps with other lands :( Other operations: st_intersection(a,b) , st_difference(a,b) , st_symdifference(a,b) , st_buffer(c) , st_convexhull(c) Spatial joins: Which lands intersects? # select p.pid -- the land , count(o.pid) as total_intersections -- qty of intersections , array_agg(o.pid) as intersected_parcels -- the other lands from land as p inner join land as o on (p.pid <> o.pid and st_intersects(p.the_geom, o.the_geom)) group by p.pid order by p.pid; -- First row returned: pid IN ('000000225', '000027745','000092727','000057051') Which kind of overlap? select count(o.pid) as total_intersections -- Overlaps? , count(case when st_overlaps(o.the_geom,p.the_geom) then 1 else null end) as o_overlaps_p -- It is the same? , count(case when st_equals(o.the_geom,p.the_geom) then 1 else null end) as o_equals_p from land as p inner join land as o on (p.pid <> o.pid and st_intersects(p.the_geom, o.the_geom)); st_overlaps(a,b) returns true if the geometries share some but not all the points, and the intersection has the same dimension as a , b Cleaning the mess: Reassigning residents # update residents set pid = a.newpid from ( select p.pid, min(o.pid) as newpid from land as p inner join land as o on (p.pid = o.pid or st_equals(p.the_geom, o.the_geom)) group by p.pid having p.pid <> min(o.pid)) as a where residents.pid = a.pid returning * -- Return all the updated residents -- so you can see what you just do -- (or you can store it in a another table using CTAS) Cleaning the mess: Deleting the dupe land # -- Add a new column for storing the house types alter table land add column land_type_other varchar[]; -- Copy the types to the first parcel update land set land_type_other = a.dupe_types from ( select p.pid , min(o.pid) as newpid , array_agg(distinct o.land_type) as dupe_types from land as p inner join land as o on (st_equals(p.the_geom, o.the_geom)) group by p.pid having count(p.pid) > 1 and p.pid = min(o.pid) ) as a where land.pid = a.pid returning *; -- Delete the parcels delete from land where pid in (select p.pid from land as p inner join land as o on (st_equals(p.the_geom, o.the_geom)) group by p.pid having count(p.pid) > 1 and p.pid <> min(o.pid)) ; Spatial analytics: Questions # How many kinds under 12 are further than a km of an elementary school? select sum(num_children_b12)*100.00/(select sum(num_children_b12) from residents) from residents as r inner join land as l on r.pid = l.pid left join ( select pid, the_geom from land where land_type = 'elementary school' or 'elementary school' = any(land_type_other) ) as eschools on st_dwithin(l.the_geom, eschools.the_geom, 1000) where eschools.pid is null; How much area are in empty lands? select st_area(st_union(the_geom))/1000000 from land where land_type = 'vacant land'; Which are the 10 nearest houses to the lakes? select h.hyd_name, array( select bldg_name from buildings b where b.bldg_type like '%family' order by h.the_geom <#> b.the_geom limit 5 ) from hydrology h where h.hyd_name in ('lake 1', 'elephantine youth'); Note the <#> (bounding box), <-> (centroids) are distance operators, see here and here . Another example: mapping civilizations # Intro # Recently this article was published: Spatializing 6,000 years of global urbanization from 3700 BC to AD 2000 Reba, M., Reitsma, F. and Seto, C. , 2016 The article describes all the cities since 3700 BC, including name, population and the position (latitude, longitude). We will use a subset ( chandlerV2 ) of the data for transforming it to a table, and then generating a geojson . Uploading the data # In the directory ./data/Historical Urban Population Growth Data cvslook chandlerV2.csv It will fail, because some encoding issues iconv -f iso-8859-1 -t utf-8 chandlerV2.csv > chandler_utf8.csv csvsql --db postgresql://dssg_gis:dssg-gis@localhost:8889/gis_tutorial \\ --insert chandlerV2_utf8.csv --table chandler --db-schema nanounanue SQL stuff select count(*) from chandler; -- How many cities do we have? New table for easier manipulation create table cities as -- CTAS select \"City\" as city, \"Country\" as country, \"Latitude\" as y_lat, \"Longitude\" as x_lon from chandler; Adding a geometry column and transform to Point alter table cities add column geom geometry(Point, 4326); -- Transforming Lon/Lat to Points update cities set geom = ST_SetSRID(ST_MakePoint(x_lon, y_lat), 4326); Converting to GeoJSON \\copy ( select row_to_json(fc) from ( select 'featurecollection' as type, array_to_json(array_agg(f)) as features from ( select 'feature' as type , st_asgeojson(cities.geom)::json as geometry , row_to_json( (select c from (select city, country) as c) ) as properties from cities ) as f ) as fc) to '~/cities.geojson'; This type of file could be used with d3.js for making interactive plots. For better performance you could use topojson Thank you #","title":"A little of theory"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#a-little-of-theory","text":"","title":"A little of theory"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-database","text":"Storage of spatial data Analysis of geographic data Manipulation of spatial objects just like the other database objects Creation of subsets of data Fixing geographic data \\ldots \\ldots Database backend for apps","title":"Spatial database"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-data","text":"Data which describes or represents either a location or a shape Points, lines, polygons Besides the geometrical properties, the spatial data has attributes.","title":"Spatial data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-data_1","text":"Examples: Geocodable address Crime patterns EMS / patient location Weather information City planning Hazard detection","title":"Spatial data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#relationships","text":"Proximity Adjacency (touching, connectivity) Containment","title":"Relationships"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#operations","text":"Area Length Intersection Union Buffer","title":"Operations"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#why-a-db-instead-of-a-file","text":"Spatial data is usually related to other types of data.","title":"Why a db instead of a file?"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#how-load-data-to-the-db","text":"shp2pgsql imports standard esri shapefiles and dbf ogr2ogr imports 20 different vector and flat files","title":"How load data to the db?"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#the-spatial-data-that-is-not-spatial-data","text":"longitude latitude disease date 26.870436 -31.909519 mumps 13/12/2008 26.868682 -31.909259 mumps 24/12/2008 26.867707 -31.910494 mumps 22/01/2009 26.854908 -31.920759 measles 11/01/2009 26.855817 -31.921929 measles 26/01/2009 26.852764 -31.921929 measles 10/02/2009 26.854778 -31.925112 measles 22/02/2009 26.869072 -31.911988 mumps 02/02/2009 (the disease and date columns are the attributes of this data)","title":"The spatial data that is not spatial data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#shape-files","text":"Stored in files on the computer The most common one is probably the 'shape file' It consists of at least three different files that work together to store vector data extension description `.shp` the geometry file `.dbf` the attributes file `.shx` index file","title":"shape files"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#vector-data","text":"Is stored as a series of x,y coordinate pairs inside the computer's memory. Vector data is used to represent points (1 vertex) , lines (polyline) (2 or more vertices, but the first and the last one are different) and areas (polygons). A vector feature has its shape represented using geometry . The geometry is made up of one or more interconnected vertices. A vertex describes a position in space using an x, y and optionally z axis. The x and y values will depend on the coordinate reference system ( CRS ) being used.","title":"Vector data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#problems-with-vector-data","text":"","title":"Problems with vector data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#raster-data","text":"Stored as a grid of values Each cell or pixel represents a geographical region, and the value of the pixel represents some attribute of the region Use it when you want to represent a continuous information across an area Multi-band images, each band contains different information","title":"Raster data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#problems-with-raster-data","text":"High resolution raster data requires a huge amount of computer storage.","title":"Problems with raster data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#demo-exercise","text":"","title":"Demo / exercise"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#connect-to-the-db","text":"host: gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com port: 5432 username: dssg_gis password: dssg-gis db name:gis_tutorial SSH Tunneling ssh -fNT -L \\ 8889:gis-tutorial.c5faqozfo86k.us-west-2.rds.amazonaws.com:5432 \\ -i ~/.ssh/your-dssh-key ec2-instance.dssg.io ## ssh tunneling Command line client psql -h localhost -p 8889 -U dssg_gis gis_tutorial","title":"Connect to the db"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#setup","text":"create an schema using your github account (mine is nanounanue ) create schema your-github-username;","title":"Setup"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#upload-the-first-shapefiles","text":"There are several shapefiles in the data directory First, we can see some information from the files ogrinfo -al roads.shp Observe that the projection is ... projcs[\"nad83_massachusetts_mainland\", geogcs[\"gcs_north_american_1983\", datum[\"north_american_datum_1983\", spheroid[\"grs_1980\",6378137,298.257222101]], primem[\"greenwich\",0], unit[\"degree\",0.017453292519943295]], projection[\"lambert_conformal_conic_2sp\"], parameter[\"standard_parallel_1\",42.68333333333333], parameter[\"standard_parallel_2\",41.71666666666667], parameter[\"latitude_of_origin\",41], parameter[\"central_meridian\",-71.5], parameter[\"false_easting\",200000], parameter[\"false_northing\",750000], unit[\"meter\",1]] ... This projection measures the area in meters. but Using shp2psql tool upload the following files: roads , land , hydrology shp2psql --host=localhost --port=8889 --username=dssg_gis \\ -f roads.shp gis your-github-username.roads \\ | psql -h localhost -p 8889 -u dssg_gis gis_tutorial","title":"Upload the first shapefiles"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#if-you-want-to-change-the-projection-to-wgs-1984-the-one-used-in-google-maps","text":"","title":"if you want to change the projection to wgs 1984 (the one used in google maps)"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#you-need-to-add-the-flag-s-269864326-before-the-name-of-the-database-gis","text":"If you open QGIS you should see something like the following: and after some customization: note that we have lands over the roads and over the water .","title":"you need to add the flag -s 26986:4326 before the name of the database (gis)"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-predicates-for-cleaning","text":"We will use st_intersects() and st_dwithin() for removing the land which is touch with roads and water, and if it is too far of roads and water, respectively See the file for the sql statements. NOTE: For use of the EXISTS(subquery) look here and here St_intersects(a,b) returns true if exists at least one point in common between the geometrical objects a and b . St_dwithin(a,b,distance) returns true if the geometries a and b are within the specified distance of one another. Other functions: st_equals , st_disjoint , st_touches , st_crosses , st_overlaps , st_contains .","title":"Spatial predicates for cleaning"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#add-more-data-buildings-and-residents","text":"Upload to the database the shapefiles buildings and residents . ## This time I will use ogr2ogr, but this is for demostration purpose only ## It is easier use shp2pgsql ogr2ogr -f \"PostgreSQL\" \\ PG:\"host=localhost user=dssg_gis dbname=gis_tutorial password=dssg-gis port=8889\" \\ buildings.shp -nln your-github-username.buildings","title":"Add more data: buildings and residents"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-joins-creating-new-views","text":"As you can see, is not a spatial data. It is a regular psv file. But it contains the pid of the land in which the resident lives. csvhead -d '|' ./data/my_town/residents.psv | head How can I convert this data in spatial data? select r.* -- All the attributes of resident , st_centroid(l.the_geom) -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r.pid = l.pid; Ok, very well. But, How can I see this new \"data\" in QGIS ? You need to create a view create or replace view residents_loc as select row_number() over() as rl_id -- We need an unique identifier , r.* -- All the attributes of resident , st_centroid(l.the_geom) as the_geom -- The centroid of the land in which this resident lives from residents as r inner join -- only the matches land as l on r.pid = l.pid;","title":"Spatial joins: creating new views"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-operations-legal-issues-in-our-town","text":"How much real state area do we have? select sum(st_area(the_geom))/1000000 as total_sq_km , st_area(st_union(the_geom))/1000000 as no_overlap_total_sq_km -- st_union dissolves the overlaps! from land; Oh, oh. And buildings? select sum(st_area(the_geom))/1000000 as total_sq_km , st_area(st_union(the_geom))/1000000 as no_overlap_total_sq_km from buildings; We have buildings inside buildings, and some lands overlaps with other lands :( Other operations: st_intersection(a,b) , st_difference(a,b) , st_symdifference(a,b) , st_buffer(c) , st_convexhull(c)","title":"Spatial operations: Legal issues in our town"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-joins-which-lands-intersects","text":"select p.pid -- the land , count(o.pid) as total_intersections -- qty of intersections , array_agg(o.pid) as intersected_parcels -- the other lands from land as p inner join land as o on (p.pid <> o.pid and st_intersects(p.the_geom, o.the_geom)) group by p.pid order by p.pid; -- First row returned: pid IN ('000000225', '000027745','000092727','000057051') Which kind of overlap? select count(o.pid) as total_intersections -- Overlaps? , count(case when st_overlaps(o.the_geom,p.the_geom) then 1 else null end) as o_overlaps_p -- It is the same? , count(case when st_equals(o.the_geom,p.the_geom) then 1 else null end) as o_equals_p from land as p inner join land as o on (p.pid <> o.pid and st_intersects(p.the_geom, o.the_geom)); st_overlaps(a,b) returns true if the geometries share some but not all the points, and the intersection has the same dimension as a , b","title":"Spatial joins: Which lands intersects?"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#cleaning-the-mess-reassigning-residents","text":"update residents set pid = a.newpid from ( select p.pid, min(o.pid) as newpid from land as p inner join land as o on (p.pid = o.pid or st_equals(p.the_geom, o.the_geom)) group by p.pid having p.pid <> min(o.pid)) as a where residents.pid = a.pid returning * -- Return all the updated residents -- so you can see what you just do -- (or you can store it in a another table using CTAS)","title":"Cleaning the mess: Reassigning residents"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#cleaning-the-mess-deleting-the-dupe-land","text":"-- Add a new column for storing the house types alter table land add column land_type_other varchar[]; -- Copy the types to the first parcel update land set land_type_other = a.dupe_types from ( select p.pid , min(o.pid) as newpid , array_agg(distinct o.land_type) as dupe_types from land as p inner join land as o on (st_equals(p.the_geom, o.the_geom)) group by p.pid having count(p.pid) > 1 and p.pid = min(o.pid) ) as a where land.pid = a.pid returning *; -- Delete the parcels delete from land where pid in (select p.pid from land as p inner join land as o on (st_equals(p.the_geom, o.the_geom)) group by p.pid having count(p.pid) > 1 and p.pid <> min(o.pid)) ;","title":"Cleaning the mess: Deleting the dupe land"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#spatial-analytics-questions","text":"How many kinds under 12 are further than a km of an elementary school? select sum(num_children_b12)*100.00/(select sum(num_children_b12) from residents) from residents as r inner join land as l on r.pid = l.pid left join ( select pid, the_geom from land where land_type = 'elementary school' or 'elementary school' = any(land_type_other) ) as eschools on st_dwithin(l.the_geom, eschools.the_geom, 1000) where eschools.pid is null; How much area are in empty lands? select st_area(st_union(the_geom))/1000000 from land where land_type = 'vacant land'; Which are the 10 nearest houses to the lakes? select h.hyd_name, array( select bldg_name from buildings b where b.bldg_type like '%family' order by h.the_geom <#> b.the_geom limit 5 ) from hydrology h where h.hyd_name in ('lake 1', 'elephantine youth'); Note the <#> (bounding box), <-> (centroids) are distance operators, see here and here .","title":"Spatial analytics: Questions"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#another-example-mapping-civilizations","text":"","title":"Another example: mapping civilizations"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#intro","text":"Recently this article was published: Spatializing 6,000 years of global urbanization from 3700 BC to AD 2000 Reba, M., Reitsma, F. and Seto, C. , 2016 The article describes all the cities since 3700 BC, including name, population and the position (latitude, longitude). We will use a subset ( chandlerV2 ) of the data for transforming it to a table, and then generating a geojson .","title":"Intro"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#uploading-the-data","text":"In the directory ./data/Historical Urban Population Growth Data cvslook chandlerV2.csv It will fail, because some encoding issues iconv -f iso-8859-1 -t utf-8 chandlerV2.csv > chandler_utf8.csv csvsql --db postgresql://dssg_gis:dssg-gis@localhost:8889/gis_tutorial \\ --insert chandlerV2_utf8.csv --table chandler --db-schema nanounanue SQL stuff select count(*) from chandler; -- How many cities do we have? New table for easier manipulation create table cities as -- CTAS select \"City\" as city, \"Country\" as country, \"Latitude\" as y_lat, \"Longitude\" as x_lon from chandler; Adding a geometry column and transform to Point alter table cities add column geom geometry(Point, 4326); -- Transforming Lon/Lat to Points update cities set geom = ST_SetSRID(ST_MakePoint(x_lon, y_lat), 4326); Converting to GeoJSON \\copy ( select row_to_json(fc) from ( select 'featurecollection' as type, array_to_json(array_agg(f)) as features from ( select 'feature' as type , st_asgeojson(cities.geom)::json as geometry , row_to_json( (select c from (select city, country) as c) ) as properties from cities ) as f ) as fc) to '~/cities.geojson'; This type of file could be used with d3.js for making interactive plots. For better performance you could use topojson","title":"Uploading the data"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/tutorial/#thank-you","text":"","title":"Thank you"},{"location":"curriculum/2_data_exploration_and_analysis/gis_analysis/postgis-workshop/data/Historical Urban Population Growth Data/","text":"Data from the article: Spatializing 6,000 years of global urbanization from 3700 BC to AD 2000 Located: http://www.nature.com/articles/sdata201634#data-records Last consulted: June 29, 2016","title":"Home"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/","text":"Git tutorial # What is git? (and why you should be using it for practically everything) # Image source Git is a free version control system which helps you keep track of file changes in your computer. Think of it as a time machine that lets you go back to any point in your project development. While git is most used in software development, you can use it for anything you like ( writing books , for example), as long as your files are plain text (e.g. source code, latex files), you won't have any issue with git (this guide is actually hosted using git, git-ception!). Simply speaking, git saves snapshots of your work called commits , after a commit is done, you can go back and forth to check the state of your project, maybe you were experimenting with some new function and realized the old one was better, no problem, you can bring back anything! Image source The entire development of your project is stored in your computer, but we know that's dangerous, so you can also host a remote copy (just like you do with Dropbox or Google Drive). What is github? # There are many , many providers that let you store your git repositories (that's how you call a git project ) but the most widely used is github (you'll be using it for dssg). Apart from storing a copy of your projects, github comes with a lot of useful features. For example, you can use it to share your projects with your colleagues, so they can see (or modify if you want) your project. git sounds awesome! How do I get it? # Chances are, git is already installed on your computer. If not, you can get it from here . OS X users: use homebrew, if you don't know what homebrew is, you probably didn't read the prerequisites :( Can I get buttons and stuff? # git is a command line tool, which means it doesn't have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote serve (like in dssg) is best way of doing it. However, if you still want a GUI (e.g. for using git in your computer), here are some options available: Options for Mac GitKraken (Windows and Mac) Ok, how do I do the magic? # Resources for beginners # 15 minute tutorial to learn git - This is a must for people to get started. git - the simple guide - A simple guide to get to know the most important concepts. Resouces for becoming a git ninja # A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. A git style guide , complete with branch naming, suggestions on how to handle commit messages, and more. READ THIS BEFORE YOU LEAVE (please) # By default, git saves everything inside the folder where you initiated the repo. When working on software projects there are files you DON'T want to save on git (e.g. database passwords, especially if you have a remote copy). To prevent git from saving files, create a file and name it .gitignore in the folder where you ran git init . In such file, you can add rules to let git know what you want it to ignore. For more information, read this . Besides sensitive data, you want to also ignore intermediate files generated automatically by some programming languages or libraries. There are templates available depending on the tools you use. There's also a nice command line tool to fetch such templates.","title":"Git tutorial"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#git-tutorial","text":"","title":"Git tutorial"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#what-is-git-and-why-you-should-be-using-it-for-practically-everything","text":"Image source Git is a free version control system which helps you keep track of file changes in your computer. Think of it as a time machine that lets you go back to any point in your project development. While git is most used in software development, you can use it for anything you like ( writing books , for example), as long as your files are plain text (e.g. source code, latex files), you won't have any issue with git (this guide is actually hosted using git, git-ception!). Simply speaking, git saves snapshots of your work called commits , after a commit is done, you can go back and forth to check the state of your project, maybe you were experimenting with some new function and realized the old one was better, no problem, you can bring back anything! Image source The entire development of your project is stored in your computer, but we know that's dangerous, so you can also host a remote copy (just like you do with Dropbox or Google Drive).","title":"What is git? (and why you should be using it for practically everything)"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#what-is-github","text":"There are many , many providers that let you store your git repositories (that's how you call a git project ) but the most widely used is github (you'll be using it for dssg). Apart from storing a copy of your projects, github comes with a lot of useful features. For example, you can use it to share your projects with your colleagues, so they can see (or modify if you want) your project.","title":"What is github?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#git-sounds-awesome-how-do-i-get-it","text":"Chances are, git is already installed on your computer. If not, you can get it from here . OS X users: use homebrew, if you don't know what homebrew is, you probably didn't read the prerequisites :(","title":"git sounds awesome! How do I get it?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#can-i-get-buttons-and-stuff","text":"git is a command line tool, which means it doesn't have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote serve (like in dssg) is best way of doing it. However, if you still want a GUI (e.g. for using git in your computer), here are some options available: Options for Mac GitKraken (Windows and Mac)","title":"Can I get buttons and stuff?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#ok-how-do-i-do-the-magic","text":"","title":"Ok, how do I do the magic?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#resources-for-beginners","text":"15 minute tutorial to learn git - This is a must for people to get started. git - the simple guide - A simple guide to get to know the most important concepts.","title":"Resources for beginners"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#resouces-for-becoming-a-git-ninja","text":"A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. A git style guide , complete with branch naming, suggestions on how to handle commit messages, and more.","title":"Resouces for becoming a git ninja"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/#read-this-before-you-leave-please","text":"By default, git saves everything inside the folder where you initiated the repo. When working on software projects there are files you DON'T want to save on git (e.g. database passwords, especially if you have a remote copy). To prevent git from saving files, create a file and name it .gitignore in the folder where you ran git init . In such file, you can add rules to let git know what you want it to ignore. For more information, read this . Besides sensitive data, you want to also ignore intermediate files generated automatically by some programming languages or libraries. There are templates available depending on the tools you use. There's also a nice command line tool to fetch such templates.","title":"READ THIS BEFORE YOU LEAVE  (please)"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/","text":"GitHub Continued # Agenda Branches Committing (and why it's important to commit OFTEN!) Merging Branches Finding (and fixing) bugs with commit history .gitignore Tags Pull Requests To get started, clone this repo . Branches # Branching is the way to work on different versions of a repository at one time. By default your repository has one branch named master which is considered to be the definitive branch. People use branches to experiment and make edits before committing them to master . When you create a branch off the master branch, you\u2019re making a copy, or snapshot, of master as it was at that point in time. If someone else made changes to the master branch while you were working on your branch, you could pull in those updates. This diagram shows: The master branch A new branch called feature (because we\u2019re doing \u2018feature work\u2019 on this branch) The journey that feature takes before it\u2019s merged into master Have you ever saved different versions of a file? Something like: story.txt story-mollie-edit.txt story-mollie-edit-reviewed.txt Branches accomplish similar goals in GitHub repositories. Creating a Branch # On Command line: # Create new branch with git branch [branchname] Move to the new branch with git checkout branchname Now you have two branches, master and your new branch. They look exactly the same, but not for long! Next we\u2019ll add our changes to the new branch. On Github: # Go to this practice repository . Click the drop down at the top of the file list that says branch: master . Type a branch name (perhaps your name?) into the new branch text box. Select the blue Create branch box or hit \u201cEnter\u201d on your keyboard. Making and Committing Changes # Now, you\u2019re on the code view for your new branch, which is a copy of master . Let\u2019s make some edits. On GitHub, saved changes are called commits. Each commit has an associated commit message, which is a description explaining why a particular change was made. Commit messages capture the history of your changes, so other contributors can understand what you\u2019ve done and why. Some reasons why COMMITTING OFTEN IS IMPORTANT : - Committing often makes each bit-size change easy to parse for your teammates - If bugs appear, you can checkout past versions and compare the commit history. If you are committing often, it will be much easier to find where the bug krept in. Rid yourself of your fear of committment! There are five practice files in the practice repo . Let's split up which document we'll work with based on rows. On the Command Line: # Open your practice markdown file Make a change somewhere in the document. Perhaps you want to profess your love for your favorite sweets, or add some new confections to the cupcake ipsum . Or, go crazy and just make your own file with whatever gibberish you like. Add file to staging - - git add * Commit your changes - - git commit -m 'some description of edit I just made' Push - git push On Github: # Click the file. Click the pencil icon in the upper right corner of the file view to edit. In the editor, write a bit about yourself. Write a commit message that describes your changes. Click Commit changes button. Merging branches # On Command Line: # Switch to master - git checkout master Merge the edits from your branch - git merge [branchname] Delete old branchname - git branch -d [branchname] ( note: don't delete your branch yet ) Using commit history to find bugs # Why is committing so important you might ask? Well, one reason is that you can use your commit history to find bugs!!! Let's take a look at how that might be done. Run git log to see your commit history. From here, you can use the version commit number to checkout different versions of your code. To explore a past version, you can simply run git checkout [revision] where revision is the commit hash (for example: 12345678901234567890123456789012345678ab). You are now viewing the code as it was during that version! Let's say a bug has crept into the code at some point, and you're trying to find where that happened. You can simply checkout various versions of your code, run the buggy file, and find the point at which the bug was introduced. Then, you'll want to look at the difference between the last version where the code was not present, and the first version where it was introduced. You can check out these differences by running git diff [revision1] [revision2] . (e.g. git diff d8f59d0d27a50adba45a54fd50047ea94d6effcf 12f74c691786445d675f1b45ed244778eb3328dd ) Something to keep in mind is that if you are not committing often (for example, committing every few days instead of multiple times a day), this process will be unhelpful because the differences will be so large and the commit messages will not be very meaningful/helpful. Make sure to commit often with informative commit messages! :) Return to your most recent version with: git checkout master .gitignore # .gitignore files specify which files are ignored in a git repository. Example: #ignore a single file `mycode.class` #ignore an entire directory `/mydebugdir/` #ignore a file type `*.json` #add an exception (using !) to the preceding rule to track a specific file `!package.json` Let's create an empty .gitignore file (touch .gitignore ) and add some stuff! Tags # This summer, we'd like you to use weekly tags so that future mentors/managers can look back at projects to see where people were during certain weeks for reference. To create a tag run git tag <tagname> . This will create a local tag with the current state of the branch you are on. When pushing to your remote repo, tags are NOT included by default. You will need to explicitly say that you want to push your tags to your remote repo. To push your tag run git push origin <tag> . Or to push all tags (in the case there are multiple), you'd run git push origin --tags . In our case, we'll just be working with on at a time. Create and push a new weekly tag each Monday morning. Pull Requests # Pull Requests are the heart of collaboration on GitHub. When you open a pull request, you\u2019re proposing your changes and requesting that someone review and pull in your contribution and merge them into their branch. Pull requests show diffs, or differences, of the content from both branches. The changes, additions, and subtractions are shown in green and red. As soon as you make a commit, you can open a pull request and start a discussion, even before the code is finished. By using GitHub\u2019s @mention system in your pull request message, you can ask for feedback from specific people or teams, whether they\u2019re down the hall or 10 time zones away. You can even open pull requests in your own repository and merge them yourself. It\u2019s a great way to learn the GitHub Flow before working on larger projects. Click the Pull Request tab, then from the Pull Request page, click the green New pull request button. In the Example comparisons box, select hte branch you made to compare with master (the original). Look over your changes in the diffs on the compare page to make sure they're what you want to submit. When you're satisfied these are the changes you want to submit, click the green Create Pull Request button. Give your pull request a title and write a brief description of your changes. Click Create pull request ! Merging a Pull Request # In this final step, it\u2019s time to bring your changes together \u2013 merging your readme-edits branch into the master branch. Click on the green Merge pull request button to merge the changes into master Click confirm merge Since you're now done with this new branch, you can delete the branch by clicking the Delete branch button. Other stuff: # There are a bunch of these out there, but here is one that you might find useful on your journey. Oh yeah, and some of this is shamelessly stolen from this post . :)","title":"GitHub Continued"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#github-continued","text":"Agenda Branches Committing (and why it's important to commit OFTEN!) Merging Branches Finding (and fixing) bugs with commit history .gitignore Tags Pull Requests To get started, clone this repo .","title":"GitHub Continued"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#branches","text":"Branching is the way to work on different versions of a repository at one time. By default your repository has one branch named master which is considered to be the definitive branch. People use branches to experiment and make edits before committing them to master . When you create a branch off the master branch, you\u2019re making a copy, or snapshot, of master as it was at that point in time. If someone else made changes to the master branch while you were working on your branch, you could pull in those updates. This diagram shows: The master branch A new branch called feature (because we\u2019re doing \u2018feature work\u2019 on this branch) The journey that feature takes before it\u2019s merged into master Have you ever saved different versions of a file? Something like: story.txt story-mollie-edit.txt story-mollie-edit-reviewed.txt Branches accomplish similar goals in GitHub repositories.","title":"Branches"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#creating-a-branch","text":"","title":"Creating a Branch"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#on-command-line","text":"Create new branch with git branch [branchname] Move to the new branch with git checkout branchname Now you have two branches, master and your new branch. They look exactly the same, but not for long! Next we\u2019ll add our changes to the new branch.","title":"On Command line:"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#on-github","text":"Go to this practice repository . Click the drop down at the top of the file list that says branch: master . Type a branch name (perhaps your name?) into the new branch text box. Select the blue Create branch box or hit \u201cEnter\u201d on your keyboard.","title":"On Github:"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#making-and-committing-changes","text":"Now, you\u2019re on the code view for your new branch, which is a copy of master . Let\u2019s make some edits. On GitHub, saved changes are called commits. Each commit has an associated commit message, which is a description explaining why a particular change was made. Commit messages capture the history of your changes, so other contributors can understand what you\u2019ve done and why. Some reasons why COMMITTING OFTEN IS IMPORTANT : - Committing often makes each bit-size change easy to parse for your teammates - If bugs appear, you can checkout past versions and compare the commit history. If you are committing often, it will be much easier to find where the bug krept in. Rid yourself of your fear of committment! There are five practice files in the practice repo . Let's split up which document we'll work with based on rows.","title":"Making and Committing Changes"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#on-the-command-line","text":"Open your practice markdown file Make a change somewhere in the document. Perhaps you want to profess your love for your favorite sweets, or add some new confections to the cupcake ipsum . Or, go crazy and just make your own file with whatever gibberish you like. Add file to staging - - git add * Commit your changes - - git commit -m 'some description of edit I just made' Push - git push","title":"On the Command Line:"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#on-github_1","text":"Click the file. Click the pencil icon in the upper right corner of the file view to edit. In the editor, write a bit about yourself. Write a commit message that describes your changes. Click Commit changes button.","title":"On Github:"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#merging-branches","text":"","title":"Merging branches"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#on-command-line_1","text":"Switch to master - git checkout master Merge the edits from your branch - git merge [branchname] Delete old branchname - git branch -d [branchname] ( note: don't delete your branch yet )","title":"On Command Line:"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#using-commit-history-to-find-bugs","text":"Why is committing so important you might ask? Well, one reason is that you can use your commit history to find bugs!!! Let's take a look at how that might be done. Run git log to see your commit history. From here, you can use the version commit number to checkout different versions of your code. To explore a past version, you can simply run git checkout [revision] where revision is the commit hash (for example: 12345678901234567890123456789012345678ab). You are now viewing the code as it was during that version! Let's say a bug has crept into the code at some point, and you're trying to find where that happened. You can simply checkout various versions of your code, run the buggy file, and find the point at which the bug was introduced. Then, you'll want to look at the difference between the last version where the code was not present, and the first version where it was introduced. You can check out these differences by running git diff [revision1] [revision2] . (e.g. git diff d8f59d0d27a50adba45a54fd50047ea94d6effcf 12f74c691786445d675f1b45ed244778eb3328dd ) Something to keep in mind is that if you are not committing often (for example, committing every few days instead of multiple times a day), this process will be unhelpful because the differences will be so large and the commit messages will not be very meaningful/helpful. Make sure to commit often with informative commit messages! :) Return to your most recent version with: git checkout master","title":"Using commit history to find bugs"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#gitignore","text":".gitignore files specify which files are ignored in a git repository. Example: #ignore a single file `mycode.class` #ignore an entire directory `/mydebugdir/` #ignore a file type `*.json` #add an exception (using !) to the preceding rule to track a specific file `!package.json` Let's create an empty .gitignore file (touch .gitignore ) and add some stuff!","title":".gitignore"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#tags","text":"This summer, we'd like you to use weekly tags so that future mentors/managers can look back at projects to see where people were during certain weeks for reference. To create a tag run git tag <tagname> . This will create a local tag with the current state of the branch you are on. When pushing to your remote repo, tags are NOT included by default. You will need to explicitly say that you want to push your tags to your remote repo. To push your tag run git push origin <tag> . Or to push all tags (in the case there are multiple), you'd run git push origin --tags . In our case, we'll just be working with on at a time. Create and push a new weekly tag each Monday morning.","title":"Tags"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#pull-requests","text":"Pull Requests are the heart of collaboration on GitHub. When you open a pull request, you\u2019re proposing your changes and requesting that someone review and pull in your contribution and merge them into their branch. Pull requests show diffs, or differences, of the content from both branches. The changes, additions, and subtractions are shown in green and red. As soon as you make a commit, you can open a pull request and start a discussion, even before the code is finished. By using GitHub\u2019s @mention system in your pull request message, you can ask for feedback from specific people or teams, whether they\u2019re down the hall or 10 time zones away. You can even open pull requests in your own repository and merge them yourself. It\u2019s a great way to learn the GitHub Flow before working on larger projects. Click the Pull Request tab, then from the Pull Request page, click the green New pull request button. In the Example comparisons box, select hte branch you made to compare with master (the original). Look over your changes in the diffs on the compare page to make sure they're what you want to submit. When you're satisfied these are the changes you want to submit, click the green Create Pull Request button. Give your pull request a title and write a brief description of your changes. Click Create pull request !","title":"Pull Requests"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#merging-a-pull-request","text":"In this final step, it\u2019s time to bring your changes together \u2013 merging your readme-edits branch into the master branch. Click on the green Merge pull request button to merge the changes into master Click confirm merge Since you're now done with this new branch, you can delete the branch by clicking the Delete branch button.","title":"Merging a Pull Request"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/github_continued/#other-stuff","text":"There are a bunch of these out there, but here is one that you might find useful on your journey. Oh yeah, and some of this is shamelessly stolen from this post . :)","title":"Other stuff:"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/01_BasicGit/","text":"Git Overview # The tutorial will give a quick overview of what git is, how to use it and how to work as a team using the GitHub workflow. For this tutorial you will need to have access to Git, a Terminal and a Text Editor. Git in a nutshell # Image source Git is a version control system which helps you keep track changes in files you make during the development of your project. Think of it as a lab notebook that lets you go back-and-forth to any point in your project, an undo button, and a tool to safely and efficiently collaborate with others on a shared project. All serious software projects use version control. While git is mostly used in software development, it can be used for anything you like ( writing books , for example), as long as your files are plain text (e.g., source code, latex files). Simply speaking, git saves snapshots of your work called commits ; after a commit is created, you can go back and forth through different commits in your project -- maybe you were experimenting with some new function and realized the old function was better, no problem, you can bring back anything! The collections of commits and associated metadata form the repository of your project. Image source The entire development of your project, the repository , is stored on your computer, but we know that's dangerous, so you can also host a remote copy on a server like GitHub, Bitbucket, or GitLab. Hosting a project's repository on GitHub also allows for the distribution of your work and collaboration. This prevents endless emailing of source code and the following situation: git sounds awesome! How do I get it? # Chances are, git is already installed on your computer. To check open-up a terminal and type git . If not, you can get it from here . OS X users: use homebrew Can I get buttons and stuff? # git is a command line tool, which means it doesn't natively have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote server you will unlikely be able to use a GUI. However, if you still want a GUI (e.g., for using git on your computer), here are some options available: Options for Mac GitKraken (Windows and Mac) Keep in mind that if you are logging into a remote machine like AWS. A GUI may not be an option. This tutorial will cover the basics of git and hosting a project on GitHub. Configure your Git Profile # First things first. You need to configure you git client so your commits are attributed to you and you get pretty output. Do the following: # How my git configuration currently look like git config --list My workspace # Adding some, if you don't have a user.name or user.email set git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor 'nano' #or vim, emacs sublime For a list of text editors see Software Carpentry's list Also do the following (important). Ask about this during the branching section of the tutorial if you want to know more. git config --global push.default current You now have your git client configured. Next we will create our first repository.","title":"Git Overview"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/01_BasicGit/#git-overview","text":"The tutorial will give a quick overview of what git is, how to use it and how to work as a team using the GitHub workflow. For this tutorial you will need to have access to Git, a Terminal and a Text Editor.","title":"Git Overview"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/01_BasicGit/#git-in-a-nutshell","text":"Image source Git is a version control system which helps you keep track changes in files you make during the development of your project. Think of it as a lab notebook that lets you go back-and-forth to any point in your project, an undo button, and a tool to safely and efficiently collaborate with others on a shared project. All serious software projects use version control. While git is mostly used in software development, it can be used for anything you like ( writing books , for example), as long as your files are plain text (e.g., source code, latex files). Simply speaking, git saves snapshots of your work called commits ; after a commit is created, you can go back and forth through different commits in your project -- maybe you were experimenting with some new function and realized the old function was better, no problem, you can bring back anything! The collections of commits and associated metadata form the repository of your project. Image source The entire development of your project, the repository , is stored on your computer, but we know that's dangerous, so you can also host a remote copy on a server like GitHub, Bitbucket, or GitLab. Hosting a project's repository on GitHub also allows for the distribution of your work and collaboration. This prevents endless emailing of source code and the following situation:","title":"Git in a nutshell"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/01_BasicGit/#git-sounds-awesome-how-do-i-get-it","text":"Chances are, git is already installed on your computer. To check open-up a terminal and type git . If not, you can get it from here . OS X users: use homebrew","title":"git sounds awesome! How do I get it?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/01_BasicGit/#can-i-get-buttons-and-stuff","text":"git is a command line tool, which means it doesn't natively have a graphical user interface. Using the git cli is the most flexible way of working with git, and if you are working on a remote server you will unlikely be able to use a GUI. However, if you still want a GUI (e.g., for using git on your computer), here are some options available: Options for Mac GitKraken (Windows and Mac) Keep in mind that if you are logging into a remote machine like AWS. A GUI may not be an option. This tutorial will cover the basics of git and hosting a project on GitHub.","title":"Can I get buttons and stuff?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/01_BasicGit/#configure-your-git-profile","text":"First things first. You need to configure you git client so your commits are attributed to you and you get pretty output. Do the following: # How my git configuration currently look like git config --list My workspace # Adding some, if you don't have a user.name or user.email set git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor 'nano' #or vim, emacs sublime For a list of text editors see Software Carpentry's list Also do the following (important). Ask about this during the branching section of the tutorial if you want to know more. git config --global push.default current You now have your git client configured. Next we will create our first repository.","title":"Configure your Git Profile"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/02_CreateRepo/","text":"A# Create a Repository Let's work on creating our first git repository. All shell commands will be prefixed with > All comments, text not meant to be parsed by the computer, is prefixed with # . Create a git repository # First create a directory for our project and cd into the directory > mkdir -v nyc-311 > cd nyc-311 The -v flag produces verbose output so you can see what happened after the invocation of the command. Now lets initialize the git repository. All git commands start with git <verb> . To initialize the git repo for our project we invoke the command > git init Let's look at the contents of our directory using the command ls -a > ls -a . .. .git/ The -a flag tells the ls command to include hidden directories when displaying files. We can see that there is now a .git directory. Unless you really know what you are doing DO NOT EVER modify anything in this directory. If you delete this directory, the entire history of your project will be gone. Make our first commit!! # All good projects should have a README.md to describe the project. So let's start with that. Fire up you favorite text editor and name a file README.md . Add something like this in your README: # Exploring 311 Calls in NYC ## Description This repo is for an analysis of 311 calls in NYC using Python 3.4 The # are part of markdown syntax for designating headings. Now let's look at the status of our repo using git status : > git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We can see that git knows that we have added a file but it is untracked . What this means is that git knows that this file has been added but it is currently \"untracked\" by git. Like the command said let's use git add to track changes in the the file by invoking the command git add README.md . > git add README.md Now lets look at the status of the repository by invoking the command git status again. > git status On branch master Initial commit Changes to be committed: (use \"git rm --cached <file>...\" to unstage) new file: README.md Now that we have added the file it has been \"staged\" in the staging area. We can now make our first commit! > git commit When you invoke this command an editor should pop-up and you have to leave a commit message. Good commit messages lead to a usable git log and separates the novice git users from the competent practitioners. Generally you should follow these guidelines in a commit message: First line is a one line summary of the commit that is in title case and less then 80 characters, written in the imperative voice. Second line should be blank. Third and subsequent lines should be more details of the commit. Anyone can look at a commit and examine what was changed. The commit message is where you provide a context of what and why you did what you did in the project. A good rule of thumb is If applied this commit will, My commit message is the following: Checking in README file * Added short description of the project * Added python3 as a dependency ### Using Nano as a text editor If you are using nano, to save your text use Ctrl-O, write a filename and to exit use Ctrl-X. Now that we have made our first commit we can examine our log! > git log The first line should output * commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 05 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency The first line is a unique identifier of your commit. The second give information on who made the commit. The third line gives the date. The rest is the commit message. To just get titles of commit messages you can use the following command > git log --oneline Another useful command is > git log --oneline --graph --all --decorate Some helpful commands # Removing files via git allows them to be recovered. git rm FILENAME git rm -r DIRECTORY Renaming files can be done by moving a file and then adding it or you can use the following command: git mv OLD NEW git rm and git mv will stage the changes that will ulimately need to be commited. That is it we have our first commit and repo! Next we are going to write some code!","title":"02 CreateRepo"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/02_CreateRepo/#create-a-git-repository","text":"First create a directory for our project and cd into the directory > mkdir -v nyc-311 > cd nyc-311 The -v flag produces verbose output so you can see what happened after the invocation of the command. Now lets initialize the git repository. All git commands start with git <verb> . To initialize the git repo for our project we invoke the command > git init Let's look at the contents of our directory using the command ls -a > ls -a . .. .git/ The -a flag tells the ls command to include hidden directories when displaying files. We can see that there is now a .git directory. Unless you really know what you are doing DO NOT EVER modify anything in this directory. If you delete this directory, the entire history of your project will be gone.","title":"Create a git repository"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/02_CreateRepo/#make-our-first-commit","text":"All good projects should have a README.md to describe the project. So let's start with that. Fire up you favorite text editor and name a file README.md . Add something like this in your README: # Exploring 311 Calls in NYC ## Description This repo is for an analysis of 311 calls in NYC using Python 3.4 The # are part of markdown syntax for designating headings. Now let's look at the status of our repo using git status : > git status On branch master Initial commit Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We can see that git knows that we have added a file but it is untracked . What this means is that git knows that this file has been added but it is currently \"untracked\" by git. Like the command said let's use git add to track changes in the the file by invoking the command git add README.md . > git add README.md Now lets look at the status of the repository by invoking the command git status again. > git status On branch master Initial commit Changes to be committed: (use \"git rm --cached <file>...\" to unstage) new file: README.md Now that we have added the file it has been \"staged\" in the staging area. We can now make our first commit! > git commit When you invoke this command an editor should pop-up and you have to leave a commit message. Good commit messages lead to a usable git log and separates the novice git users from the competent practitioners. Generally you should follow these guidelines in a commit message: First line is a one line summary of the commit that is in title case and less then 80 characters, written in the imperative voice. Second line should be blank. Third and subsequent lines should be more details of the commit. Anyone can look at a commit and examine what was changed. The commit message is where you provide a context of what and why you did what you did in the project. A good rule of thumb is If applied this commit will, My commit message is the following: Checking in README file * Added short description of the project * Added python3 as a dependency ### Using Nano as a text editor If you are using nano, to save your text use Ctrl-O, write a filename and to exit use Ctrl-X. Now that we have made our first commit we can examine our log! > git log The first line should output * commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 05 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency The first line is a unique identifier of your commit. The second give information on who made the commit. The third line gives the date. The rest is the commit message. To just get titles of commit messages you can use the following command > git log --oneline Another useful command is > git log --oneline --graph --all --decorate","title":"Make our first commit!!"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/02_CreateRepo/#some-helpful-commands","text":"Removing files via git allows them to be recovered. git rm FILENAME git rm -r DIRECTORY Renaming files can be done by moving a file and then adding it or you can use the following command: git mv OLD NEW git rm and git mv will stage the changes that will ulimately need to be commited. That is it we have our first commit and repo! Next we are going to write some code!","title":"Some helpful commands"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/03_UsingTheGitLog/","text":"Using the Git Log # In this portion of the tutorial we are going to explore the git log and how to go back to prior parts of our project. We left off creating a README file. Now the time has come to actually write some code. Create a python script to output results of analyzing 311 calls # Let's download some data to work with using the following command you can copy and paste. > curl -O https://raw.githubusercontent.com/avishekrk/pandas-cookbook/master/data/311-service-requests.csv Fire up your favorite text editor and let's write a little program called descriptive_stats.py to get the most common complaints from 311 data in NYC. If you are using nano invoke the command > nano descriptive_stats.py Add the following in the text. Don't worry you don't need to know what this means right now. We are loading some data into and finding the top 5 categories of complaints using 311 data. from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print( df_311_calls['Complaint Type'].value_counts()[:5]) Remember, to save the program in nano the command is Ctrl-O and the to exit is Ctrl-X . We have just created a python program. We can run the program using the following syntax. > python <program name> In our case we run the following command: > python descriptive_stats.py When we run the program we should get the following output HEATING 14200 GENERAL CONSTRUCTION 7471 Street Light Condition 7117 DOF Literature Request 5797 PLUMBING 5373 Name: Complaint Type, dtype: int64 These is the top 5 311 complaints for 2010 in NYC. Now that we have a working program lets commit it to the repo, just like before. > git add descriptive_stats.py # to the staging area > git commit -m \"Checking in descriptive_stats.py, output top 5 311 complaints\" In this case rather then launching a text editor to write a commit message we used the -m option to make an in-line commit message. Our change doesn't require a lengthy commit message. Now we should have a commited version of descriptive stats. Let's now decide that we want the top 10 311 complaints and modify our program to output the top 10 results. Our current program should now be: from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') print( df_311_calls['Complaint Type'].value_counts[:10]) Let's commit that change: git add descriptive_stats.py git commit -m \"Changed the top 5 results to the top 10 results in descriptive_stats.py\" If we look at our git log we should new be able to see history of our changes: > git log commit 42c35933c4d52708c2562c1c05361b152a2b9230 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:55:29 2016 -0600 Changed the top 5 results to the top 10 results in descriptive_stats.py commit ab85797b2c3d68fb0c97535080079138888b5556 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:52:52 2016 -0600 Checking-in descriptive_stats.py outputs the top 5 311 complaints commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency Now let's look at the difference between the two commits in the log using the git diff command. The git diff command is important for seeing changes in your source code and comparing one commit against another. If we now invoke the command > git diff HEAD~1 diff --git a/descriptive_stats.py b/descriptive_stats.py index 09b7168..c38d3e3 100644 --- a/descriptive_stats.py +++ b/descriptive_stats.py @@ -3,4 +3,4 @@ import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') -print( df_311_calls['Complaint Type'].value_counts()[:5] ) +print( df_311_calls['Complaint Type'].value_counts()[:10] ) First HEAD is shorthand for the latest commit in the repository. HEAD~1 is a shorthand for the lastest commit minus one. For instance HEAD~20 refers to the a commit 20 commits ago. The output of the diff file is the following. The first line looks similiar to a diff command. The second line shows the commit identifiers of the two commits being compared. The next two lines are the files being compared. The interesting part is at the bottom. The line with the - sign is our prior commit. The line with a + sign the the current commit. We can see the difference is the change between 5 to 10. We can \"checkout\" old versions of our files using the checkout command. This is a very handy feature for when we break something and want to start from a working copy or if we have an old feature that has since been discarded it can be restored. Let's now go back to our prior commit using the git checkout command git checkout HEAD~1 descriptive_stats.py If we look at our file descriptive_stats.py we will have reverted back to our old version. git checkout HEAD descriptive_stats.py Another useful git diff command is: git diff --staged where we can examine the differences between files that have been staged for commit and the last commit. Next up we are going to go over how to host a project on GitHub.","title":"Using the Git Log"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/03_UsingTheGitLog/#using-the-git-log","text":"In this portion of the tutorial we are going to explore the git log and how to go back to prior parts of our project. We left off creating a README file. Now the time has come to actually write some code.","title":"Using the Git Log"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/03_UsingTheGitLog/#create-a-python-script-to-output-results-of-analyzing-311-calls","text":"Let's download some data to work with using the following command you can copy and paste. > curl -O https://raw.githubusercontent.com/avishekrk/pandas-cookbook/master/data/311-service-requests.csv Fire up your favorite text editor and let's write a little program called descriptive_stats.py to get the most common complaints from 311 data in NYC. If you are using nano invoke the command > nano descriptive_stats.py Add the following in the text. Don't worry you don't need to know what this means right now. We are loading some data into and finding the top 5 categories of complaints using 311 data. from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print( df_311_calls['Complaint Type'].value_counts()[:5]) Remember, to save the program in nano the command is Ctrl-O and the to exit is Ctrl-X . We have just created a python program. We can run the program using the following syntax. > python <program name> In our case we run the following command: > python descriptive_stats.py When we run the program we should get the following output HEATING 14200 GENERAL CONSTRUCTION 7471 Street Light Condition 7117 DOF Literature Request 5797 PLUMBING 5373 Name: Complaint Type, dtype: int64 These is the top 5 311 complaints for 2010 in NYC. Now that we have a working program lets commit it to the repo, just like before. > git add descriptive_stats.py # to the staging area > git commit -m \"Checking in descriptive_stats.py, output top 5 311 complaints\" In this case rather then launching a text editor to write a commit message we used the -m option to make an in-line commit message. Our change doesn't require a lengthy commit message. Now we should have a commited version of descriptive stats. Let's now decide that we want the top 10 311 complaints and modify our program to output the top 10 results. Our current program should now be: from __future__ import print_function import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') print( df_311_calls['Complaint Type'].value_counts[:10]) Let's commit that change: git add descriptive_stats.py git commit -m \"Changed the top 5 results to the top 10 results in descriptive_stats.py\" If we look at our git log we should new be able to see history of our changes: > git log commit 42c35933c4d52708c2562c1c05361b152a2b9230 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:55:29 2016 -0600 Changed the top 5 results to the top 10 results in descriptive_stats.py commit ab85797b2c3d68fb0c97535080079138888b5556 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 16:52:52 2016 -0600 Checking-in descriptive_stats.py outputs the top 5 311 complaints commit aaf89fd77e9b43d99fe32823843a7519b2108c90 Author: Clark Kent <clark.kent@dailyplanet.com> Date: Sat Nov 12 13:45:11 2016 -0600 Checking in README file * Added short description of the project * Added python3 as a dependency Now let's look at the difference between the two commits in the log using the git diff command. The git diff command is important for seeing changes in your source code and comparing one commit against another. If we now invoke the command > git diff HEAD~1 diff --git a/descriptive_stats.py b/descriptive_stats.py index 09b7168..c38d3e3 100644 --- a/descriptive_stats.py +++ b/descriptive_stats.py @@ -3,4 +3,4 @@ import pandas as pd fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv('311-service-requests.csv') -print( df_311_calls['Complaint Type'].value_counts()[:5] ) +print( df_311_calls['Complaint Type'].value_counts()[:10] ) First HEAD is shorthand for the latest commit in the repository. HEAD~1 is a shorthand for the lastest commit minus one. For instance HEAD~20 refers to the a commit 20 commits ago. The output of the diff file is the following. The first line looks similiar to a diff command. The second line shows the commit identifiers of the two commits being compared. The next two lines are the files being compared. The interesting part is at the bottom. The line with the - sign is our prior commit. The line with a + sign the the current commit. We can see the difference is the change between 5 to 10. We can \"checkout\" old versions of our files using the checkout command. This is a very handy feature for when we break something and want to start from a working copy or if we have an old feature that has since been discarded it can be restored. Let's now go back to our prior commit using the git checkout command git checkout HEAD~1 descriptive_stats.py If we look at our file descriptive_stats.py we will have reverted back to our old version. git checkout HEAD descriptive_stats.py Another useful git diff command is: git diff --staged where we can examine the differences between files that have been staged for commit and the last commit. Next up we are going to go over how to host a project on GitHub.","title":"Create a python script to output results of analyzing 311 calls"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/","text":"GitHub # This part of the tutorial will go over how to host your project on GitHub The virtue of using a .gitignore file The GitHub workflow How to solve a merge conflict What is GitHub? # There are several popular providers -- Bitbucket , GitLab -- that let you store your git repositories but the most widely used is GitHub . Apart from storing a copy of your projects, GitHub comes with a lot of useful features. For example, you can use it to share your projects with your colleagues so they can see or modify your project. Is is also a collaboration tool that you can use to work as a part of a team. Getting started on GitHub? # Go to https://github.com and create a free account. Create a new repository called NYC-311 The repository URL will then be https://github.com/username/NYC-311 Add the remote repository in your local repository > git remote add origin https://github.com/username/nyc-311.git You can then see the remote repository with the following command > git remote -v The remote is named \"origin\" which is a common choice for the primary repository. Now we are going to push the local changes on our repository to the GitHub repository using the command > git push --set-upstream origin master What we have done is taken a copy of our repository and pushed it to GitHub. When you push changes to GitHub in the future you just need to use the command git push origin master To pull in changes in your repository done by yourself or another person you can use the following command git pull origin master To see this in action grab a friend and have them clone your repo with the command. First make sure they also have a GitHub account and add them as a collaborator to your repository. If you can't find a friend you can still do this part of the tutorial just skip the parts about using git clone and git pull . > git clone https://github.com/username/nyc-311.git They will then have a copy of your repository. Now cd into the project folder and add a special file called .gitignore . Fire up a text editor and add the following #Ignore pyc files *.pyc The .gitignore file is a special file that git looks at when trying to make a commit. We added the entry '*.pyc'. This is an instruction for git to not commit files that end in .pyc . Files that end in pyc are python bytecode files that will appear when you run your python code. All lines prefixed with # are comments and will be ignored; they are meant to be read by people not computers. Now lets add this file to our repository git add .gitignore git commit -m \"Added .gitignore\" And push our changes to the repository git push origin master Now on your machine git pull the new state of the repository git pull origin master If we then use the command ls -a you should then see the .gitignore file. And if we look at the log using 'git log` you should see the new commit. GitHub Flow # In this portion of the tutorial we will go over branching and the general GitHub workflow. So far we have been doing the \"solo\" workflow which consists of the following: mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" git push origin master # hack # more hack We are now going to introduce the GitHub flow that is largely done in teams. In the GitHub flow we never code anything unless there is a need to. When there is a need we then create an issue on the GitHub repository. Good issues should be Clear Defined output Actionable (written in the Imperative Voice) Could be completed at most in few days Examples Good : /Fix the bug in .../ Good : /Add a method that does .../ Bad : /Solve the project/ Bad : /Some error happen/ Here is how to create a GitHub issue. Once we have an issue we will then pull from the repo and create a branch . A branch is a copy of the code base separate from the main master branch where we can work on our issue (e.g, fixing a bug, adding a feature) without affecting the master branch during our work and then ultimately merge our change into the master branch. The flow goes something like this: ## Pull from the repo git pull ## Decide what do you want to do and create an issue git checkout -b a-meaningful-name The command git checkout -b creates a new branch which in this case is named \"a-meaningful-name\". We can see what branch we are on by using the command git branch which will show all the branches in the local repository and place an * next to the branch we are currently on. ## hack, hack, hack, add/rm, commit ## Push to the repo and create a remote branch git push ## Create a pull-request and describe your work (Suggest/add a reviewer) ## Someone then reviews your code ## The pull-request is closed and the remote branch is destroyed ## Switch to master locally git checkout master ## Pull the most recent changes (including yours) git pull ## Delete your local branch git branch -d a-meaningful-name Here is how to create a GitHub pull request. Solving a merge conflict # As you work on projects with others you will inevitably run into merge conflicts. A merge conflict is caused when you and another person edits the same line of a file. Git will not know which line is the correct one and create a conflict. Let's make a conflict! #create a branch called drama > git checkout -b drama #now modify the descriptive_stats.py file and change the top 10 #values to top 13 #commit your changes > git add descriptive_stats.py > git commit -m \"Changed top 10 to top 13 in descriptive_stats.py #switch back to master > git checkout master #now modify the descriptive_stats.py file and change the top 10 #values to top 3 # commit your changes > git add descriptive_stats.py > git commit -m \"Changed the top 10 to the top 3 in descriptive_stats.py\" Now we are going to merge the drama branch into the master branch > git merge drama Auto-merging descriptive_stats.py CONFLICT (content): Merge conflict in descriptive_stats.py Automatic merge failed; fix conflicts and then commit the result. Our arbitrary drama has now lead to a conflict. If we check the status we should see the following On branch master You have unmerged paths. (fix conflicts and run \"git commit\") Unmerged paths: (use \"git add <file>...\" to mark resolution) both modified: descriptive_stats.py Let's examine the conflicting file descriptive_stats.py , you should see the following: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) <<<<<<< HEAD print(df_311_calls['Complaint Type'].value_counts()[:3]) ======= print(df_311_calls['Complaint Type'].value_counts()[:13]) >>>>>>> drama The >>>>>>> and <<<<<<< denote the section of the conflicting code. HEAD means the following line if from the maser branch while drama shows that the preceding line is from the drama branch. The lines of the two branches are separated by the ======= . Given a merge conflict we have three choices: either keep the line from the master branch, keep the line from the drama branch or create an entirely new line. In this case we are going to keep the line from the master branch. To do that we delete the merge conflict markers and the line from the drama branch and then make a commit. Your code should look like this: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print(df_311_calls['Complaint Type'].value_counts()[:3]) Now add this file and make a commit. git add descriptive_stats.py git commit -m \"Fixed merge conflict in descriptive stats\" Our merge conflict is know solved. That concludes the tutorial on GitHub! Good Luck! Acknowledgements, References and Further Resources # This tutorial is derived from tutorials created by Eduardo Blancas Reyes, Benedict Kuester, Adolfo De Unanue, Software Carpentry , and ASU PHY-494 . Furthur resources for becoming a git master are: * Software Carpentry -- a more in-depth intro tutorial 15 minute tutorial to learn git - Intro tutorial. git - the simple guide - A simple guide to get to know the most important concepts. A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. Pro Git -- An in-depth discussion written by git masters.","title":"GitHub"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/#github","text":"This part of the tutorial will go over how to host your project on GitHub The virtue of using a .gitignore file The GitHub workflow How to solve a merge conflict","title":"GitHub"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/#what-is-github","text":"There are several popular providers -- Bitbucket , GitLab -- that let you store your git repositories but the most widely used is GitHub . Apart from storing a copy of your projects, GitHub comes with a lot of useful features. For example, you can use it to share your projects with your colleagues so they can see or modify your project. Is is also a collaboration tool that you can use to work as a part of a team.","title":"What is GitHub?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/#getting-started-on-github","text":"Go to https://github.com and create a free account. Create a new repository called NYC-311 The repository URL will then be https://github.com/username/NYC-311 Add the remote repository in your local repository > git remote add origin https://github.com/username/nyc-311.git You can then see the remote repository with the following command > git remote -v The remote is named \"origin\" which is a common choice for the primary repository. Now we are going to push the local changes on our repository to the GitHub repository using the command > git push --set-upstream origin master What we have done is taken a copy of our repository and pushed it to GitHub. When you push changes to GitHub in the future you just need to use the command git push origin master To pull in changes in your repository done by yourself or another person you can use the following command git pull origin master To see this in action grab a friend and have them clone your repo with the command. First make sure they also have a GitHub account and add them as a collaborator to your repository. If you can't find a friend you can still do this part of the tutorial just skip the parts about using git clone and git pull . > git clone https://github.com/username/nyc-311.git They will then have a copy of your repository. Now cd into the project folder and add a special file called .gitignore . Fire up a text editor and add the following #Ignore pyc files *.pyc The .gitignore file is a special file that git looks at when trying to make a commit. We added the entry '*.pyc'. This is an instruction for git to not commit files that end in .pyc . Files that end in pyc are python bytecode files that will appear when you run your python code. All lines prefixed with # are comments and will be ignored; they are meant to be read by people not computers. Now lets add this file to our repository git add .gitignore git commit -m \"Added .gitignore\" And push our changes to the repository git push origin master Now on your machine git pull the new state of the repository git pull origin master If we then use the command ls -a you should then see the .gitignore file. And if we look at the log using 'git log` you should see the new commit.","title":"Getting started on GitHub?"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/#github-flow","text":"In this portion of the tutorial we will go over branching and the general GitHub workflow. So far we have been doing the \"solo\" workflow which consists of the following: mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" git push origin master # hack # more hack We are now going to introduce the GitHub flow that is largely done in teams. In the GitHub flow we never code anything unless there is a need to. When there is a need we then create an issue on the GitHub repository. Good issues should be Clear Defined output Actionable (written in the Imperative Voice) Could be completed at most in few days Examples Good : /Fix the bug in .../ Good : /Add a method that does .../ Bad : /Solve the project/ Bad : /Some error happen/ Here is how to create a GitHub issue. Once we have an issue we will then pull from the repo and create a branch . A branch is a copy of the code base separate from the main master branch where we can work on our issue (e.g, fixing a bug, adding a feature) without affecting the master branch during our work and then ultimately merge our change into the master branch. The flow goes something like this: ## Pull from the repo git pull ## Decide what do you want to do and create an issue git checkout -b a-meaningful-name The command git checkout -b creates a new branch which in this case is named \"a-meaningful-name\". We can see what branch we are on by using the command git branch which will show all the branches in the local repository and place an * next to the branch we are currently on. ## hack, hack, hack, add/rm, commit ## Push to the repo and create a remote branch git push ## Create a pull-request and describe your work (Suggest/add a reviewer) ## Someone then reviews your code ## The pull-request is closed and the remote branch is destroyed ## Switch to master locally git checkout master ## Pull the most recent changes (including yours) git pull ## Delete your local branch git branch -d a-meaningful-name Here is how to create a GitHub pull request.","title":"GitHub Flow"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/#solving-a-merge-conflict","text":"As you work on projects with others you will inevitably run into merge conflicts. A merge conflict is caused when you and another person edits the same line of a file. Git will not know which line is the correct one and create a conflict. Let's make a conflict! #create a branch called drama > git checkout -b drama #now modify the descriptive_stats.py file and change the top 10 #values to top 13 #commit your changes > git add descriptive_stats.py > git commit -m \"Changed top 10 to top 13 in descriptive_stats.py #switch back to master > git checkout master #now modify the descriptive_stats.py file and change the top 10 #values to top 3 # commit your changes > git add descriptive_stats.py > git commit -m \"Changed the top 10 to the top 3 in descriptive_stats.py\" Now we are going to merge the drama branch into the master branch > git merge drama Auto-merging descriptive_stats.py CONFLICT (content): Merge conflict in descriptive_stats.py Automatic merge failed; fix conflicts and then commit the result. Our arbitrary drama has now lead to a conflict. If we check the status we should see the following On branch master You have unmerged paths. (fix conflicts and run \"git commit\") Unmerged paths: (use \"git add <file>...\" to mark resolution) both modified: descriptive_stats.py Let's examine the conflicting file descriptive_stats.py , you should see the following: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) <<<<<<< HEAD print(df_311_calls['Complaint Type'].value_counts()[:3]) ======= print(df_311_calls['Complaint Type'].value_counts()[:13]) >>>>>>> drama The >>>>>>> and <<<<<<< denote the section of the conflicting code. HEAD means the following line if from the maser branch while drama shows that the preceding line is from the drama branch. The lines of the two branches are separated by the ======= . Given a merge conflict we have three choices: either keep the line from the master branch, keep the line from the drama branch or create an entirely new line. In this case we are going to keep the line from the master branch. To do that we delete the merge conflict markers and the line from the drama branch and then make a commit. Your code should look like this: from __future__ import print_function import pandas fname_data = '311-service-requests.csv' df_311_calls = pd.read_csv(fname_data) print(df_311_calls['Complaint Type'].value_counts()[:3]) Now add this file and make a commit. git add descriptive_stats.py git commit -m \"Fixed merge conflict in descriptive stats\" Our merge conflict is know solved. That concludes the tutorial on GitHub! Good Luck!","title":"Solving a merge conflict"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/04_GitHub/#acknowledgements-references-and-further-resources","text":"This tutorial is derived from tutorials created by Eduardo Blancas Reyes, Benedict Kuester, Adolfo De Unanue, Software Carpentry , and ASU PHY-494 . Furthur resources for becoming a git master are: * Software Carpentry -- a more in-depth intro tutorial 15 minute tutorial to learn git - Intro tutorial. git - the simple guide - A simple guide to get to know the most important concepts. A successful git branching model - A model to work with git using branches. This model is widely used in the open source community. Learn Git Branching - Understanding what branches and rebases are, in an amazing interactive tutorial. Reset Demystified - A blog post on git reset which develops some useful concepts along the way. Understanding git for real by exploring the .git directory - A blog post on what's inside a commit. Pro Git -- An in-depth discussion written by git masters.","title":"Acknowledgements, References and Further Resources"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/basic_git_tutorial/data/","text":"Data was taken from the NYC Open Data Portal https://nycopendata.socrata.com","title":"Home"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/githubflow/","text":"Github flow # Some notes about github flow...","title":"Github flow"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/githubflow/#github-flow","text":"Some notes about github flow...","title":"Github flow"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/01_firststeps/","text":"In this tutorial, we will go through some git basics, using commands that you will use again and again in your work. Git is a pretty amazing tool, and you will only be scratching the surface in the next half hour. Still, you should be able to use git to keep a history of your changes to files, to exchange your work with others, and to build on their changes in return. Don't worry if the commands seem like arcane incantations for now; we will cover many of the concepts that underlie them in the coming weeks. Your group is composed of fellows with different levels of experience with git. Help each other loads, and ask each other all the questions! Commands you'll learn # In this tutorial you'll be using the following commands: * ls -a : List files in a directory ( ls ) even ones that begin with . ( -a ) * cd : Change directories * git clone : Clone a repository from GitHub (or another \"remote\") * git add : Tell git you'd like to save changes to this file in your history * git commit : Tell git you'd like to save the changes you've git add ed * git status : Your favorite command: \"What's going on?!\" Concepts you will learn # Navigating a file system Cloning a repository Taking a snapshot of your work Create a git configuration # #Check to see what your git configuration looks like git config --list It is important to set up your configuration so you commits are attribted to you. #Set up your git configuration git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor \"nano\" #you can set this to whatever text editor you are comfortable with Also really important to do git config --global push.default current Create a repository # First things first: You need to create a repository (which is the place that stores your work and changes). There are several ways to do this, but one of the most convenient ones is to start on GitHub - that way, your repo will be super easy to share with your... fellow fellows. Learner A : Go to github.com , log in, and find the 'New Repository' button. Name your repo 'soup', and give it a short description if you like; the remaining options can be left at their default. (Ask your partners for details if you're interested!) Learner A : Now, you need to get the repo on your local machine! Find (or have your partners point you to) the 'Clone or download' button, and copy the URL that pops up. Then go to your command line and do: git clone https://github.com/username/rhymes.git , where you need to substitute the URL for the one that you just copied from GitHub. This will then create a complete copy of the entire project. Learner A : Once git is done copying all kinds of things from your newly creating repo on github, we can look around a bit. First, go into the new folder that has the same name as your repo: cd soup . Say ls -a ; you will see that git has created the (hidden) directory called .git , where it stores all its inner workings. If you say git remote -v , you can see where you cloned your repo from (and where changes will go to later!). Saying git status tells you that you are at the start of working on this repo, and, importantly, that there is 'nothing to commit'. This means that you haven't told git to keep track of anything yet. Learner A : In your 'rhymes' folder, add a new file called words.md . Open it, and add a few words that rhyme. Pro tip: Ask your partners for suggestions as to what rhymes with 'orange'. Learner A : Do git status again. What has changed? Learner A : You need to tell git that you'd like take a snapshot of your work (which you can later go back to and do all kinds of interesting things with). Say git add words.md . Then check git status again. Do you see how git now tells you how words.md is now among the 'changes to be commited' (i.e., the changes that will go into your snapshot)? Learner A : It's time to take the snapshot! Say git commit -m 'blablabla' , where you should substitute the blablabla with a short description of what you have done in this snapshot, like 'create file with rhymes' . Congratulations! You have made a commit to your repo. Check git status again. Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Go to the next tutorial . Image source","title":"01 firststeps"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/01_firststeps/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * ls -a : List files in a directory ( ls ) even ones that begin with . ( -a ) * cd : Change directories * git clone : Clone a repository from GitHub (or another \"remote\") * git add : Tell git you'd like to save changes to this file in your history * git commit : Tell git you'd like to save the changes you've git add ed * git status : Your favorite command: \"What's going on?!\"","title":"Commands you'll learn"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/01_firststeps/#concepts-you-will-learn","text":"Navigating a file system Cloning a repository Taking a snapshot of your work","title":"Concepts you will learn"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/01_firststeps/#create-a-git-configuration","text":"#Check to see what your git configuration looks like git config --list It is important to set up your configuration so you commits are attribted to you. #Set up your git configuration git config --global user.name \"Clark Kent\" git config --global user.email \"clark.kent@dailyplanet.com\" git config --global color.ui \"auto\" git config --global core.editor \"nano\" #you can set this to whatever text editor you are comfortable with Also really important to do git config --global push.default current","title":"Create a git configuration"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/01_firststeps/#create-a-repository","text":"First things first: You need to create a repository (which is the place that stores your work and changes). There are several ways to do this, but one of the most convenient ones is to start on GitHub - that way, your repo will be super easy to share with your... fellow fellows. Learner A : Go to github.com , log in, and find the 'New Repository' button. Name your repo 'soup', and give it a short description if you like; the remaining options can be left at their default. (Ask your partners for details if you're interested!) Learner A : Now, you need to get the repo on your local machine! Find (or have your partners point you to) the 'Clone or download' button, and copy the URL that pops up. Then go to your command line and do: git clone https://github.com/username/rhymes.git , where you need to substitute the URL for the one that you just copied from GitHub. This will then create a complete copy of the entire project. Learner A : Once git is done copying all kinds of things from your newly creating repo on github, we can look around a bit. First, go into the new folder that has the same name as your repo: cd soup . Say ls -a ; you will see that git has created the (hidden) directory called .git , where it stores all its inner workings. If you say git remote -v , you can see where you cloned your repo from (and where changes will go to later!). Saying git status tells you that you are at the start of working on this repo, and, importantly, that there is 'nothing to commit'. This means that you haven't told git to keep track of anything yet. Learner A : In your 'rhymes' folder, add a new file called words.md . Open it, and add a few words that rhyme. Pro tip: Ask your partners for suggestions as to what rhymes with 'orange'. Learner A : Do git status again. What has changed? Learner A : You need to tell git that you'd like take a snapshot of your work (which you can later go back to and do all kinds of interesting things with). Say git add words.md . Then check git status again. Do you see how git now tells you how words.md is now among the 'changes to be commited' (i.e., the changes that will go into your snapshot)? Learner A : It's time to take the snapshot! Say git commit -m 'blablabla' , where you should substitute the blablabla with a short description of what you have done in this snapshot, like 'create file with rhymes' . Congratulations! You have made a commit to your repo. Check git status again. Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Go to the next tutorial . Image source","title":"Create a repository"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/02_showtheothers/","text":"Learner A has done some work on the repo. It's time to share that work with the other two! Commands you'll learn # In this tutorial you'll be using the following commands: * git log : Show a list of your commits so far * git push : Sync your commits back to your repo's GitHub page (or another 'remote') Share What You Did # Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Learner A : Now, it's time to sync your commit back to GitHub. Say git push origin master . Go to your repo's webpage on GitHub and refresh it. See your rhymes? Learner B and C : Clone Learner A's repo. Show Learner A the local files that you got from cloning. Show Learner A a git log on your clone. The commit message is there, too! Learner B and C : High five Learner A. (And Matt and Joe. They love that.) Go to the next tutorial .","title":"02 showtheothers"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/02_showtheothers/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * git log : Show a list of your commits so far * git push : Sync your commits back to your repo's GitHub page (or another 'remote')","title":"Commands you'll learn"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/02_showtheothers/#share-what-you-did","text":"Learner A : Over time, you will do many commits. To see a list of them, do git log . There's only one so far! Learner A : Now, it's time to sync your commit back to GitHub. Say git push origin master . Go to your repo's webpage on GitHub and refresh it. See your rhymes? Learner B and C : Clone Learner A's repo. Show Learner A the local files that you got from cloning. Show Learner A a git log on your clone. The commit message is there, too! Learner B and C : High five Learner A. (And Matt and Joe. They love that.) Go to the next tutorial .","title":"Share What You Did"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/03_addingon/","text":"While having a repo for yourself is nice for version control, how do you add to somebody else's work? There are some pretty neat ways of handling this, but right now, we'll go through one of the simplest. Commands you'll learn # In this tutorial you'll be using the following commands: * git pull : Get changes from the repo's GitHub page (or another 'remote') into your local folder * git push : Sync your commits back to your repo's GitHub page (or another 'remote') Adding to It # Learner C : Create a new repo called 'soup', and add Learner A and Learner B as collaborators. Learner C : Add a file ingredients.md . Populate it with a short list of ingredients that would make a medieval witch scratch her head. Commit your list. Add a few more ingredients (or delete some!). Commit again. Push your commits with git push origin master . Learners A and B : Get the necessary URL from Learner C, then clone Learner C's 'soup' repo ( git clone https://github.com/learnerc/soup.git , where you need to substitute the correct URL), and go into the repo's folder: cd soup/ Learners A and B : Do git log to see what Learner C has been up to so far. Learner C : Add another ingredient to your file, commit and push again. Learner A and B : As Learner C has added more stuff, you need to update your local clone of their repository! To do this, call git pull . Check what just happened with git log . Do you see the new commit that Learner A only added to their repo after you first cloned it? Learner A : In the repo's folder, create a new file called spices.md . In that file, list some fantasmorgacically hot spices. Do a git status . What does have git to say about your new file so far? Learner A : Do git add spices.md to tell git that it should prepare for a snapshot of your changes to that file. (Just like in the first tutorial.) Check git status . Learner A : Do git commit -m 'blablabla' to create a new snapshot (commit) of the repo as you have it now, which includes your new file now! You should probably make the commit message more meaningful than 'blablabla'. Then, push your changes: git push origin master , to synchronize your local commits with your repo on GitHub (your 'remote'). Learner B and C : Pull the changes that Learner A made, and show Learner A that you now have the spice list. Also show Learner A your git log . Learner A's commit is there! (Great success. ) Here is the next tutorial . Talking of spicy food, here is YouTube channel of a Dutchman eating chili peppers while interviewing people .","title":"03 addingon"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/03_addingon/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * git pull : Get changes from the repo's GitHub page (or another 'remote') into your local folder * git push : Sync your commits back to your repo's GitHub page (or another 'remote')","title":"Commands you'll learn"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/03_addingon/#adding-to-it","text":"Learner C : Create a new repo called 'soup', and add Learner A and Learner B as collaborators. Learner C : Add a file ingredients.md . Populate it with a short list of ingredients that would make a medieval witch scratch her head. Commit your list. Add a few more ingredients (or delete some!). Commit again. Push your commits with git push origin master . Learners A and B : Get the necessary URL from Learner C, then clone Learner C's 'soup' repo ( git clone https://github.com/learnerc/soup.git , where you need to substitute the correct URL), and go into the repo's folder: cd soup/ Learners A and B : Do git log to see what Learner C has been up to so far. Learner C : Add another ingredient to your file, commit and push again. Learner A and B : As Learner C has added more stuff, you need to update your local clone of their repository! To do this, call git pull . Check what just happened with git log . Do you see the new commit that Learner A only added to their repo after you first cloned it? Learner A : In the repo's folder, create a new file called spices.md . In that file, list some fantasmorgacically hot spices. Do a git status . What does have git to say about your new file so far? Learner A : Do git add spices.md to tell git that it should prepare for a snapshot of your changes to that file. (Just like in the first tutorial.) Check git status . Learner A : Do git commit -m 'blablabla' to create a new snapshot (commit) of the repo as you have it now, which includes your new file now! You should probably make the commit message more meaningful than 'blablabla'. Then, push your changes: git push origin master , to synchronize your local commits with your repo on GitHub (your 'remote'). Learner B and C : Pull the changes that Learner A made, and show Learner A that you now have the spice list. Also show Learner A your git log . Learner A's commit is there! (Great success. ) Here is the next tutorial . Talking of spicy food, here is YouTube channel of a Dutchman eating chili peppers while interviewing people .","title":"Adding to It"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/04_thesecretsauce/","text":"Sometimes, you want to keep a files in your repo's directory local, and not share it. Git has an app, errm, file, for that! Commands you'll learn # In this tutorial you'll be using the following commands: * .gitignore : A file that lists files (or file and folder name patterns) that should be excluded from commits. Keeping It Special # Learner B : You would like to add an ingredient to the soup which should not be mentioned to the public, or your partners. Create a file sauce.secret , and name your secret ingredient in it. Do a git status . What does git think about your new file? Learner B : Create a .gitignore file, and add a line saying *.secret to it. Do git status again. What has changed? Your secret file should no longer appear there. Learner B : Tell git that you want it to include the .gitignore file in your commit: git add .gitignore . Then commit and push your changes. Learner A and C : Get the changes that Learner B just made: Say git pull . List all files (includen hidden ones) with ls -a . Did you receive the .gitignore ? What about the sauce.secret ? Which of those two files appears in the repo on GitHub? Also have a look at your git log . Here is the next tutorial .","title":"04 thesecretsauce"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/04_thesecretsauce/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * .gitignore : A file that lists files (or file and folder name patterns) that should be excluded from commits.","title":"Commands you'll learn"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/04_thesecretsauce/#keeping-it-special","text":"Learner B : You would like to add an ingredient to the soup which should not be mentioned to the public, or your partners. Create a file sauce.secret , and name your secret ingredient in it. Do a git status . What does git think about your new file? Learner B : Create a .gitignore file, and add a line saying *.secret to it. Do git status again. What has changed? Your secret file should no longer appear there. Learner B : Tell git that you want it to include the .gitignore file in your commit: git add .gitignore . Then commit and push your changes. Learner A and C : Get the changes that Learner B just made: Say git pull . List all files (includen hidden ones) with ls -a . Did you receive the .gitignore ? What about the sauce.secret ? Which of those two files appears in the repo on GitHub? Also have a look at your git log . Here is the next tutorial .","title":"Keeping It Special"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/05_backtothepast/","text":"Sometimes, you check out a repo, happily working along, and then just screw your files up badly. Let's do that. Commands you'll learn # In this tutorial you'll be using the following commands: * git checkout : Take a snapshot (commit) and make your working directory look like it. I Just Want To Go Back # Learner A, B, and C : Go to your 'soup' repo from the previous exercise. Just to be sure that everybody is up to date, everybody should call git pull . You can also have a look at your git log ; they should all look the same. Learner A, B, and C : Individually, do something bad to your local files (only in the repo, of course!). Delete the ingredients.md , or delete lines in one of the files, and then save them. Learner B ! Do not delete your sauce.secret ! It has never been committed, so you cannot get it back! Learner A, B, and C : Now, again individually, you decide that those working changes were really not great, and you want to restore the last commit. There are several ways of doing this in git. Say git status - it will recommend one possible way. Learner A, B, and C : As git status tells you, you can use git checkout -- <filename> to discard your working changes. If you call git checkout -- . , you will discard all your working changes in your current directory, and thus be thrown back to the last commit. Be super careful. This discards your working changes. They will be lost. (If you feel like it, google git stash at this point if you would like to back up your working changes, just to be safe.) Learner A, B, and C : What do you do if you have committed changes that you would like to undo? That is, if you want to go back to a commit far back in time? There are really neat ways of doing this in git, even if you have already pushed (shared) your newer commits to GitHub. Cliffhanger: You will learn about this in the git branching tutorial. ( git revert and git reset is where it's at.) Image source","title":"05 backtothepast"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/05_backtothepast/#commands-youll-learn","text":"In this tutorial you'll be using the following commands: * git checkout : Take a snapshot (commit) and make your working directory look like it.","title":"Commands you'll learn"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/05_backtothepast/#i-just-want-to-go-back","text":"Learner A, B, and C : Go to your 'soup' repo from the previous exercise. Just to be sure that everybody is up to date, everybody should call git pull . You can also have a look at your git log ; they should all look the same. Learner A, B, and C : Individually, do something bad to your local files (only in the repo, of course!). Delete the ingredients.md , or delete lines in one of the files, and then save them. Learner B ! Do not delete your sauce.secret ! It has never been committed, so you cannot get it back! Learner A, B, and C : Now, again individually, you decide that those working changes were really not great, and you want to restore the last commit. There are several ways of doing this in git. Say git status - it will recommend one possible way. Learner A, B, and C : As git status tells you, you can use git checkout -- <filename> to discard your working changes. If you call git checkout -- . , you will discard all your working changes in your current directory, and thus be thrown back to the last commit. Be super careful. This discards your working changes. They will be lost. (If you feel like it, google git stash at this point if you would like to back up your working changes, just to be safe.) Learner A, B, and C : What do you do if you have committed changes that you would like to undo? That is, if you want to go back to a commit far back in time? There are really neat ways of doing this in git, even if you have already pushed (shared) your newer commits to GitHub. Cliffhanger: You will learn about this in the git branching tutorial. ( git revert and git reset is where it's at.) Image source","title":"I Just Want To Go Back"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/06_moar/","text":"Interesting Stuff To Do # If you're familar with git already, do two things: Set up a repo and share it with your collaborators. Practice creating branches, and then submitting pull requests to master for your changes. Somebody else should approve and merge your changes. Rotate, so that everybody in your group gets to approve somebody else pull request. Individually, do this awesome tutorial .","title":"Interesting Stuff To Do"},{"location":"curriculum/2_data_exploration_and_analysis/git-and-github/tutorials/06_moar/#interesting-stuff-to-do","text":"If you're familar with git already, do two things: Set up a repo and share it with your collaborators. Practice creating branches, and then submitting pull requests to master for your changes. Somebody else should approve and merge your changes. Rotate, so that everybody in your group gets to approve somebody else pull request. Individually, do this awesome tutorial .","title":"Interesting Stuff To Do"},{"location":"curriculum/2_data_exploration_and_analysis/intro-to-git-and-python/","text":"","title":"Home"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/","text":"Networks # Background and Motivation # Networks are a measurable representation of patterns of relationships connecting entities in an abstract or actual space. The constituent parts of a network are nodes which are connected by ties. Networks have been used to model airplane traffic from airports, supply chains, friendship networks, and even amorphous materials like window glass, cells, and proteins. The importance of networks analysis is that it captures the effect of where and how an individual actor is positioned among others. In this tutorial, we are going to examine the friendship network of 34 students in a karate class. A political rivalry arose in the class and divided the class into two factions, eventually leading to a fissure and two separate karate classes. The club held periodic meetings to vote on policy decisions. When one of the faction heads, individuals 1 and 34, called a meeting, they would communicate the information only to members firmly in their faction, in order to ensure that the majority of members attending the meeting were in their faction, thereby guaranteeing that their policies would pass. Meeting times were not publicly announced, but spread from friend to friend in the social network. In this tutorial we will explore graphical representations of this network, degree metrics, centrality metrics, how to calculate the shortest path between nodes, and community detection. We will be using the NetworkX Python Library developed at Los Alamos National Laboratory (LANL). Installation # To install the virtual environment and dependencies run the following command ./install_network_env.sh && source network-venv/bin/activate","title":"Networks"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/#networks","text":"","title":"Networks"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/#background-and-motivation","text":"Networks are a measurable representation of patterns of relationships connecting entities in an abstract or actual space. The constituent parts of a network are nodes which are connected by ties. Networks have been used to model airplane traffic from airports, supply chains, friendship networks, and even amorphous materials like window glass, cells, and proteins. The importance of networks analysis is that it captures the effect of where and how an individual actor is positioned among others. In this tutorial, we are going to examine the friendship network of 34 students in a karate class. A political rivalry arose in the class and divided the class into two factions, eventually leading to a fissure and two separate karate classes. The club held periodic meetings to vote on policy decisions. When one of the faction heads, individuals 1 and 34, called a meeting, they would communicate the information only to members firmly in their faction, in order to ensure that the majority of members attending the meeting were in their faction, thereby guaranteeing that their policies would pass. Meeting times were not publicly announced, but spread from friend to friend in the social network. In this tutorial we will explore graphical representations of this network, degree metrics, centrality metrics, how to calculate the shortest path between nodes, and community detection. We will be using the NetworkX Python Library developed at Los Alamos National Laboratory (LANL).","title":"Background and Motivation"},{"location":"curriculum/2_data_exploration_and_analysis/network-analysis/#installation","text":"To install the virtual environment and dependencies run the following command ./install_network_env.sh && source network-venv/bin/activate","title":"Installation"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/","text":"Record Linkage # Background and Motivation # The goal of record linkage is to determine if pairs of records describe the same entity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkages also goes by the terms -- data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora -- in various fields. There are several approaches to record linkage that includes exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not. Installation # The tutorial is Python2/3 compatible. The dependencies can be found in the requirements.txt file. Create a virtual environment: virtualenv --no-site-packages rl-venv source rl-venv/bin/activate pip install -r requirements.txt","title":"Record Linkage"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#record-linkage","text":"","title":"Record Linkage"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#background-and-motivation","text":"The goal of record linkage is to determine if pairs of records describe the same entity. This is important for removing duplicates from a data source or joining two separate data sources together. Record linkages also goes by the terms -- data matching, merge/purge, duplication detection, de-duping, reference matching, co-reference/anaphora -- in various fields. There are several approaches to record linkage that includes exact matching, rule-based linking and probabilistic linking. An example of exact matching is joining records based on social security number. Rule-based matching involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. In probabilistic record linkage, linkage weights are calculated based on records and a threshold is applied to make a decision of whether to link records or not.","title":"Background and Motivation"},{"location":"curriculum/2_data_exploration_and_analysis/record-linkage/#installation","text":"The tutorial is Python2/3 compatible. The dependencies can be found in the requirements.txt file. Create a virtual environment: virtualenv --no-site-packages rl-venv source rl-venv/bin/activate pip install -r requirements.txt","title":"Installation"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/","text":"Text Analysis Example # Motivation and Background # Text Analysis is used for summarizing or getting useful information out of a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g., surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible, scaleable, adaptive and cost-effective way. Text Analysis can help with the following tasks: Searches and information retrieval : Help find relevant information in large databases such a systematic literature review. Clustering and text categorization : Techniques like topic modeling modeling can summarize a large corpus of text by finding the most important phrases. Text Summarizing : Create category-sensitive text summaries of a large corpus of text. Machine Translation : Translate from one language to another. In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post originated from and also do topic modeling to categorize posts. Dependencies # To run this notebook you will need the packages listed in requirements.txt . To install run pip install -r requirements.txt in the command line. You will also need jupyter notebook and python3 installed. Data # The data was filtered from this dataset . To unzip the data, run gunzip ./data/RC_2015-05.json.gz Further Resources # Natural Language Processing with Python Getting Started with NLP","title":"Text Analysis Example"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#text-analysis-example","text":"","title":"Text Analysis Example"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#motivation-and-background","text":"Text Analysis is used for summarizing or getting useful information out of a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g., surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible, scaleable, adaptive and cost-effective way. Text Analysis can help with the following tasks: Searches and information retrieval : Help find relevant information in large databases such a systematic literature review. Clustering and text categorization : Techniques like topic modeling modeling can summarize a large corpus of text by finding the most important phrases. Text Summarizing : Create category-sensitive text summaries of a large corpus of text. Machine Translation : Translate from one language to another. In this tutorial we are going to analyze reddit posts from May 2015 in order to classify which subreddit a post originated from and also do topic modeling to categorize posts.","title":"Motivation and Background"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#dependencies","text":"To run this notebook you will need the packages listed in requirements.txt . To install run pip install -r requirements.txt in the command line. You will also need jupyter notebook and python3 installed.","title":"Dependencies"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#data","text":"The data was filtered from this dataset . To unzip the data, run gunzip ./data/RC_2015-05.json.gz","title":"Data"},{"location":"curriculum/2_data_exploration_and_analysis/text-analysis/#further-resources","text":"Natural Language Processing with Python Getting Started with NLP","title":"Further Resources"},{"location":"curriculum/3_modeling_and_machine_learning/","text":"Modeling and Machine Learning # Let's make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating your models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through.","title":"Operations Research"},{"location":"curriculum/3_modeling_and_machine_learning/#modeling-and-machine-learning","text":"Let's make some models! Most of the modeling techniques you'll use, whether supervised or unsupervised, will fall under the umbrella of machine learning , but that's not all you need to know. Knowing some social science will go a long way when it comes to formulating your models appropriately, designing experiments to evaluate model performance, and understanding what conclusions you can make based on your results. Quantitative Social Science and Causal Inference with Observational Data will help you think these questions through.","title":"Modeling and Machine Learning"},{"location":"curriculum/3_modeling_and_machine_learning/causal-inference/","text":"","title":"Home"},{"location":"curriculum/3_modeling_and_machine_learning/machine-learning/","text":"Machine Learning # Motivation # You've probably heard about machine learning, artificial intelligence, etc. It's an integral part of data science. So we'll teach you the basics about ML: the process, formulating a problem in a machine learning setting, feature generation, model selection and evaluation, and how to interpret results before eventually deploying the models. For a more detailed coverage of machine learning for social good and policy, you can take a look at lectures for the Machine Learning and Public Policy class .","title":"Machine Learning"},{"location":"curriculum/3_modeling_and_machine_learning/machine-learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"curriculum/3_modeling_and_machine_learning/machine-learning/#motivation","text":"You've probably heard about machine learning, artificial intelligence, etc. It's an integral part of data science. So we'll teach you the basics about ML: the process, formulating a problem in a machine learning setting, feature generation, model selection and evaluation, and how to interpret results before eventually deploying the models. For a more detailed coverage of machine learning for social good and policy, you can take a look at lectures for the Machine Learning and Public Policy class .","title":"Motivation"},{"location":"curriculum/3_modeling_and_machine_learning/operations-research/","text":"","title":"Home"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/readme/","text":"Pipelines #","title":"Pipelines"},{"location":"curriculum/3_modeling_and_machine_learning/pipelines/readme/#pipelines","text":"","title":"Pipelines"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/","text":"\"We're All Social Scientists Now\" # An Introduction to Quantitative Social Science # In this session, we learn some of the fundamental differences between machine learning and quantitative social sciences. We also identify some useful datasets and lessons learned for DSSG projects. Potential Teachouts # Experiments Quasi-experiments Observational studies Slides # You can access the slides for this session here . Additional Reading # \"We're All Social Scientists Now\" \"Causal Inference in Statistics: An Overview\" \"Causal Inference in Social Science: An Elementary Introduction\" \"Fuck Nuance\" Counterfactuals and Causal Inference Mostly Harmless Econometrics Prediction Policy Problems","title":"\"We're All Social Scientists Now\""},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#were-all-social-scientists-now","text":"","title":"\"We're All Social Scientists Now\""},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#an-introduction-to-quantitative-social-science","text":"In this session, we learn some of the fundamental differences between machine learning and quantitative social sciences. We also identify some useful datasets and lessons learned for DSSG projects.","title":"An Introduction to Quantitative Social Science"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#potential-teachouts","text":"Experiments Quasi-experiments Observational studies","title":"Potential Teachouts"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#slides","text":"You can access the slides for this session here .","title":"Slides"},{"location":"curriculum/3_modeling_and_machine_learning/quantitative-social-science/#additional-reading","text":"\"We're All Social Scientists Now\" \"Causal Inference in Statistics: An Overview\" \"Causal Inference in Social Science: An Elementary Introduction\" \"Fuck Nuance\" Counterfactuals and Causal Inference Mostly Harmless Econometrics Prediction Policy Problems","title":"Additional Reading"},{"location":"curriculum/communication/","text":"Presentations and Communications # Remember that there's no point to doing data science for social good work if no one understands what you did and can put it to good use. You'll need to be able to communicate your work: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable.","title":"Intro"},{"location":"curriculum/communication/#presentations-and-communications","text":"Remember that there's no point to doing data science for social good work if no one understands what you did and can put it to good use. You'll need to be able to communicate your work: Data Visualization and Presentation Skills will help with that, whether you're communicating your work to a public audience or to stakeholders. When you're working directly with a project partner and are creating tools for them to use, keep Usability and User Interfaces in mind to make sure that whatever tools you create will be useful and usable.","title":"Presentations and Communications"},{"location":"curriculum/communication/user_interface/","text":"Usability and User Interface Design # Motivation # When you're working on data science for social good projects, you'll usually be partnering with (or working within) an organization, and there will be many stakeholders, or people who influence or will be influenced by your work. These stakeholders will be both within and outside the organization, including you, whoever is funding the work, the people who create and collect the data, and the people who are actually affected by your analysis and the decisions it drives. To make sure that your work is creating a useful and usable solution to a real problem (and not just wasting your and everyone else's time), we borrow some ideas from the school of agile development. Concepts # Agile development: iterative development, user stories Wireframes Resources # Slides Suggested reading: The Design of Everyday Things by Donald Norman, The Lean Startup by Eric Ries, User Stories Applied for Agile Software Development by Mike Cohn","title":"User interface"},{"location":"curriculum/communication/user_interface/#usability-and-user-interface-design","text":"","title":"Usability and User Interface Design"},{"location":"curriculum/communication/user_interface/#motivation","text":"When you're working on data science for social good projects, you'll usually be partnering with (or working within) an organization, and there will be many stakeholders, or people who influence or will be influenced by your work. These stakeholders will be both within and outside the organization, including you, whoever is funding the work, the people who create and collect the data, and the people who are actually affected by your analysis and the decisions it drives. To make sure that your work is creating a useful and usable solution to a real problem (and not just wasting your and everyone else's time), we borrow some ideas from the school of agile development.","title":"Motivation"},{"location":"curriculum/communication/user_interface/#concepts","text":"Agile development: iterative development, user stories Wireframes","title":"Concepts"},{"location":"curriculum/communication/user_interface/#resources","text":"Slides Suggested reading: The Design of Everyday Things by Donald Norman, The Lean Startup by Eric Ries, User Stories Applied for Agile Software Development by Mike Cohn","title":"Resources"},{"location":"curriculum/communication/writing/","text":"","title":"Writing reports"},{"location":"curriculum/dbs/getting_data_in/","text":"","title":"Getting data in"},{"location":"curriculum/dbs/getting_data_out/","text":"","title":"Getting data out"},{"location":"curriculum/dbs/other_types/","text":"","title":"Other types of DBs"},{"location":"curriculum/dbs/relational_design/","text":"","title":"Designing a DB"},{"location":"curriculum/dbs/why/","text":"","title":"Why a DB?"},{"location":"curriculum/deployment/advanced_pipelines/","text":"","title":"Advanced pipelines"},{"location":"curriculum/deployment/how_to/","text":"","title":"How to deploy"},{"location":"curriculum/deployment/monitor/","text":"","title":"Monitor"},{"location":"curriculum/deployment/update/","text":"","title":"Update"},{"location":"curriculum/eda/clustering/","text":"","title":"ML as a data exploration tool (Clustering)"},{"location":"curriculum/eda/data_stories/","text":"","title":"Data stories concept and code"},{"location":"curriculum/eda/gis/","text":"","title":"GIS"},{"location":"curriculum/eda/intro/","text":"","title":"Introduction to EDA"},{"location":"curriculum/eda/network/","text":"","title":"Network"},{"location":"curriculum/eda/tableau/","text":"","title":"Tableau (?)"},{"location":"curriculum/eda/text/","text":"","title":"Text"},{"location":"curriculum/eda/visualization/","text":"","title":"Visualization"},{"location":"curriculum/experimental_design/case_studies/","text":"","title":"Case studies"},{"location":"curriculum/experimental_design/intro/","text":"","title":"Experiment design"},{"location":"curriculum/get_data/flat_files/","text":"","title":"Flat files"},{"location":"curriculum/get_data/images/","text":"","title":"Working with images"},{"location":"curriculum/get_data/scrapping/","text":"","title":"APIs and scrapping"},{"location":"curriculum/get_data/security/","text":"","title":"Data security"},{"location":"curriculum/get_data/text/","text":"","title":"Working with text"},{"location":"curriculum/link_data/record_linkage/","text":"","title":"Record linkage"},{"location":"curriculum/methods/causal_inference/","text":"","title":"Causal inference methods"},{"location":"curriculum/methods/or/","text":"","title":"OR/optimization methods"},{"location":"curriculum/methods/social_science/","text":"","title":"Social science methods"},{"location":"curriculum/methods/statistical_analysis/","text":"","title":"Other statistical analysis methods"},{"location":"curriculum/ml/methods/","text":"","title":"Machine learning methods"},{"location":"curriculum/ml/pipeline_ii/","text":"","title":"ML pipeline II"},{"location":"curriculum/ml/problem_formulation/","text":"","title":"ML problem formulation"},{"location":"curriculum/ml/templates/","text":"","title":"Templates of policy problems"},{"location":"curriculum/ml/tips/","text":"","title":"Practical tips on how to use them, parameters, etc."},{"location":"curriculum/ml/feature_engineering/case_studies/","text":"","title":"Case studies"},{"location":"curriculum/ml/feature_engineering/intro/","text":"","title":"Feature engineering"},{"location":"curriculum/ml/feature_engineering/workshop/","text":"","title":"Workshop on feature engineering"},{"location":"curriculum/ml/labels/implications/","text":"","title":"Implications of a label"},{"location":"curriculum/ml/labels/one_or_many/","text":"","title":"One or many"},{"location":"curriculum/ml/metrics/examples/","text":"","title":"Examples"},{"location":"curriculum/ml/metrics/overview/","text":"","title":"Overview"},{"location":"curriculum/ml/postmodeling/bias_analysis/","text":"","title":"Bias analysis"},{"location":"curriculum/ml/postmodeling/error_analysis/","text":"","title":"Error analysis"},{"location":"curriculum/ml/postmodeling/feature_importance/","text":"","title":"Feature importance"},{"location":"curriculum/ml/postmodeling/list_comparison/","text":"","title":"Comparing lists"},{"location":"curriculum/ml/postmodeling/model_comparison/","text":"","title":"Comparing different models"},{"location":"curriculum/ml/postmodeling/understanding/","text":"","title":"Model understanding"},{"location":"curriculum/ml/selection/audition/","text":"","title":"Audition"},{"location":"curriculum/ml/selection/bias/","text":"","title":"Bias"},{"location":"curriculum/ml/selection/interpretability/","text":"","title":"Interpretability"},{"location":"curriculum/ml/selection/performance/","text":"","title":"Performance"},{"location":"curriculum/ml/selection/stability/","text":"","title":"Stability"},{"location":"curriculum/ml/validation/field_trials/","text":"","title":"Field trials"},{"location":"curriculum/ml/validation/kfold/","text":"","title":"K-fold cross-validation"},{"location":"curriculum/ml/validation/process_and_goal/","text":"","title":"Process and goal"},{"location":"curriculum/ml/validation/tcc/","text":"","title":"Temporal cross-validation"},{"location":"curriculum/programming_best_practices/","text":"Programming Best Practices # As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in.","title":"Intro"},{"location":"curriculum/programming_best_practices/#programming-best-practices","text":"As you begin to work on larger, more complicated projects, and work in teams with other programmers, you'll save yourself and your teammates a lot of grief and frustration by writing legible, good code and writing tests . You'll also need to document and package up your work so that other people can understand and reproduce your results, so check out the reproducible software tutorial. As you continue to develop these skills, you'll start to change settings and configurations for various applications, so check out pimp my dotfiles for some tips on how to customize the environments you're working in.","title":"Programming Best Practices"},{"location":"curriculum/programming_best_practices/legible-good-code/","text":"Best Practices: Writing Legible, Good Code # Motivation # All fellows will have to write code that is usable and understandable by their peers and partners, and potentially other 3rd parties. What does that entail in practice? Many fellows are coming from academic backgrounds, have self-taught programming skills, and have never worked collaboratively on a software project. We want to help them establish good habits and avoid common mistakes. (Adapted from Tutorial by Kevin Wilson, 2016 DSSG Technical Mentor) For whom do we write code? # What you write (for people). # def fib ( n ): \"\"\" :param int n: The Fibonnaci index you want to return :return: The nth Fibonnaci number :rtype: int \"\"\" if n < 2 : return 1 else : return fib ( n - 1 ) + fib ( n - 2 ) Here is an example of a function. Why write this function? Well, literally, you give the function an integer n , and the function gives you back the n th Fibonnaci number. Writing code allows you to have the computer calculate the n th Fibonnaci number, which it can probably do much faster than you can, especially as n gets larger and larger. It also allows your human comrades to see for themselves how you rattle off arbitrary Fibonacci numbers so fast. But you don't really write code for the computer... What the computer sees (assembly language for the processor). # ... because if you did, it would look like this. 2 0 LOAD_FAST 0 (n) 3 LOAD_CONST 1 (2) 6 COMPARE_OP 0 (<) 9 POP_JUMP_IF_FALSE 16 3 12 LOAD_CONST 2 (1) 15 RETURN_VALUE 5 >> 16 LOAD_GLOBAL 0 (fib) 19 LOAD_FAST 0 (n) 22 LOAD_CONST 2 (1) 25 BINARY_SUBTRACT 26 CALL_FUNCTION 1 29 LOAD_GLOBAL 0 (fib) 32 LOAD_FAST 0 (n) 35 LOAD_CONST 1 (2) 38 BINARY_SUBTRACT 39 CALL_FUNCTION 1 42 BINARY_ADD 43 RETURN_VALUE 44 LOAD_CONST 0 (None) 47 RETURN_VALUE Seriously, the code is for you ... # ...one month from now when you've completely forgotten what the heck alpha was... ...or tomorrow when your teammate wonders why all those 3 s are in the database.... ...or when your system is on fire at 2 AM (the witching hour of production systems), you're tired, and your customers are calling wondering what the heck is going on over there (why are they awake, anyway?)... ...or when I'm doing code review for you... There is only one law of good coding: Code is for humans, not for computers . # This law has 4 consequences: Give things informative names. Document inputs and outputs. Don't repeat yourself. Reduce cognitive load: Use PEP-8. Give Things Informative Names # What does the function below do? import math def doit ( x ): output = [] y = 2 while x != 1 : if x % y == 0 : output . append ( y ) x //= y else : y += 1 continue return output def magic ( input ): filter = set () return sorted ( x for x in y for y in input if ( len ( x ) < 20 and x not in filter ) or filter . add ( x )) def pretty_pictures (): df = pd . read_csv ( '2016.csv.gz' , names = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ]) first = df [ weather_df . STATION == 'US1ILCK0010' ] first [ first . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == 'US1ILCK0014' ] second [ second . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return first [ first . TYPE == 'PRCP' ] . mean (), second [ second . TYPE == 'PRCP' ] . mean () Names should have meaning to the intended readers, e.g., You a week from now Your teammates tomorrow Your future teammates who have to deal with your code [ i , j , k ] for iterators are OK, alpha with a reference to a specific paper is not kwargs are a great place to name things CONSTANT_VALUES are too Do not fear long names; you have autocomplete. The TAB key is your friend. Document Inputs and Outputs # import math def factor ( x ): output = [] fact = 2 while x != 1 : if x % fact == 0 : output . append ( y ) x //= fact else : fact += 1 continue return output def unique_flatten ( the_input , max_length = 20 ): flattened_set = { val for val in row for row in the_input } filtered_list = [ val for val in flattened_set if len ( val ) < max_length ] filtered_list . sort () return filtered_list WEATHER_HEADERS = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ] PRECIPITATION_TYPE = 'PRCP' CHICAGO_STATION_NAMES = [ 'US1ILCK0010' , 'US1ILCK0014' ] def precipitation_in_chicago (): df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES ] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ] . mean (), second [ second . TYPE == PRECIPITATION_TYPE ] . mean ()) Every function should have a docstring The docstring should: Describe the function briefly Explicitly document the inputs with :param type name: description Explicitly document the return value with :returns: description Explicitly document the return type with :rtype: Note this follows the Sphinx/RST syntax guide . You can also follow the Numpy Format . Just be consistent. Don't Repeat Yourself # The WET (Write Everything Twice) Way # def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = {} for val in left : if val not in left_counts : left_counts [ val ] = 1 else : left_counts [ val ] += 1 right_counts = {} for val in right : if val not in right_counts : right_counts [ val ] = 1 else : right_counts [ val ] += 1 left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts In this case, the value counting logic is redundant and can be abstracted away into a function. The DRY Way: Use a function to avoid redundancy # def value_counts ( the_list ): \"\"\" :param list[object] the_list: The list whose values we'll count :rtype: dict[object, int] :return: A dict from the value to its count \"\"\" output = {} for val in the_list : if val not in output : output [ val ] = 1 else : output [ val ] += 1 return output def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = value_counts ( left ) right_counts = value_counts ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts Better... But, there is no need to reinvent the wheel with the value_count function if it has already been implemented for you. The DRY-er Way # from collections import Counter def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = Counter ( left ) right_counts = Counter ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts The Python Standard Library has a collections library that has a Counter object already built. The WET Way: Plotting # Here is a soggy example of a function that makes plots. Good: Docstring, somewhat informative name. Bad: This function does multiple things, and uses a hard path ('2016.csv.gz'). def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 1 ]] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ]. mean (), second [ second . TYPE == PRECIPITATION_TYPE ]. mean ()) The DRY Way: Plotting # In this example, we've made a helper function. In this example, the function has a better name, doesn't load the dataframe within the function, and has a docstring specifying what the inputs and outputs are as well as their types. def plot_precipitation ( df , station_id , output_file = 'out.png' ): \"\"\" Plot the precipitation at the passed weather station and return the mean precipitation among all values. :param pd.DataFrame df: NOAA data (see parsers.py for more info) :param str station_id: The station to plot :param str output_file: Where to store the output plot :return: The mean precipitation in the data frame :rtype: float \"\"\" res_df = df [ df . STATION == station_id ] res_df [ res_df . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( output_file ) return res_df [ 'VALUE' ]. mean () def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) return ( plot_precipitation ( df , CHICAGO_STATION_NAMES [ 0 ] , output_file = 'first_fig.png' ), plot_precipitation ( df , CHICAGO_STATION_NAMES [ 1 ] , output_file = 'second_fig.png' )) Use functions to not repeat yourself Good rule of thumb: If you've gone 20 lines without making a comment (e.g., the docstring of a function), you likely should add one. Reduce Cognitive Load. # Follow PEP-8 # How long does it take you to understand what this function is doing? def GCD ( a , b ): \"\"\"Return the GCD of a and b\"\"\" a , b = max ( a , b ), min ( a , b ) return b if a % b == 0 else GCD ( b , a % b ) What makes this one better? # def gcd ( a , b ): \"\"\"Return the GCD of a and b :param int a: The first number (positive) :param int b: The second number (positive) :return: The GCD of a and b :rtype: int \"\"\" if a < b : b , a = a , b if a % b : return gcd ( b , a % b ) return b PEP-8 is your friend! # 79 character lines (comes from the days of punchcards) Use parenthesis for lines that span multiple lines function_names are snake_case ClassNames are CamelCase CONSTANT_NAMES are BIG_CASE One statement per line Use spaces not tabs! You can use a checker to PEP-8 your code. # pip install autopep8 autopep8 --in-place mypythonfile.py # Summary: Code is for humans, not computers. # Give things informative names Document inputs and outputs Don't repeat yourself (or others!) PEP-8 is your friend Exercises # Fix this class! # class myclass ( object ): def __init__ ( self , R , I ): self . R = R self . I = I def Multiply ( self , other ): return myclass ( self . R * other . R - self . I * other . I , self . R * other . I + self . I * other . R ) # Fix this function! # def bang ( n ): return n == 1 or ( n * bang ( n )) Fix this function! # def read_data ( filename ): \"\"\" Return the precipitation field from the csv passed in \"\"\" with open ( filename , 'r' ) as f : return [ line . split ( ',' ) [ 2 ] for line in f ]","title":"Legible, good code"},{"location":"curriculum/programming_best_practices/legible-good-code/#best-practices-writing-legible-good-code","text":"","title":"Best Practices: Writing Legible, Good Code"},{"location":"curriculum/programming_best_practices/legible-good-code/#motivation","text":"All fellows will have to write code that is usable and understandable by their peers and partners, and potentially other 3rd parties. What does that entail in practice? Many fellows are coming from academic backgrounds, have self-taught programming skills, and have never worked collaboratively on a software project. We want to help them establish good habits and avoid common mistakes. (Adapted from Tutorial by Kevin Wilson, 2016 DSSG Technical Mentor)","title":"Motivation"},{"location":"curriculum/programming_best_practices/legible-good-code/#for-whom-do-we-write-code","text":"","title":"For whom do we write code?"},{"location":"curriculum/programming_best_practices/legible-good-code/#what-you-write-for-people","text":"def fib ( n ): \"\"\" :param int n: The Fibonnaci index you want to return :return: The nth Fibonnaci number :rtype: int \"\"\" if n < 2 : return 1 else : return fib ( n - 1 ) + fib ( n - 2 ) Here is an example of a function. Why write this function? Well, literally, you give the function an integer n , and the function gives you back the n th Fibonnaci number. Writing code allows you to have the computer calculate the n th Fibonnaci number, which it can probably do much faster than you can, especially as n gets larger and larger. It also allows your human comrades to see for themselves how you rattle off arbitrary Fibonacci numbers so fast. But you don't really write code for the computer...","title":"What you write (for people)."},{"location":"curriculum/programming_best_practices/legible-good-code/#what-the-computer-sees-assembly-language-for-the-processor","text":"... because if you did, it would look like this. 2 0 LOAD_FAST 0 (n) 3 LOAD_CONST 1 (2) 6 COMPARE_OP 0 (<) 9 POP_JUMP_IF_FALSE 16 3 12 LOAD_CONST 2 (1) 15 RETURN_VALUE 5 >> 16 LOAD_GLOBAL 0 (fib) 19 LOAD_FAST 0 (n) 22 LOAD_CONST 2 (1) 25 BINARY_SUBTRACT 26 CALL_FUNCTION 1 29 LOAD_GLOBAL 0 (fib) 32 LOAD_FAST 0 (n) 35 LOAD_CONST 1 (2) 38 BINARY_SUBTRACT 39 CALL_FUNCTION 1 42 BINARY_ADD 43 RETURN_VALUE 44 LOAD_CONST 0 (None) 47 RETURN_VALUE","title":"What the computer sees (assembly language for the processor)."},{"location":"curriculum/programming_best_practices/legible-good-code/#seriously-the-code-is-for-you","text":"...one month from now when you've completely forgotten what the heck alpha was... ...or tomorrow when your teammate wonders why all those 3 s are in the database.... ...or when your system is on fire at 2 AM (the witching hour of production systems), you're tired, and your customers are calling wondering what the heck is going on over there (why are they awake, anyway?)... ...or when I'm doing code review for you...","title":"Seriously, the code is for you..."},{"location":"curriculum/programming_best_practices/legible-good-code/#there-is-only-one-law-of-good-coding-code-is-for-humans-not-for-computers","text":"This law has 4 consequences: Give things informative names. Document inputs and outputs. Don't repeat yourself. Reduce cognitive load: Use PEP-8.","title":"There is only one law of good coding: Code is for humans, not for computers."},{"location":"curriculum/programming_best_practices/legible-good-code/#give-things-informative-names","text":"What does the function below do? import math def doit ( x ): output = [] y = 2 while x != 1 : if x % y == 0 : output . append ( y ) x //= y else : y += 1 continue return output def magic ( input ): filter = set () return sorted ( x for x in y for y in input if ( len ( x ) < 20 and x not in filter ) or filter . add ( x )) def pretty_pictures (): df = pd . read_csv ( '2016.csv.gz' , names = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ]) first = df [ weather_df . STATION == 'US1ILCK0010' ] first [ first . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == 'US1ILCK0014' ] second [ second . TYPE == 'PRCP' ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return first [ first . TYPE == 'PRCP' ] . mean (), second [ second . TYPE == 'PRCP' ] . mean () Names should have meaning to the intended readers, e.g., You a week from now Your teammates tomorrow Your future teammates who have to deal with your code [ i , j , k ] for iterators are OK, alpha with a reference to a specific paper is not kwargs are a great place to name things CONSTANT_VALUES are too Do not fear long names; you have autocomplete. The TAB key is your friend.","title":"Give Things Informative Names"},{"location":"curriculum/programming_best_practices/legible-good-code/#document-inputs-and-outputs","text":"import math def factor ( x ): output = [] fact = 2 while x != 1 : if x % fact == 0 : output . append ( y ) x //= fact else : fact += 1 continue return output def unique_flatten ( the_input , max_length = 20 ): flattened_set = { val for val in row for row in the_input } filtered_list = [ val for val in flattened_set if len ( val ) < max_length ] filtered_list . sort () return filtered_list WEATHER_HEADERS = [ 'STATION' , 'DATE' , 'TYPE' , 'VALUE' , 'MEASUREMENT_FLAG' , 'QUALITY_FLAG' , 'SOURCE_FLAG' , 'OBS_TIME' ] PRECIPITATION_TYPE = 'PRCP' CHICAGO_STATION_NAMES = [ 'US1ILCK0010' , 'US1ILCK0014' ] def precipitation_in_chicago (): df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES ] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ] . plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ] . mean (), second [ second . TYPE == PRECIPITATION_TYPE ] . mean ()) Every function should have a docstring The docstring should: Describe the function briefly Explicitly document the inputs with :param type name: description Explicitly document the return value with :returns: description Explicitly document the return type with :rtype: Note this follows the Sphinx/RST syntax guide . You can also follow the Numpy Format . Just be consistent.","title":"Document Inputs and Outputs"},{"location":"curriculum/programming_best_practices/legible-good-code/#dont-repeat-yourself","text":"","title":"Don't Repeat Yourself"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-wet-write-everything-twice-way","text":"def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = {} for val in left : if val not in left_counts : left_counts [ val ] = 1 else : left_counts [ val ] += 1 right_counts = {} for val in right : if val not in right_counts : right_counts [ val ] = 1 else : right_counts [ val ] += 1 left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts In this case, the value counting logic is redundant and can be abstracted away into a function.","title":"The WET (Write Everything Twice) Way"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-dry-way-use-a-function-to-avoid-redundancy","text":"def value_counts ( the_list ): \"\"\" :param list[object] the_list: The list whose values we'll count :rtype: dict[object, int] :return: A dict from the value to its count \"\"\" output = {} for val in the_list : if val not in output : output [ val ] = 1 else : output [ val ] += 1 return output def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = value_counts ( left ) right_counts = value_counts ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts Better... But, there is no need to reinvent the wheel with the value_count function if it has already been implemented for you.","title":"The DRY Way: Use a function to avoid redundancy"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-dry-er-way","text":"from collections import Counter def max_intersection ( left , right ): \"\"\" Compute the value counts of the left and right lists and then return the maximum for each value. :param list[object] left: The left list :param list[object] right: The right list :rtype: dict[object, int] :return: A dictionary from the value to max(# occurrences in left, # occurrences in right) \"\"\" left_counts = Counter ( left ) right_counts = Counter ( right ) left_counts . update ({ key : max ( left_counts . get ( key , 0 ), val ) for key , val in right_counts . items ()}) return left_counts The Python Standard Library has a collections library that has a Counter object already built.","title":"The DRY-er Way"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-wet-way-plotting","text":"Here is a soggy example of a function that makes plots. Good: Docstring, somewhat informative name. Bad: This function does multiple things, and uses a hard path ('2016.csv.gz'). def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) first = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 0 ]] first [ first . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'first_fig.png' ) second = df [ weather_df . STATION == CHICAGO_STATION_NAMES [ 1 ]] second [ second . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( 'second_fig.png' ) return ( first [ first . TYPE == PRECIPITATION_TYPE ]. mean (), second [ second . TYPE == PRECIPITATION_TYPE ]. mean ())","title":"The WET Way: Plotting"},{"location":"curriculum/programming_best_practices/legible-good-code/#the-dry-way-plotting","text":"In this example, we've made a helper function. In this example, the function has a better name, doesn't load the dataframe within the function, and has a docstring specifying what the inputs and outputs are as well as their types. def plot_precipitation ( df , station_id , output_file = 'out.png' ): \"\"\" Plot the precipitation at the passed weather station and return the mean precipitation among all values. :param pd.DataFrame df: NOAA data (see parsers.py for more info) :param str station_id: The station to plot :param str output_file: Where to store the output plot :return: The mean precipitation in the data frame :rtype: float \"\"\" res_df = df [ df . STATION == station_id ] res_df [ res_df . TYPE == PRECIPITATION_TYPE ][ 'VALUE' ]. plot () plt . title ( \"Precipitation values\" ) plt . xlabel ( \"Day\" ) plt . ylabel ( \"Value\" ) plt . savefig ( output_file ) return res_df [ 'VALUE' ]. mean () def precipitation_in_chicago (): \"\"\" Saves plots of the precipitation from two Chicago weather stations in 2016 to `first_fig.png` and `second_fig.png` and returns the mean precipitation at them for the year. :return: The mean precipitation at two weather stations in 2016 :rtype: (float, float) \"\"\" df = pd . read_csv ( '2016.csv.gz' , names = WEATHER_HEADERS ) return ( plot_precipitation ( df , CHICAGO_STATION_NAMES [ 0 ] , output_file = 'first_fig.png' ), plot_precipitation ( df , CHICAGO_STATION_NAMES [ 1 ] , output_file = 'second_fig.png' )) Use functions to not repeat yourself Good rule of thumb: If you've gone 20 lines without making a comment (e.g., the docstring of a function), you likely should add one.","title":"The DRY Way: Plotting"},{"location":"curriculum/programming_best_practices/legible-good-code/#reduce-cognitive-load","text":"","title":"Reduce Cognitive Load."},{"location":"curriculum/programming_best_practices/legible-good-code/#follow-pep-8","text":"How long does it take you to understand what this function is doing? def GCD ( a , b ): \"\"\"Return the GCD of a and b\"\"\" a , b = max ( a , b ), min ( a , b ) return b if a % b == 0 else GCD ( b , a % b )","title":"Follow PEP-8"},{"location":"curriculum/programming_best_practices/legible-good-code/#what-makes-this-one-better","text":"def gcd ( a , b ): \"\"\"Return the GCD of a and b :param int a: The first number (positive) :param int b: The second number (positive) :return: The GCD of a and b :rtype: int \"\"\" if a < b : b , a = a , b if a % b : return gcd ( b , a % b ) return b","title":"What makes this one better?"},{"location":"curriculum/programming_best_practices/legible-good-code/#pep-8-is-your-friend","text":"79 character lines (comes from the days of punchcards) Use parenthesis for lines that span multiple lines function_names are snake_case ClassNames are CamelCase CONSTANT_NAMES are BIG_CASE One statement per line Use spaces not tabs!","title":"PEP-8 is your friend!"},{"location":"curriculum/programming_best_practices/legible-good-code/#you-can-use-a-checker-to-pep-8-your-code","text":"","title":"You can use a checker to PEP-8 your code."},{"location":"curriculum/programming_best_practices/legible-good-code/#pip-install-autopep8-autopep8-in-place-mypythonfilepy","text":"","title":"pip install autopep8"},{"location":"curriculum/programming_best_practices/legible-good-code/#summary-code-is-for-humans-not-computers","text":"Give things informative names Document inputs and outputs Don't repeat yourself (or others!) PEP-8 is your friend","title":"Summary: Code is for humans, not computers."},{"location":"curriculum/programming_best_practices/legible-good-code/#exercises","text":"","title":"Exercises"},{"location":"curriculum/programming_best_practices/legible-good-code/#fix-this-class","text":"","title":"Fix this class!"},{"location":"curriculum/programming_best_practices/legible-good-code/#class-myclassobject-def-__init__self-r-i-selfr-r-selfi-i-def-multiplyself-other-return-myclassselfr-otherr-selfi-otheri-selfr-otheri-selfi-otherr","text":"","title":"class myclass(object):"},{"location":"curriculum/programming_best_practices/legible-good-code/#fix-this-function","text":"def bang ( n ): return n == 1 or ( n * bang ( n ))","title":"Fix this function!"},{"location":"curriculum/programming_best_practices/legible-good-code/#fix-this-function_1","text":"def read_data ( filename ): \"\"\" Return the precipitation field from the csv passed in \"\"\" with open ( filename , 'r' ) as f : return [ line . split ( ',' ) [ 2 ] for line in f ]","title":"Fix this function!"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/","text":"Living in command land, or: how I learned to stop worrying and love the terminal # Where to start? # Navigating online (stackoverflow) and offline (--help & the man pages). # Terminal land is vast and may appear nebulous to the unseasoned scripter. Where do we start? Let search engines light the way, and manual pages hold your hand. Adding \"bash\" or \"in terminal\" to search terms like \"replace whitespaces\" or \"filesize\" will point you in the right direction. \"My internet is down!\" man pages are stored on your computer, so you can reference the documentation even when you are outside the internet! For example... Search: \"replace whitespaces in filename in bash\" Tip! The search engine DuckDuckGo returns StackOverflow answers as the first-hit-preview ;) find -name \"* *\" -type d | rename 's/ /_/g' find -name \"* *\" -type f | rename 's/ /_/g' So what else can we do with rename ? $ man rename > ... -n, -nono No action: print names of files to be renamed, but don't rename. This is helpful when you're not too confident about what your command will do. ... use /<keyword> to search for <keyword> in the manual. Command Line 101 # Mind the command # The first rule of command line is \"be careful what you wish for\". The computer will do exactly what you say, but human's may have trouble speaking the computer's language. This can be dangerous when you're running commands like rm (remove), or mv (move, also used for renaming files). You can \"echo\" your commands to just print the command text without actually running the command. This can save your files and sometimes even your jorb! (Tip! Don't delete all your data with a misplaced mv ) You can create dummy files to use for this tutorial sing the touch command, in case you don't want to operate on real files until you're comfortable with these commands. Let's start by creating a file with space bars in the name. touch space\\ bars\\ .txt Note the use of the escape character \\ to signal that we intend to use the space bar as a character in our filename string. Without the backslashes, the command is interepreted as touch with several separate arguments, so in fact... touch space bars .txt ...will create 3 files seperate files! space , bars , and .txt . Where am I? # pwd prints the name of the current working directory cd .. changes directory to one level/folder up cd ~/ goes to the home directory What's in my folder? # ls lists the contents in your current dictory. ls -l \"long listing\" format ( -l ) shows the filesize, date of last change, and file permissions tree lists the contents of the current directory and all sub-directories as a tree structure (great for peeking into folder structures!) tree -L 2 limits the tree expansion to 2 levels tree -hs shows file sizes ( -s ) in human-readable format ( -h ) What's in my file? # head -n10 $f shows the \"head\" of the file, in this case the top 10 lines tail -n10 $f shows the \"tail\" of the file tail -n10 $f | watch -n1 watches the tail of the file for any changes every second ( -n1 ) tail -f -n10 $f follows ( -f ) the tail of the file every time it changes, useful if you are checking the log of a running program wc $f counts words, lines and characters in a file (separate counts using -w or -l or -c ) Where is my file? # find -name \"<lost_file_name>\" -type f finds files by name find -name \"<lost_dir_name>\" -type d finds directories by name Renaming files # Rename files with rename . For example, to replace all space bars with underscores: rename 's/ /_/g' space\\ bars\\ .txt This command substitutes ( s ) space bars ( / / ) for underscores ( /_/ ) in the entire file name (globally, g ). (The 3 slashes can be replaced by any sequence of 3 characters, so 's# #_#g' would also work and can sometimes be more legible, for example when you need to escape a special character with a backslash.) You can replace multiple characters at a time by using a simple logical OR \"regular expression\" ( | ) such as [ |?] which will replace every space bar or question mark. rename 's/[ |?]/_/g' space\\ bars?.txt (The file will be renamed to space_bars_.txt ) Bonus points: rename 'y/A-Z/a-z/' renames files to all-lowercase rename 'y/a-z/A-Z/' renames files to all-uppercase Caveats for git users # Moving files around on your computer can confuse git. If you are git-tracking a file, make sure to use the following alternatives so git knows what's going on. git mv /source/path/$move_me /destination/path/$move_me git rm $remove_me Data structures # Variables are declared with a single \"=\" and no spaces. location=\"Lisbon\" Arrays are enclosed in brackets. array=(abc 123 doremi) If you echo the array, you will get the first element. $ echo $array > abc To echo the full array, expand the array with @: $ echo ${array[@]} > abc 123 doremi Control flow and logic # Every bash statement is separated by a semicolon. This allows us to write one-liners that would normally be spread out over multiple lines. So a for loop... for i in {a..z}; do echo $i; done ...can be written as a one-liner: for i in {a..z}; do echo $i; done Tricks # Brace expansion allows you to iterate over a range of possible variables. $ echo {0..9} > 0 1 2 3 4 5 6 7 8 9 $ echo {0..9..2} > 0 2 4 8 $ echo happy_birthday.{wav,mp3,flac} > happy_birthday.wav happy_birthday.mp3 happy_birthday.flac Functions # We can write functions in shell scripts as well! The syntax looks like this... function_name(args) { function_body } You can even define shell functions inside your ~/.bashrc profile when a simple alias just won't do... For example, run a jupyter notebook remotely through an SSH tunnel and forward the connection to your localhost: jupyter_local() { ssh -i ~/.ssh/<key>.pem -NfL \"$1\":localhost:\"$2\" <user>@<host>; } Then we can just write... jupyter_local 8888 8889 ...to run a jupyter server on <host> (@ port 8888) and view it on our local machine (@port 8889) Surfing the net # You can send HTTP requests to URLs from the command line. You can retrieve a page by sending a GET request: curl -iX GET https://duckduckgo.com Or just the response header: curl -I https://duckduckgo.com From which you can parse out the status code, which is useful to see if the page is responding (200 OK) or non-existinent (404 File Not Found), etc. curl -I https://duckduckgo.com 2>/dev/null | head -n 1 | cut -d$' ' -f2 where... 2>/dev/null redirects the stderr to oblivion head -n 1 reads the top line only cut -d$' ' -f2 separates the line by the divider (spacebar) and takes the 2nd field (which is the numerical HTTP response status code). Working remotely via SSH # When working via SSH, a connection interruption can terminate your running scripts, lose your environment varaibles and lose your command history! D: There's a way to avoid this. Actually there's two: screen and tmux are two programs that allow you to run a terminal session remotely on a remote server which you can interact with from your own machine via SSH. So if you ever lose connection to the server, your terminal session is still running - you just have to log back into it. You can also run multiple independent terminal sessions on the same server this way. tmux (aka: terminal multiplexer) # # \"Ping\" the server to check if it's reachable (it \"pongs\" back... get it?) ping <server> # ssh into the server ssh <user>@<server> # Open a tmux session tmux # List existing sessions tmux ls # Attach (a) to a target session (-t #) tmux a -t 1 # Rename the current window Ctrl+b+, # Kill the current pane Ctrl+b+x # Create a new pane Ctrl+b+c # Split windows horizontally into two Ctrl+b+\" # Split windows vertically into two Ctrl+b+% # Tohttps://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ggle between horizontal/vertical splits Ctrl+b+space Tmux can easily be configured by editing the tmux configuration file at ~/.tmux.conf If you search \"tmux cheatsheet\" on (DuckDuckGo.com)[https://duckduckgo.com], the preview search result reveals some more useful commands [: VIM (Vi iMproved) - text editor # Why bother? Vim is a powerful, lightweight, open-source, cross-platform text editor, that comes pre-installed on Unix systems. Vi was written in 1976 by Bill Joy at Sun Microsystems, and has been improved in 1991 to Vim. Vim was designed for maximum efficiency and minimum bandwidth when working on old modems. It does not require use of the mouse or arrow keys. Much of learning Vim is just habit and muscle memory, in the first place this can be frustrating, but soon becomes second nature. But I'm scared? Don't worry here are some useful hints, tips, and tricks for using vim. Pleas note if at any point during this session you feel bewildered, nauseous, or perhaps euphoric, remain calm and press the Esc key to get back to normal. Where should I start? A comprehensive although slightly dry start point for learning vim is through the vimtutor document available as standard with vim (just type vimtutor and hit Enter). A more fun way to get used to moving in vim is playing this fun maze game. https://vim-adventures.com/ A useful cheatsheet: https://vim.rtorr.com/ The Bare Necessities: Vim has three modes: 1. Normal: this is for normal movement through a file (press I for Insert, or V for visual) 2. Insert: this is for editing files and adding text (press Esc to get back to Normal) 3. Visual: this is for highlighting lines in files (press Esc to get back to Normal) Navigation: # Movement is through h , j , k , and l go to the top of the file with gg go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line 0 jump to the end of the line $ Editing: # delete a character x delete a line dd delete 10 lines 10dd change a word cw undo a change u redo a change `Ctrl+r' go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line `` jump to the end of the line $ start editing on line below o start editing at end of line A Highlighting (visual mode): # select a line V select 8 lines 8V yank or copy y paste p search /pattern see next search match n see previous search match N Exiting # Okay enough, get me out of here: * quit a file :q * write changes to a file :w (normal humanoids call this saving a file) * no, really get me out of here, I don't care about saving :q! The stuff they don't teach: * :%s/old/new/gc substitute old pattern for new globally but check each (commonly humanoids refer to this as find and replace) * :10,20s/old/new/g substitute the old pattern for the new only between lines 10 and 20 * select a column of text Ctrl+V+j+j+j+j * comment a column of text Ctrl+V+j+j+j+j+# * go to file explorer :Ex * open a file on my remote vim scp://path/to/file/ * change your ~/.vimrc * set nu add numbering * set hlsearch highlight search results If you are convinced and want to go one step further you can configure vim as an IDE for python development here https://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ vim promotes social good Vim is Charityware. You can use and copy it as much as you like, but you are encouraged to make a donation for needy children in Uganda. Please see kcc below or visit the ICCF web site, available at these URLs: http://iccf-holland.org/ http://www.vim.org/iccf/ http://www.iccf.nl/ You can also sponsor the development of Vim. Vim sponsors can vote for features. See sponsor. The money goes to Uganda anyway. Bonus points # Rogue terminals # We all make mistakes. Sometimes we make mistakes in infinite loops. What do we do when \"Ctrl+C\" is not enough? top or htop allow us to see what processes are running on our computer. (cf. System Monitor @ Mac) Every process has an ID ( pid ) which we can use to send a kill command to it. ps -ef | grep badprocess | awk '{print $2}' | kill `xargs $1` Sometimes badprocess spawns other badprocess processes... so we can loop over them all. ps -ef | grep badprocess | awk '{print $2}' | for f in `xargs $1`; do kill $f; done Parallel programming (sort of) # Run parallel processes on a multi-core system using GNU parallel. Typically, High-Performance Computing clusters have multi-cores (think quad-quad-quad-core), but running your script on the HPC is not enough to exploit it. What if you could run your script multiple times across each of the cores? NUM_JOBS=16 parallel -j=$NUM_JOBS --dry-run <script.sh> Remove --dry-run to actually run the script ;) dry-run shows you what will happen without actually running any code - it's a good way to double-check the expected behaviour of your script before. Custom prompts # You can customise your command prompt by changing the $PS1 variable. Motivational cow # If you need a little inspiration, let the fortune package brighten up your day! Even better, let an ASCII cow lighten up your day! # Install the fortune and cowsay packages sudo apt-get install cowsay fortune # \"Pipe\" the output of fortune into the cowsay command fortune | cowthink _______________________________ / Don't Worry, Be Happy. \\ \\ -- Meher Baba / ------------------------------- O ^__^ o (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Living in command land, or:<br/>how I learned to stop worrying and love the terminal"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#living-in-command-land-orhow-i-learned-to-stop-worrying-and-love-the-terminal","text":"","title":"Living in command land, or:how I learned to stop worrying and love the terminal"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#where-to-start","text":"","title":"Where to start?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#navigating-online-stackoverflow-and-offline-help-the-man-pages","text":"Terminal land is vast and may appear nebulous to the unseasoned scripter. Where do we start? Let search engines light the way, and manual pages hold your hand. Adding \"bash\" or \"in terminal\" to search terms like \"replace whitespaces\" or \"filesize\" will point you in the right direction. \"My internet is down!\" man pages are stored on your computer, so you can reference the documentation even when you are outside the internet! For example... Search: \"replace whitespaces in filename in bash\" Tip! The search engine DuckDuckGo returns StackOverflow answers as the first-hit-preview ;) find -name \"* *\" -type d | rename 's/ /_/g' find -name \"* *\" -type f | rename 's/ /_/g' So what else can we do with rename ? $ man rename > ... -n, -nono No action: print names of files to be renamed, but don't rename. This is helpful when you're not too confident about what your command will do. ... use /<keyword> to search for <keyword> in the manual.","title":"Navigating online (stackoverflow) and offline (--help &amp; the man pages)."},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#command-line-101","text":"","title":"Command Line 101"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#mind-the-command","text":"The first rule of command line is \"be careful what you wish for\". The computer will do exactly what you say, but human's may have trouble speaking the computer's language. This can be dangerous when you're running commands like rm (remove), or mv (move, also used for renaming files). You can \"echo\" your commands to just print the command text without actually running the command. This can save your files and sometimes even your jorb! (Tip! Don't delete all your data with a misplaced mv ) You can create dummy files to use for this tutorial sing the touch command, in case you don't want to operate on real files until you're comfortable with these commands. Let's start by creating a file with space bars in the name. touch space\\ bars\\ .txt Note the use of the escape character \\ to signal that we intend to use the space bar as a character in our filename string. Without the backslashes, the command is interepreted as touch with several separate arguments, so in fact... touch space bars .txt ...will create 3 files seperate files! space , bars , and .txt .","title":"Mind the command"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#where-am-i","text":"pwd prints the name of the current working directory cd .. changes directory to one level/folder up cd ~/ goes to the home directory","title":"Where am I?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#whats-in-my-folder","text":"ls lists the contents in your current dictory. ls -l \"long listing\" format ( -l ) shows the filesize, date of last change, and file permissions tree lists the contents of the current directory and all sub-directories as a tree structure (great for peeking into folder structures!) tree -L 2 limits the tree expansion to 2 levels tree -hs shows file sizes ( -s ) in human-readable format ( -h )","title":"What's in my folder?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#whats-in-my-file","text":"head -n10 $f shows the \"head\" of the file, in this case the top 10 lines tail -n10 $f shows the \"tail\" of the file tail -n10 $f | watch -n1 watches the tail of the file for any changes every second ( -n1 ) tail -f -n10 $f follows ( -f ) the tail of the file every time it changes, useful if you are checking the log of a running program wc $f counts words, lines and characters in a file (separate counts using -w or -l or -c )","title":"What's in my file?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#where-is-my-file","text":"find -name \"<lost_file_name>\" -type f finds files by name find -name \"<lost_dir_name>\" -type d finds directories by name","title":"Where is my file?"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#renaming-files","text":"Rename files with rename . For example, to replace all space bars with underscores: rename 's/ /_/g' space\\ bars\\ .txt This command substitutes ( s ) space bars ( / / ) for underscores ( /_/ ) in the entire file name (globally, g ). (The 3 slashes can be replaced by any sequence of 3 characters, so 's# #_#g' would also work and can sometimes be more legible, for example when you need to escape a special character with a backslash.) You can replace multiple characters at a time by using a simple logical OR \"regular expression\" ( | ) such as [ |?] which will replace every space bar or question mark. rename 's/[ |?]/_/g' space\\ bars?.txt (The file will be renamed to space_bars_.txt ) Bonus points: rename 'y/A-Z/a-z/' renames files to all-lowercase rename 'y/a-z/A-Z/' renames files to all-uppercase","title":"Renaming files"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#caveats-for-git-users","text":"Moving files around on your computer can confuse git. If you are git-tracking a file, make sure to use the following alternatives so git knows what's going on. git mv /source/path/$move_me /destination/path/$move_me git rm $remove_me","title":"Caveats for git users"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#data-structures","text":"Variables are declared with a single \"=\" and no spaces. location=\"Lisbon\" Arrays are enclosed in brackets. array=(abc 123 doremi) If you echo the array, you will get the first element. $ echo $array > abc To echo the full array, expand the array with @: $ echo ${array[@]} > abc 123 doremi","title":"Data structures"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#control-flow-and-logic","text":"Every bash statement is separated by a semicolon. This allows us to write one-liners that would normally be spread out over multiple lines. So a for loop... for i in {a..z}; do echo $i; done ...can be written as a one-liner: for i in {a..z}; do echo $i; done","title":"Control flow and logic"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#tricks","text":"Brace expansion allows you to iterate over a range of possible variables. $ echo {0..9} > 0 1 2 3 4 5 6 7 8 9 $ echo {0..9..2} > 0 2 4 8 $ echo happy_birthday.{wav,mp3,flac} > happy_birthday.wav happy_birthday.mp3 happy_birthday.flac","title":"Tricks"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#functions","text":"We can write functions in shell scripts as well! The syntax looks like this... function_name(args) { function_body } You can even define shell functions inside your ~/.bashrc profile when a simple alias just won't do... For example, run a jupyter notebook remotely through an SSH tunnel and forward the connection to your localhost: jupyter_local() { ssh -i ~/.ssh/<key>.pem -NfL \"$1\":localhost:\"$2\" <user>@<host>; } Then we can just write... jupyter_local 8888 8889 ...to run a jupyter server on <host> (@ port 8888) and view it on our local machine (@port 8889)","title":"Functions"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#surfing-the-net","text":"You can send HTTP requests to URLs from the command line. You can retrieve a page by sending a GET request: curl -iX GET https://duckduckgo.com Or just the response header: curl -I https://duckduckgo.com From which you can parse out the status code, which is useful to see if the page is responding (200 OK) or non-existinent (404 File Not Found), etc. curl -I https://duckduckgo.com 2>/dev/null | head -n 1 | cut -d$' ' -f2 where... 2>/dev/null redirects the stderr to oblivion head -n 1 reads the top line only cut -d$' ' -f2 separates the line by the divider (spacebar) and takes the 2nd field (which is the numerical HTTP response status code).","title":"Surfing the net"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#working-remotely-via-ssh","text":"When working via SSH, a connection interruption can terminate your running scripts, lose your environment varaibles and lose your command history! D: There's a way to avoid this. Actually there's two: screen and tmux are two programs that allow you to run a terminal session remotely on a remote server which you can interact with from your own machine via SSH. So if you ever lose connection to the server, your terminal session is still running - you just have to log back into it. You can also run multiple independent terminal sessions on the same server this way.","title":"Working remotely via SSH"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#tmuxaka-terminal-multiplexer","text":"# \"Ping\" the server to check if it's reachable (it \"pongs\" back... get it?) ping <server> # ssh into the server ssh <user>@<server> # Open a tmux session tmux # List existing sessions tmux ls # Attach (a) to a target session (-t #) tmux a -t 1 # Rename the current window Ctrl+b+, # Kill the current pane Ctrl+b+x # Create a new pane Ctrl+b+c # Split windows horizontally into two Ctrl+b+\" # Split windows vertically into two Ctrl+b+% # Tohttps://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ggle between horizontal/vertical splits Ctrl+b+space Tmux can easily be configured by editing the tmux configuration file at ~/.tmux.conf If you search \"tmux cheatsheet\" on (DuckDuckGo.com)[https://duckduckgo.com], the preview search result reveals some more useful commands [:","title":"tmux(aka: terminal multiplexer)"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#vim-vi-improved-text-editor","text":"Why bother? Vim is a powerful, lightweight, open-source, cross-platform text editor, that comes pre-installed on Unix systems. Vi was written in 1976 by Bill Joy at Sun Microsystems, and has been improved in 1991 to Vim. Vim was designed for maximum efficiency and minimum bandwidth when working on old modems. It does not require use of the mouse or arrow keys. Much of learning Vim is just habit and muscle memory, in the first place this can be frustrating, but soon becomes second nature. But I'm scared? Don't worry here are some useful hints, tips, and tricks for using vim. Pleas note if at any point during this session you feel bewildered, nauseous, or perhaps euphoric, remain calm and press the Esc key to get back to normal. Where should I start? A comprehensive although slightly dry start point for learning vim is through the vimtutor document available as standard with vim (just type vimtutor and hit Enter). A more fun way to get used to moving in vim is playing this fun maze game. https://vim-adventures.com/ A useful cheatsheet: https://vim.rtorr.com/ The Bare Necessities: Vim has three modes: 1. Normal: this is for normal movement through a file (press I for Insert, or V for visual) 2. Insert: this is for editing files and adding text (press Esc to get back to Normal) 3. Visual: this is for highlighting lines in files (press Esc to get back to Normal)","title":"VIM (Vi iMproved) - text editor"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#navigation","text":"Movement is through h , j , k , and l go to the top of the file with gg go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line 0 jump to the end of the line $","title":"Navigation:"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#editing","text":"delete a character x delete a line dd delete 10 lines 10dd change a word cw undo a change u redo a change `Ctrl+r' go to the end of the file with G go to line ten 10G move forward 1 word w move forward 10 words 10w move back 1 word b move back 10 words 10b jump to the start of the line `` jump to the end of the line $ start editing on line below o start editing at end of line A","title":"Editing:"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#highlighting-visual-mode","text":"select a line V select 8 lines 8V yank or copy y paste p search /pattern see next search match n see previous search match N","title":"Highlighting (visual mode):"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#exiting","text":"Okay enough, get me out of here: * quit a file :q * write changes to a file :w (normal humanoids call this saving a file) * no, really get me out of here, I don't care about saving :q! The stuff they don't teach: * :%s/old/new/gc substitute old pattern for new globally but check each (commonly humanoids refer to this as find and replace) * :10,20s/old/new/g substitute the old pattern for the new only between lines 10 and 20 * select a column of text Ctrl+V+j+j+j+j * comment a column of text Ctrl+V+j+j+j+j+# * go to file explorer :Ex * open a file on my remote vim scp://path/to/file/ * change your ~/.vimrc * set nu add numbering * set hlsearch highlight search results If you are convinced and want to go one step further you can configure vim as an IDE for python development here https://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/ vim promotes social good Vim is Charityware. You can use and copy it as much as you like, but you are encouraged to make a donation for needy children in Uganda. Please see kcc below or visit the ICCF web site, available at these URLs: http://iccf-holland.org/ http://www.vim.org/iccf/ http://www.iccf.nl/ You can also sponsor the development of Vim. Vim sponsors can vote for features. See sponsor. The money goes to Uganda anyway.","title":"Exiting"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#bonus-points","text":"","title":"Bonus points"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#rogue-terminals","text":"We all make mistakes. Sometimes we make mistakes in infinite loops. What do we do when \"Ctrl+C\" is not enough? top or htop allow us to see what processes are running on our computer. (cf. System Monitor @ Mac) Every process has an ID ( pid ) which we can use to send a kill command to it. ps -ef | grep badprocess | awk '{print $2}' | kill `xargs $1` Sometimes badprocess spawns other badprocess processes... so we can loop over them all. ps -ef | grep badprocess | awk '{print $2}' | for f in `xargs $1`; do kill $f; done","title":"Rogue terminals"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#parallel-programming-sort-of","text":"Run parallel processes on a multi-core system using GNU parallel. Typically, High-Performance Computing clusters have multi-cores (think quad-quad-quad-core), but running your script on the HPC is not enough to exploit it. What if you could run your script multiple times across each of the cores? NUM_JOBS=16 parallel -j=$NUM_JOBS --dry-run <script.sh> Remove --dry-run to actually run the script ;) dry-run shows you what will happen without actually running any code - it's a good way to double-check the expected behaviour of your script before.","title":"Parallel programming (sort of)"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#custom-prompts","text":"You can customise your command prompt by changing the $PS1 variable.","title":"Custom prompts"},{"location":"curriculum/programming_best_practices/living-in-the-terminal/#motivational-cow","text":"If you need a little inspiration, let the fortune package brighten up your day! Even better, let an ASCII cow lighten up your day! # Install the fortune and cowsay packages sudo apt-get install cowsay fortune # \"Pipe\" the output of fortune into the cowsay command fortune | cowthink _______________________________ / Don't Worry, Be Happy. \\ \\ -- Meher Baba / ------------------------------- O ^__^ o (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Motivational cow"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/","text":"dotfiles - make working on any computer and feel just like home # On this tutorial, you'll learn about dotfiles , what they are, why are they important and how to use them. What are dotfiles anyway? # Dotfiles are used to configure system settings, you can configure anything from them. From text editor syntax to a list of commands to execute each time you open a terminal session. Dotfiles names start with . , e.g. .bash_profile , .Rprofile , .vimrc and most of them reside in your home folder. As you get familiar with the command line, you are going to start tweaking your computer with personal settings. Maybe setting special shortcuts for common commands (e.g. typing jnb to start Jupyter instead of typing jupyter notebook ). As your system becomes more and more customized, it's going to be pretty different to the original configuration. So imagine you have dozens of nice shortcuts and configuration settings for your favorite applications, then you start working on the DSSG server and all the magic is gone... not cool. A common practice is to store your files in a git repository. This way you have a history of the modifications you've done but more important, a copy you can grab from anywhere (e.g. the DSSG server) and get all your magic. Finding your dotfiles # Most applications store a dotfile in your home folder, type the following in the command line to see yours: find . -name '.*' -maxdepth 1 Here are some of mine: .Rprofile #settings for your R sessions .vimrc #vim settings .bash_profile #shell settings Let's see how my .Rprofile looks: ## Change colors when running R in the terminal if ( Sys.getenv ( \"TERM\" ) == \"xterm-256color\" ) library ( \"colorout\" ) If you have an .Rprofile file, Whatever it is there will be executed when you start an R session. Mine just loads a package colorout which adds nice colors to the R interpreter. Note: even though every dotfile starts with . , not everything that starts with a . is a dotfile. For example .DS_Store is a file you'll find on many folder if you use OS X, this file stores custom attributes for the folder but you don't want to modify it directly, it's just a file the system uses to keep track of folder customizations (e.g. changing the icon) .bashrc and .bash_profile # When you open a terminal, a program called bash starts, this program let's you execute commands such as cd , ls , etc. Bash is highly configurable through its dotfiles: .bashrc and .bash_profile . There are some differences between those two and they get executed at different times, but a nice setting to get started is to make .bash_profile call .bashrc and set your configuration file in the later. To to that follow this steps: # open .bash_profile open ~/.bash_profile Your default editor will open the file, chances are the file contains some settings already, to avoid breaking your system, do not delete anything and just put this at the top of the file. # just load ~/.bashrc if [ -f ~/.bashrc ] ; then source ~/.bashrc fi Now you can start twerking your shell, for example adding shortcuts. Let's create one that outputs only folders in our current working directory. First, open your .bashrc : open ~/.bashrc Add this line: # List only directories alias lsd = 'ls -l | grep \"^d\"' Save the file. Close the terminal and open a new one. Now, every time you execute lsd , your command line will print only folders in the current directory and not the files. Ok, that was a pretty simple example, not let's see how to use git. Using git to manage your dotfiles # As I mentioned before, your dotfiles live in your home folder (type cd ~; pwd to see which is yours). Your home folder contains a lot of stuff and you probably don't want to create a git repo there (please don't). To solve this issue we can do the following: create a folder anywhere in your computer, create your dotfiles there and then link them to your home folder, where your applications expect your dotfiles to be. Let's imagine you want to save your dotfiles in ~/dotfiles . Run the following to create the folder add some files and start a git repository: # create and move to the folder mkdir ~/dotfiles ; cd ~/dotfiles # get the content from your original .bashrc and copy it in your # .bashrc stored in ~/dotfiles, to the same with .bash_profile cat ~/.bashrc > ~/dotfiles/.bashrc cat ~/.bash_profile > ~/dotfiles/.bash_profile # init repo and commit git init git add --all git commit -m 'dotfiles are awesome' Now you have a copy of your .bashrc and .bash_profile outside your home folder and you created a repo to store them. But there's one step missing, if you modify your dotfiles in ~/dotfiles , your computer won't do anything because it will look in your home folder. To fix it we need to link our files in ~/dotfiles to our home folder. To do that we'll create symlinks , which are basically pointers to files, that way you can store your dotfiles anywhere and your computer is still going to find them. # link files in ~/dotfiles to your home folder ln -s ~/dotfiles/.bashrc ~/.bashrc ln -s ~/dotfiles/.bash_profile ~/.bash_profile Now, you can modify, commit, push, pull from ~/dotfiles and still make your computer find them in your home folder! Now, create a remote repository to host them on github. If you don't know how, check out the git tutorial . Using your dot files in another machine # At this point you setup your dotfiles (only two for now) using git, you can version them and backup using github. Let's see how to bring your dotfiles to a new machine. First, login in the new machine and clone your repo: git clone https://github.com/youruser/yourrepo cd yourrepo You just got your files, now it's time to link them to your home folder on this new machine. ln -s .bashrc ~/.bashrc ln -s .bash_profile ~/.bash_profile Now, you can use your local settings in the server! It's all about automation # Manually linking each dotfiles is tedious, let's automate it. The easiest way of doing it is to add a script in your repo to run the code to create the links, the problem is that every time you create a new dotfile, you'll also need to update the script. If you only have a couple of dotfiles this is fine. But if you want superpowers, you can automate the process so next time you use a new machine, setting up your dot files will look like this: git clone https://github.com/youruser/yourrepo cd yourrepo ./install There are many ways of doing the ./install step but you need to be familiar with bash. If you want to see examples of it, see this . Resources # dotfiles - unofficial guide to dotfiles on Github","title":"Pimp my dotfiles!"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#dotfiles-make-working-on-any-computer-and-feel-just-like-home","text":"On this tutorial, you'll learn about dotfiles , what they are, why are they important and how to use them.","title":"dotfiles - make working on any computer and feel just like home"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#what-are-dotfiles-anyway","text":"Dotfiles are used to configure system settings, you can configure anything from them. From text editor syntax to a list of commands to execute each time you open a terminal session. Dotfiles names start with . , e.g. .bash_profile , .Rprofile , .vimrc and most of them reside in your home folder. As you get familiar with the command line, you are going to start tweaking your computer with personal settings. Maybe setting special shortcuts for common commands (e.g. typing jnb to start Jupyter instead of typing jupyter notebook ). As your system becomes more and more customized, it's going to be pretty different to the original configuration. So imagine you have dozens of nice shortcuts and configuration settings for your favorite applications, then you start working on the DSSG server and all the magic is gone... not cool. A common practice is to store your files in a git repository. This way you have a history of the modifications you've done but more important, a copy you can grab from anywhere (e.g. the DSSG server) and get all your magic.","title":"What are dotfiles anyway?"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#finding-your-dotfiles","text":"Most applications store a dotfile in your home folder, type the following in the command line to see yours: find . -name '.*' -maxdepth 1 Here are some of mine: .Rprofile #settings for your R sessions .vimrc #vim settings .bash_profile #shell settings Let's see how my .Rprofile looks: ## Change colors when running R in the terminal if ( Sys.getenv ( \"TERM\" ) == \"xterm-256color\" ) library ( \"colorout\" ) If you have an .Rprofile file, Whatever it is there will be executed when you start an R session. Mine just loads a package colorout which adds nice colors to the R interpreter. Note: even though every dotfile starts with . , not everything that starts with a . is a dotfile. For example .DS_Store is a file you'll find on many folder if you use OS X, this file stores custom attributes for the folder but you don't want to modify it directly, it's just a file the system uses to keep track of folder customizations (e.g. changing the icon)","title":"Finding your dotfiles"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#bashrc-and-bash_profile","text":"When you open a terminal, a program called bash starts, this program let's you execute commands such as cd , ls , etc. Bash is highly configurable through its dotfiles: .bashrc and .bash_profile . There are some differences between those two and they get executed at different times, but a nice setting to get started is to make .bash_profile call .bashrc and set your configuration file in the later. To to that follow this steps: # open .bash_profile open ~/.bash_profile Your default editor will open the file, chances are the file contains some settings already, to avoid breaking your system, do not delete anything and just put this at the top of the file. # just load ~/.bashrc if [ -f ~/.bashrc ] ; then source ~/.bashrc fi Now you can start twerking your shell, for example adding shortcuts. Let's create one that outputs only folders in our current working directory. First, open your .bashrc : open ~/.bashrc Add this line: # List only directories alias lsd = 'ls -l | grep \"^d\"' Save the file. Close the terminal and open a new one. Now, every time you execute lsd , your command line will print only folders in the current directory and not the files. Ok, that was a pretty simple example, not let's see how to use git.","title":".bashrc and .bash_profile"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#using-git-to-manage-your-dotfiles","text":"As I mentioned before, your dotfiles live in your home folder (type cd ~; pwd to see which is yours). Your home folder contains a lot of stuff and you probably don't want to create a git repo there (please don't). To solve this issue we can do the following: create a folder anywhere in your computer, create your dotfiles there and then link them to your home folder, where your applications expect your dotfiles to be. Let's imagine you want to save your dotfiles in ~/dotfiles . Run the following to create the folder add some files and start a git repository: # create and move to the folder mkdir ~/dotfiles ; cd ~/dotfiles # get the content from your original .bashrc and copy it in your # .bashrc stored in ~/dotfiles, to the same with .bash_profile cat ~/.bashrc > ~/dotfiles/.bashrc cat ~/.bash_profile > ~/dotfiles/.bash_profile # init repo and commit git init git add --all git commit -m 'dotfiles are awesome' Now you have a copy of your .bashrc and .bash_profile outside your home folder and you created a repo to store them. But there's one step missing, if you modify your dotfiles in ~/dotfiles , your computer won't do anything because it will look in your home folder. To fix it we need to link our files in ~/dotfiles to our home folder. To do that we'll create symlinks , which are basically pointers to files, that way you can store your dotfiles anywhere and your computer is still going to find them. # link files in ~/dotfiles to your home folder ln -s ~/dotfiles/.bashrc ~/.bashrc ln -s ~/dotfiles/.bash_profile ~/.bash_profile Now, you can modify, commit, push, pull from ~/dotfiles and still make your computer find them in your home folder! Now, create a remote repository to host them on github. If you don't know how, check out the git tutorial .","title":"Using git to manage your dotfiles"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#using-your-dot-files-in-another-machine","text":"At this point you setup your dotfiles (only two for now) using git, you can version them and backup using github. Let's see how to bring your dotfiles to a new machine. First, login in the new machine and clone your repo: git clone https://github.com/youruser/yourrepo cd yourrepo You just got your files, now it's time to link them to your home folder on this new machine. ln -s .bashrc ~/.bashrc ln -s .bash_profile ~/.bash_profile Now, you can use your local settings in the server!","title":"Using your dot files in another machine"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#its-all-about-automation","text":"Manually linking each dotfiles is tedious, let's automate it. The easiest way of doing it is to add a script in your repo to run the code to create the links, the problem is that every time you create a new dotfile, you'll also need to update the script. If you only have a couple of dotfiles this is fine. But if you want superpowers, you can automate the process so next time you use a new machine, setting up your dot files will look like this: git clone https://github.com/youruser/yourrepo cd yourrepo ./install There are many ways of doing the ./install step but you need to be familiar with bash. If you want to see examples of it, see this .","title":"It's all about automation"},{"location":"curriculum/programming_best_practices/pimp-my-dotfiles/#resources","text":"dotfiles - unofficial guide to dotfiles on Github","title":"Resources"},{"location":"curriculum/programming_best_practices/reproducible-software/","text":"Making Projects Reproducible # Scientific software is often developed and used by a single person. It is all too common in academia to be handed a postdoc or graduate student's old code and be unable to replicate the original study, run the software outside of the original development machine, or even get the software to work at all. The goal of this tutorial is to provide some guidelines to make your summer projects reproducible -- this means your project can be installed on another computer and give the same results you got over the summer. At the end of the summer, your project should be understandable and transferable to your future-self and anyone else who may want to pick up where you left off without having to constantly email you about how to get your project running. (Note: Your future-self doesn't have the luxury of being able to email your past-self). What is a reproducible project? # One that... works for someone other than the original team can be easily installed on another computer has documentation that describes any dependencies and how to install them comes with enough tests to indicate the software is running properly README.md(rst) # All projects should have a README that communicates the following: What the project is about A short description of the project (i.e. the problem you are trying to solve). The required dependencies to run the software The can be in the form of a requirements.txt file for Python that lists the dependencies and version numbers. The system-level dependencies. Installation instructions How to install your software and associated binaries. This can be in the form of instructions on how to use pip , apt , yum , or some other binary package manager. Example usage The inputs and outputs of your software (i.e. how to use it) with code examples. Attribution/Licensing Who did what and how others can use your software. Examples: - Chicago Food Inspections - DSSG Police EIS - Linux Kernel What to Do # Use virtual environments . Use automation tools like Make or Drake Keep your directory structure intuitive, interpretable and easy to understand . Keep your database free of \"junk tables.\" Keep only what you need and what's current. Junk tables will only confuse your future-self or others that come fresh to the project. Merge all branches into master. Branches are for adding features or patches. When you have added said feature or patch and you know you won't break the master branch, merge into master and delete the branch. Write commit messages in such a way that your log is helpful (see Git and Github tutorial .) Periodically make database backups . Write unit tests and use continuous integration so you can catch bugs quickly, particularly when you are merging new features into master. (See testing tutorial .) Document all of your functions with docstrings. (See legible, good code tutorial .) Write your python code following the PEP8 standard. (See legible, good code tutorial .) Use (4) spaces instead of tabs in your Python code for indentation. What NOT to Do # Use hard-coded paths . Require Sudo/root privileges to install your project. You can't anticipate whether or not someone will have root access to the machine they are installing your project on, so don't count on it. Additionally, you shouldn't require users to create separate user names for your project. Use non-standard formats for inputs (stick to YAML , XML , JSON , CLA , etc). My one exception to this rule is log files - which you should provide an example of in a README. Otherwise it is easier to just stick with what is already in use. Have a messy repo with random files everywhere . This is confusing, irritating and cancerous to productive enterprise. Commit data or sensitive information like database passcodes to the GitHub repo. Your repository is for your codebase, not the data. Furthermore, your data may be sensitive and need to be protected. Always assume that your repo will be public someday if you are hosting on GitHub (for your DSSG project it will be). Sensitive information also includes architecture decisions about your database. After sensitive information is pushed to GitHub, you cannot remove it completely from the repository. Have code that needs to be operationalized in Jupyter Notebooks. Jupyter notebooks are wonderful for containing your analysis, code and figures in a single document, particularly for doing exploratory analysis. They are not good for keeping the code you will need for your pipeline or code that you will eventually want to turn into a library. Virtual Environments # A virtual environment solves the problem that projectX uses version 1.x of a package while projectY uses version 2.x of a package by keeping dependencies in different environments. Install a virtualenv # pip install --user virtualenv virtualenv dssg-venv --no-site-packages #does not use any global packages You can also install a virtual environment and specify the type of python interpreter you would like to use using the -p option. This is good for keeping Python2 and Python3 dependencies separate. Python2 virtualenv dssg-py2-venv -p $(which python) --no-site-packages Python3 virtualenv dssg-py3-venv -p $(which python3) --no-site-packages Activate a virtualenv # source ./dssg-venv/bin/activate Install Dependencies # pip install -r requirements.txt Freeze Dependencies # pip freeze > requirements.txt #outputs a list of dependencies and version numbers Warning : pip freeze will output every package that was installed using pip or setup.py (setuptools). External dependencies that are from github or some other source not found on PyPi will appear but will not be found when trying to reinstall the dependencies. You can include github repositories from github in your requirements.txt file, you just have to do manual housekeeping. Other external dependencies and how to install them should be recorded in your README.md file. Note: There is also the conda environment created by Continuum Analytics. The conda environment handles creating a environment and package dependencies -- what the virtual environment + pip combination does. Conda, unlike pip, includes many non-python dependencies (e.g, MKL) as precompiled binaries that are necessary for scientific python packages. The author is currently of the opinion that if you are a beginner or using a dated OS then using a conda environment is not the worst of ideas. If you are a developer working on a development machine then compile things yourself -- an important and useful skill. Whatever path you choose be consistent about how you set up your environment and document it thoroughly. Systems Level Dependencies # Systems level dependencies are the libraries installed on your OS. For Ubuntu/Debian Linux you can get a list of them and then install them using the following: #grab systems level dependencies dpkg --get-selections > dependencies.txt #reinstall on a new machine dpkg --clear-selections sudo dpkg --set-selections < dependencies.txt Also courtesy of Tristan Crockett: installing a list of dependencies using apt xargs -a <(awk '/^\\s*[^#]/' dependencies.txt) -r -- sudo apt-get install This will give every package installed on your OS. An easier alternative is to just keep track when you install a new library and manually keep the list in a dependencies.txt file. There are also lightweight vitalization containers like Docker containers, Hyper-V images (Windows), or Ansible playbooks that can be used to \"freeze\" the systems level configuration of an OS. Backup Your Database # In PostGreSQL when a table is dropped, it is gone forever. You don't want to drop your results table on the last day of the fellowship, so it is a good idea to backup periodically. To dump your database in PostGreSQL: pg_dump -Fc --schema='raw|clean|models' -N '*public*' --no-acl -v -h <hostname> -U <dbuser> <dbname> > dssg-$(date +%F).dump Note: This can be automated with a crontab script. To restore your database from a dump: < dump_file psql -U dbuser -h dbhost dbname Hard-coded Paths # Example of Adding Shapefile with hard-coded paths # Hard-coded paths are absolute paths that are native to the machine you are using for development. It is unlikely someone else will keep their data in the exact same directory as you when trying to use your project in a separate environment. Users should be able to set location of files as command line parameters. Below are examples. load_shapefile_hardpath_v1.sh # # Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY shp2pgsql -d -s 4267:2261 -d /mnt/data/syracuse/NY_geol_dd soil.geology | psql Although this script documents the command that runs, it has a hard path and the purpose of the arguments are not clear. This script has the shelf-life of a banana. load_shapefile_hardpath_v2.sh # #!/bin/bash # Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY original_projection=4267 new_projection=2261 #projection of Upstate NY schema='soil' table='geology' shapefile='/mnt/data/syracuse/NY_geol_dd/nygeol_poly_dd.shp' #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data shp2pgsql -d -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql With this version someone can better surmise what is being done. Every time you want to load your data you have to change the filename in the script. It also checks if the table already exists in the database so the command can be used to reload data. load_shapefile_hardpath_v3.sh # #!/bin/bash #ETL script for importing shape files. PROGRAM=$(basename $0) usage=\"${PROGRAM} -s schema -t table -p original_projection [-n new_projection] [-v] shapefilename\" function die() { local errmsg=\"$1\" errcode=\"${2:-1}\" echo \"ERROR: ${errmsg}\" exit ${errcode} } #if called with no command line arguments then output usage if [ ${#} -eq 0 ] then echo ${usage} exit 1; fi #-------------------------------------------------- # process input arguments #-------------------------------------------------- verbose=\"false\" new_projection=\"\" while getopts hp:n:s:t:v OPT; do case \"${OPT}\" in h) echo \"${usage}\"; exit 0 ;; p) original_projection=\"${OPTARG}\" ;; n) new_projection=\"${OPTARG}\" ;; s) schema=\"${OPTARG}\" ;; t) table=\"${OPTARG}\" ;; v) verbose=\"true\" ;; ?) die \"unknown option or missing argument; see -h for usage\" 2 ;; esac done shift $((OPTIND - 1)) shapefile=\"$*\" if [ ${verbose} == \"true\" ] then echo 'original_projection:' $original_projection echo 'new_projection:' $new_projection echo 'schema:' $schema echo 'table:'$table echo 'shapefile:'$shapefile fi #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data if [ -z \"${new_projection}\" ] then shp2pgsql -s ${original_projection} -d ${shapefile} ${schema}.${table} | psql else shp2pgsql -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql fi In this version, you can call the script from the command line and use it for any shapefile. When called with no arguments it prints out a usage so the user does not have to look into the actual script. It also has a verbose mode for debugging. Here, there are no hard paths. Bad Directory Organization # nfp2/ \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 ada_all.yaml \u251c\u2500\u2500 ada_simple_SAMMER.yaml \u251c\u2500\u2500 Add_null_data.ipynb \u251c\u2500\u2500 Add_null_data.py \u251c\u2500\u2500 all.yaml \u251c\u2500\u2500 assemble_long_data.ipynb \u251c\u2500\u2500 binary_classifer.py \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 BRL_file_generation.ipynb \u251c\u2500\u2500 #BRL.py# \u251c\u2500\u2500 BRL.py \u251c\u2500\u2500 classification.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.py \u251c\u2500\u2500 classifier_t1.ipynb \u251c\u2500\u2500 classifier_t3.py \u251c\u2500\u2500 clique_feature_coprus.p \u251c\u2500\u2500 Clique_Features.ipynb \u251c\u2500\u2500 #Clique_Features.py# \u251c\u2500\u2500 Clique_Features.py \u251c\u2500\u2500 Clustering_Scoring.ipynb \u251c\u2500\u2500 cohort_creation.py \u251c\u2500\u2500 convert_nfp_sas_to_csv.R \u251c\u2500\u2500 corpus.ipynb \u251c\u2500\u2500 create_dropout_files.py \u251c\u2500\u2500 cross_val_copy.py \u251c\u2500\u2500 cross_val.ipynb \u251c\u2500\u2500 cross_val.py \u251c\u2500\u2500 dal_test.ipynb \u251c\u2500\u2500 data_cleaning.ipynb \u251c\u2500\u2500 data_cleaning.py \u251c\u2500\u2500 data_creation_1.yaml \u251c\u2500\u2500 data_creation_2.yaml \u251c\u2500\u2500 data_creation_3.yaml \u251c\u2500\u2500 data_creation_4.yaml \u251c\u2500\u2500 data_creation_and_model_applicaition.py \u251c\u2500\u2500 data_creation_and_model_application_1.yaml \u251c\u2500\u2500 data_creation_and_model_application_2.yaml \u251c\u2500\u2500 data_creation_and_model_application_3.yaml \u251c\u2500\u2500 data_creation_and_model_application_4.yaml \u251c\u2500\u2500 data_creation_and_model_application.yaml \u251c\u2500\u2500 data_creation_for_dropout.py \u251c\u2500\u2500 data_creation.yaml \u251c\u2500\u2500 dataframe.py \u251c\u2500\u2500 datasets.flowingdata.com \u251c\u2500\u2500 data_visualization \u251c\u2500\u2500 data_wrangling \u251c\u2500\u2500 decision_tree.yaml \u251c\u2500\u2500 dropout \u251c\u2500\u2500 dropout_explore.ipynb \u251c\u2500\u2500 experiment.log \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_10_month_to_12_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_12_month_to_14_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_14_month_to_18_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_to_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_4_month_to_6_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_6_month_to_10_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_birth_to_4_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_intake_to_birth.dot \u251c\u2500\u2500 Feature Importance.ipynb \u251c\u2500\u2500 find_model.ipynb \u251c\u2500\u2500 find_model.py \u251c\u2500\u2500 #.gitignore# \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph_code.py \u251c\u2500\u2500 Histrogram_Feature_Creation.ipynb \u251c\u2500\u2500 how_to_merge.txt \u251c\u2500\u2500 Imputation.ipynb \u251c\u2500\u2500 Imputation.py \u251c\u2500\u2500 impute \u251c\u2500\u2500 impute_and_filter-Copy0.ipynb \u251c\u2500\u2500 impute_and_filter-Copy1.ipynb \u251c\u2500\u2500 impute_and_filter.ipynb \u251c\u2500\u2500 impute_and_filter.py \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 I_S_O_M_A_P_ _m_o_m_w_t_g_a_i_n___a_n_d___b_i_r_t_h_g_m_s_2___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___s_u_m___a_n_d___P_R_E_P_G_B_M_I___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___t_i_m_e___M_A_T_E_R_N_A_L___s_u_m.png \u251c\u2500\u2500 Jeff_Models-Copy0.ipynb \u251c\u2500\u2500 Jeff_recipe.txt \u251c\u2500\u2500 #KMS.txt# \u251c\u2500\u2500 KMS.txt \u251c\u2500\u2500 legend.html \u251c\u2500\u2500 load_data.py \u251c\u2500\u2500 media \u251c\u2500\u2500 merge.py \u251c\u2500\u2500 meta_data \u251c\u2500\u2500 metr \u251c\u2500\u2500 metrics_r_f_d.p \u251c\u2500\u2500 metrics_will_drop.p \u251c\u2500\u2500 model_pipeline_2-Copy0.ipynb \u251c\u2500\u2500 model_pipeline_2.ipynb \u251c\u2500\u2500 model_pipeline_2.py \u251c\u2500\u2500 model_pipeline_3.py \u251c\u2500\u2500 #model_pipeline_5.py# \u251c\u2500\u2500 model_pipeline_5.py \u251c\u2500\u2500 model_run_rf.txt \u251c\u2500\u2500 models \u251c\u2500\u2500 model_without_pca.yaml \u251c\u2500\u2500 model_with_pca.yaml \u251c\u2500\u2500 model.yaml \u251c\u2500\u2500 name_change.pl \u251c\u2500\u2500 nbstripout \u251c\u2500\u2500 N_Features.ipynb \u251c\u2500\u2500 nfp2-public \u251c\u2500\u2500 nfpt2.tree \u251c\u2500\u2500 notes \u251c\u2500\u2500 out \u251c\u2500\u2500 out.txt \u251c\u2500\u2500 paralllel_coordinates.ipynb \u251c\u2500\u2500 pickle_files \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 pipeline_demo1.py \u251c\u2500\u2500 PipeLine_Phase1.ipynb \u251c\u2500\u2500 PipeLine_Phase1.py \u251c\u2500\u2500 pipeline_utilities.py \u251c\u2500\u2500 plot_binary.py \u251c\u2500\u2500 plot.yaml \u251c\u2500\u2500 Precision-Recall_curve_across_all_intervals.png \u251c\u2500\u2500 prediction_set_maker-Copy0.ipynb \u251c\u2500\u2500 prediction_set_maker-Copy1.ipynb \u251c\u2500\u2500 prediction_set_maker_for_dropout.ipynb \u251c\u2500\u2500 prediction_set_maker.ipynb \u251c\u2500\u2500 prediction_set_maker.py \u251c\u2500\u2500 Prep for R.ipynb \u251c\u2500\u2500 project-pipeline.dia \u251c\u2500\u2500 pyensemble \u251c\u2500\u2500 #pyliny_report.txt# \u251c\u2500\u2500 pyliny_report.txt \u251c\u2500\u2500 python_to_nb.py \u251c\u2500\u2500 Rafael_weeks.ipynb \u251c\u2500\u2500 RandomForestClassifier_on_interval_1.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_2.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_3.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_4.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_5.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_6.png \u251c\u2500\u2500 R_code \u251c\u2500\u2500 README.md \u251c\u2500\u2500 Receiver_operating_characteristic_curve_across_all_intervals.png \u251c\u2500\u2500 results_pipeline2-Copy0.ipynb \u251c\u2500\u2500 results_pipeline2.html \u251c\u2500\u2500 results_pipeline2.ipynb \u251c\u2500\u2500 results_pipeline2.py \u251c\u2500\u2500 roc_auc_score_across_all_intervals.png \u251c\u2500\u2500 rollin_visit.ipynb \u251c\u2500\u2500 run.sh \u251c\u2500\u2500 run_sklearn_model.py \u251c\u2500\u2500 run_some_pipelines.sh \u251c\u2500\u2500 run_weka.pl \u251c\u2500\u2500 sanity_check_pipeline.ipynb \u251c\u2500\u2500 sanity_check_Rafael_pipeline.ipynb \u251c\u2500\u2500 sarah_a.ipynb \u251c\u2500\u2500 Secondary_feature_gen.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael-Copy0.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.py \u251c\u2500\u2500 secondary_features.ipynb \u251c\u2500\u2500 secondary_features_on_visit_data.ipynb \u251c\u2500\u2500 see_test_model_results.ipynb \u251c\u2500\u2500 sklearn_DT.yaml \u251c\u2500\u2500 sklearn.yaml \u251c\u2500\u2500 sklearn.yaml_bk \u251c\u2500\u2500 Slicer-Copy0.ipynb \u251c\u2500\u2500 Slicer.ipynb \u251c\u2500\u2500 summary_statistics.ipynb \u251c\u2500\u2500 temporal_data_creation_bk.ipynb \u251c\u2500\u2500 temporal_data_creation.ipynb \u251c\u2500\u2500 temporal_data_creation.py \u251c\u2500\u2500 test.d \u251c\u2500\u2500 testing_fiber_2_split.p \u251c\u2500\u2500 test_model.ipynb \u251c\u2500\u2500 test.py \u251c\u2500\u2500 time_based_cross_validation-Rafael.ipynb \u251c\u2500\u2500 time_cv_impute.ipynb \u251c\u2500\u2500 timeline_creation_driver.py \u251c\u2500\u2500 Timeline_Help.ipynb \u251c\u2500\u2500 tree.dot \u251c\u2500\u2500 tr_te_to_head.py \u251c\u2500\u2500 Untitled0.ipynb \u251c\u2500\u2500 Untitled1.ipynb \u251c\u2500\u2500 Untitled2.ipynb \u251c\u2500\u2500 Untitled3.ipynb \u251c\u2500\u2500 Untitled4.ipynb \u251c\u2500\u2500 Untitled5.ipynb \u251c\u2500\u2500 Untitled6.ipynb \u251c\u2500\u2500 Untitled7.ipynb \u251c\u2500\u2500 utils \u251c\u2500\u2500 Weka.ipynb \u251c\u2500\u2500 weka_to_pr_jeff.py \u251c\u2500\u2500 weka_to_pr_raf.py \u251c\u2500\u2500 weka_to_roc.py \u2514\u2500\u2500 weka_to_roc_time.py Good Directory Organization # . \u251c\u2500\u2500 config \u251c\u2500\u2500 descriptive_stats \u2502 \u251c\u2500\u2500 mains_streets_stats \u2502 \u2514\u2500\u2500 water_work_orders \u251c\u2500\u2500 etl \u2502 \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 geology \u2502 \u251c\u2500\u2500 road_ratings \u2502 \u251c\u2500\u2500 soil \u2502 \u251c\u2500\u2500 street_line_data \u2502 \u251c\u2500\u2500 tax_data \u2502 \u251c\u2500\u2500 updated_main_data \u2502 \u251c\u2500\u2500 waterorders \u2502 \u2514\u2500\u2500 water_system \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 log \u251c\u2500\u2500 models_evaluation \u2514\u2500\u2500 results \u2514\u2500\u2500 figures Additional Resources/Inspiration for this Tutorial # 10 Rules for Robust Software Good Enough Practices in Scientific Computing Best Practices for Scientific Computing Reproducible Research SSI","title":"Reproducible software"},{"location":"curriculum/programming_best_practices/reproducible-software/#making-projects-reproducible","text":"Scientific software is often developed and used by a single person. It is all too common in academia to be handed a postdoc or graduate student's old code and be unable to replicate the original study, run the software outside of the original development machine, or even get the software to work at all. The goal of this tutorial is to provide some guidelines to make your summer projects reproducible -- this means your project can be installed on another computer and give the same results you got over the summer. At the end of the summer, your project should be understandable and transferable to your future-self and anyone else who may want to pick up where you left off without having to constantly email you about how to get your project running. (Note: Your future-self doesn't have the luxury of being able to email your past-self).","title":"Making Projects Reproducible"},{"location":"curriculum/programming_best_practices/reproducible-software/#what-is-a-reproducible-project","text":"One that... works for someone other than the original team can be easily installed on another computer has documentation that describes any dependencies and how to install them comes with enough tests to indicate the software is running properly","title":"What is a reproducible project?"},{"location":"curriculum/programming_best_practices/reproducible-software/#readmemdrst","text":"All projects should have a README that communicates the following: What the project is about A short description of the project (i.e. the problem you are trying to solve). The required dependencies to run the software The can be in the form of a requirements.txt file for Python that lists the dependencies and version numbers. The system-level dependencies. Installation instructions How to install your software and associated binaries. This can be in the form of instructions on how to use pip , apt , yum , or some other binary package manager. Example usage The inputs and outputs of your software (i.e. how to use it) with code examples. Attribution/Licensing Who did what and how others can use your software. Examples: - Chicago Food Inspections - DSSG Police EIS - Linux Kernel","title":"README.md(rst)"},{"location":"curriculum/programming_best_practices/reproducible-software/#what-to-do","text":"Use virtual environments . Use automation tools like Make or Drake Keep your directory structure intuitive, interpretable and easy to understand . Keep your database free of \"junk tables.\" Keep only what you need and what's current. Junk tables will only confuse your future-self or others that come fresh to the project. Merge all branches into master. Branches are for adding features or patches. When you have added said feature or patch and you know you won't break the master branch, merge into master and delete the branch. Write commit messages in such a way that your log is helpful (see Git and Github tutorial .) Periodically make database backups . Write unit tests and use continuous integration so you can catch bugs quickly, particularly when you are merging new features into master. (See testing tutorial .) Document all of your functions with docstrings. (See legible, good code tutorial .) Write your python code following the PEP8 standard. (See legible, good code tutorial .) Use (4) spaces instead of tabs in your Python code for indentation.","title":"What to Do"},{"location":"curriculum/programming_best_practices/reproducible-software/#what-not-to-do","text":"Use hard-coded paths . Require Sudo/root privileges to install your project. You can't anticipate whether or not someone will have root access to the machine they are installing your project on, so don't count on it. Additionally, you shouldn't require users to create separate user names for your project. Use non-standard formats for inputs (stick to YAML , XML , JSON , CLA , etc). My one exception to this rule is log files - which you should provide an example of in a README. Otherwise it is easier to just stick with what is already in use. Have a messy repo with random files everywhere . This is confusing, irritating and cancerous to productive enterprise. Commit data or sensitive information like database passcodes to the GitHub repo. Your repository is for your codebase, not the data. Furthermore, your data may be sensitive and need to be protected. Always assume that your repo will be public someday if you are hosting on GitHub (for your DSSG project it will be). Sensitive information also includes architecture decisions about your database. After sensitive information is pushed to GitHub, you cannot remove it completely from the repository. Have code that needs to be operationalized in Jupyter Notebooks. Jupyter notebooks are wonderful for containing your analysis, code and figures in a single document, particularly for doing exploratory analysis. They are not good for keeping the code you will need for your pipeline or code that you will eventually want to turn into a library.","title":"What NOT to Do"},{"location":"curriculum/programming_best_practices/reproducible-software/#virtual-environments","text":"A virtual environment solves the problem that projectX uses version 1.x of a package while projectY uses version 2.x of a package by keeping dependencies in different environments.","title":"Virtual Environments"},{"location":"curriculum/programming_best_practices/reproducible-software/#install-a-virtualenv","text":"pip install --user virtualenv virtualenv dssg-venv --no-site-packages #does not use any global packages You can also install a virtual environment and specify the type of python interpreter you would like to use using the -p option. This is good for keeping Python2 and Python3 dependencies separate. Python2 virtualenv dssg-py2-venv -p $(which python) --no-site-packages Python3 virtualenv dssg-py3-venv -p $(which python3) --no-site-packages","title":"Install a virtualenv"},{"location":"curriculum/programming_best_practices/reproducible-software/#activate-a-virtualenv","text":"source ./dssg-venv/bin/activate","title":"Activate a virtualenv"},{"location":"curriculum/programming_best_practices/reproducible-software/#install-dependencies","text":"pip install -r requirements.txt","title":"Install Dependencies"},{"location":"curriculum/programming_best_practices/reproducible-software/#freeze-dependencies","text":"pip freeze > requirements.txt #outputs a list of dependencies and version numbers Warning : pip freeze will output every package that was installed using pip or setup.py (setuptools). External dependencies that are from github or some other source not found on PyPi will appear but will not be found when trying to reinstall the dependencies. You can include github repositories from github in your requirements.txt file, you just have to do manual housekeeping. Other external dependencies and how to install them should be recorded in your README.md file. Note: There is also the conda environment created by Continuum Analytics. The conda environment handles creating a environment and package dependencies -- what the virtual environment + pip combination does. Conda, unlike pip, includes many non-python dependencies (e.g, MKL) as precompiled binaries that are necessary for scientific python packages. The author is currently of the opinion that if you are a beginner or using a dated OS then using a conda environment is not the worst of ideas. If you are a developer working on a development machine then compile things yourself -- an important and useful skill. Whatever path you choose be consistent about how you set up your environment and document it thoroughly.","title":"Freeze Dependencies"},{"location":"curriculum/programming_best_practices/reproducible-software/#systems-level-dependencies","text":"Systems level dependencies are the libraries installed on your OS. For Ubuntu/Debian Linux you can get a list of them and then install them using the following: #grab systems level dependencies dpkg --get-selections > dependencies.txt #reinstall on a new machine dpkg --clear-selections sudo dpkg --set-selections < dependencies.txt Also courtesy of Tristan Crockett: installing a list of dependencies using apt xargs -a <(awk '/^\\s*[^#]/' dependencies.txt) -r -- sudo apt-get install This will give every package installed on your OS. An easier alternative is to just keep track when you install a new library and manually keep the list in a dependencies.txt file. There are also lightweight vitalization containers like Docker containers, Hyper-V images (Windows), or Ansible playbooks that can be used to \"freeze\" the systems level configuration of an OS.","title":"Systems Level Dependencies"},{"location":"curriculum/programming_best_practices/reproducible-software/#backup-your-database","text":"In PostGreSQL when a table is dropped, it is gone forever. You don't want to drop your results table on the last day of the fellowship, so it is a good idea to backup periodically. To dump your database in PostGreSQL: pg_dump -Fc --schema='raw|clean|models' -N '*public*' --no-acl -v -h <hostname> -U <dbuser> <dbname> > dssg-$(date +%F).dump Note: This can be automated with a crontab script. To restore your database from a dump: < dump_file psql -U dbuser -h dbhost dbname","title":"Backup Your Database"},{"location":"curriculum/programming_best_practices/reproducible-software/#hard-coded-paths","text":"","title":"Hard-coded Paths"},{"location":"curriculum/programming_best_practices/reproducible-software/#example-of-adding-shapefile-with-hard-coded-paths","text":"Hard-coded paths are absolute paths that are native to the machine you are using for development. It is unlikely someone else will keep their data in the exact same directory as you when trying to use your project in a separate environment. Users should be able to set location of files as command line parameters. Below are examples.","title":"Example of Adding Shapefile with hard-coded paths"},{"location":"curriculum/programming_best_practices/reproducible-software/#load_shapefile_hardpath_v1sh","text":"# Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY shp2pgsql -d -s 4267:2261 -d /mnt/data/syracuse/NY_geol_dd soil.geology | psql Although this script documents the command that runs, it has a hard path and the purpose of the arguments are not clear. This script has the shelf-life of a banana.","title":"load_shapefile_hardpath_v1.sh"},{"location":"curriculum/programming_best_practices/reproducible-software/#load_shapefile_hardpath_v2sh","text":"#!/bin/bash # Data downloaded from this website: http://mrdata.usgs.gov/geology/state/state.php?state=NY original_projection=4267 new_projection=2261 #projection of Upstate NY schema='soil' table='geology' shapefile='/mnt/data/syracuse/NY_geol_dd/nygeol_poly_dd.shp' #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data shp2pgsql -d -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql With this version someone can better surmise what is being done. Every time you want to load your data you have to change the filename in the script. It also checks if the table already exists in the database so the command can be used to reload data.","title":"load_shapefile_hardpath_v2.sh"},{"location":"curriculum/programming_best_practices/reproducible-software/#load_shapefile_hardpath_v3sh","text":"#!/bin/bash #ETL script for importing shape files. PROGRAM=$(basename $0) usage=\"${PROGRAM} -s schema -t table -p original_projection [-n new_projection] [-v] shapefilename\" function die() { local errmsg=\"$1\" errcode=\"${2:-1}\" echo \"ERROR: ${errmsg}\" exit ${errcode} } #if called with no command line arguments then output usage if [ ${#} -eq 0 ] then echo ${usage} exit 1; fi #-------------------------------------------------- # process input arguments #-------------------------------------------------- verbose=\"false\" new_projection=\"\" while getopts hp:n:s:t:v OPT; do case \"${OPT}\" in h) echo \"${usage}\"; exit 0 ;; p) original_projection=\"${OPTARG}\" ;; n) new_projection=\"${OPTARG}\" ;; s) schema=\"${OPTARG}\" ;; t) table=\"${OPTARG}\" ;; v) verbose=\"true\" ;; ?) die \"unknown option or missing argument; see -h for usage\" 2 ;; esac done shift $((OPTIND - 1)) shapefile=\"$*\" if [ ${verbose} == \"true\" ] then echo 'original_projection:' $original_projection echo 'new_projection:' $new_projection echo 'schema:' $schema echo 'table:'$table echo 'shapefile:'$shapefile fi #create table and schema psql -c \"drop table if exists ${schema}.${table}\" psql -c \"create schema if not exists ${schema}\" #import the data if [ -z \"${new_projection}\" ] then shp2pgsql -s ${original_projection} -d ${shapefile} ${schema}.${table} | psql else shp2pgsql -s ${original_projection}:${new_projection} -d ${shapefile} ${schema}.${table} | psql fi In this version, you can call the script from the command line and use it for any shapefile. When called with no arguments it prints out a usage so the user does not have to look into the actual script. It also has a verbose mode for debugging. Here, there are no hard paths.","title":"load_shapefile_hardpath_v3.sh"},{"location":"curriculum/programming_best_practices/reproducible-software/#bad-directory-organization","text":"nfp2/ \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_ISOMAP_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_final_asq_psolve_2r.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_time4_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 10_month_to_12_month_LLE_final_asq_psocial_2r_and_time4_DURATION_time_MATERNAL_sum_and_time4_DURATION_sum_and_final_asq_comm_2r_and_whptile1.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_ISOMAP_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_PREPGBMI_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_cumulative_DURATION_sum_and_whptile2.png \u251c\u2500\u2500 12_month_to_14_month_LLE_momwtgain_and_birthgms2_and_time4_DURATION_sum_and_time5_DURATION_sum_and_time4_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_ISOMAP_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_final_asq_psolve_1r.png \u251c\u2500\u2500 4_month_to_6_month_LLE_final_asq_fmotor_1r_and_final_asq_psocial_1r_and_final_asq_gmotor_1r_and_final_asq_comm_1r_and_time2_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_ISOMAP_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_birthgms2_and_momwtgain_and_time3_cumulative_DURATION_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_MomsAgeBirth_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_cumulative_DURATION_sum_and_momwtgain.png \u251c\u2500\u2500 6_month_to_10_month_LLE_whptile1_and_time3_DURATION_time_MATERNAL_sum_and_time2_DURATION_sum_and_time3_DURATION_sum_and_time3_cumulative_DURATION_time_MATERNAL_sum.png \u251c\u2500\u2500 ada_all.yaml \u251c\u2500\u2500 ada_simple_SAMMER.yaml \u251c\u2500\u2500 Add_null_data.ipynb \u251c\u2500\u2500 Add_null_data.py \u251c\u2500\u2500 all.yaml \u251c\u2500\u2500 assemble_long_data.ipynb \u251c\u2500\u2500 binary_classifer.py \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_ISOMAP_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_momwtgain_and_birthgms2.png \u251c\u2500\u2500 birth_to_4_month_LLE_MomsAgeBirth_and_time1_DURATION_time_PERSHLTH_std_and_time1_DURATION_sum_and_time1_DURATION_time_MATERNAL_sum_and_time1_DURATION_time_PERSHLTH_sum.png \u251c\u2500\u2500 BRL_file_generation.ipynb \u251c\u2500\u2500 #BRL.py# \u251c\u2500\u2500 BRL.py \u251c\u2500\u2500 classification.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.ipynb \u251c\u2500\u2500 classifier_t1-Copy0.py \u251c\u2500\u2500 classifier_t1.ipynb \u251c\u2500\u2500 classifier_t3.py \u251c\u2500\u2500 clique_feature_coprus.p \u251c\u2500\u2500 Clique_Features.ipynb \u251c\u2500\u2500 #Clique_Features.py# \u251c\u2500\u2500 Clique_Features.py \u251c\u2500\u2500 Clustering_Scoring.ipynb \u251c\u2500\u2500 cohort_creation.py \u251c\u2500\u2500 convert_nfp_sas_to_csv.R \u251c\u2500\u2500 corpus.ipynb \u251c\u2500\u2500 create_dropout_files.py \u251c\u2500\u2500 cross_val_copy.py \u251c\u2500\u2500 cross_val.ipynb \u251c\u2500\u2500 cross_val.py \u251c\u2500\u2500 dal_test.ipynb \u251c\u2500\u2500 data_cleaning.ipynb \u251c\u2500\u2500 data_cleaning.py \u251c\u2500\u2500 data_creation_1.yaml \u251c\u2500\u2500 data_creation_2.yaml \u251c\u2500\u2500 data_creation_3.yaml \u251c\u2500\u2500 data_creation_4.yaml \u251c\u2500\u2500 data_creation_and_model_applicaition.py \u251c\u2500\u2500 data_creation_and_model_application_1.yaml \u251c\u2500\u2500 data_creation_and_model_application_2.yaml \u251c\u2500\u2500 data_creation_and_model_application_3.yaml \u251c\u2500\u2500 data_creation_and_model_application_4.yaml \u251c\u2500\u2500 data_creation_and_model_application.yaml \u251c\u2500\u2500 data_creation_for_dropout.py \u251c\u2500\u2500 data_creation.yaml \u251c\u2500\u2500 dataframe.py \u251c\u2500\u2500 datasets.flowingdata.com \u251c\u2500\u2500 data_visualization \u251c\u2500\u2500 data_wrangling \u251c\u2500\u2500 decision_tree.yaml \u251c\u2500\u2500 dropout \u251c\u2500\u2500 dropout_explore.ipynb \u251c\u2500\u2500 experiment.log \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_10_month_to_12_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_12_month_to_14_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_14_month_to_18_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_18_month_to_20_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_4_month_to_6_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_6_month_to_10_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_birth_to_4_month.dot \u251c\u2500\u2500 Feature_graph_for_interval_with_top_2_features_for_interval_intake_to_birth.dot \u251c\u2500\u2500 Feature Importance.ipynb \u251c\u2500\u2500 find_model.ipynb \u251c\u2500\u2500 find_model.py \u251c\u2500\u2500 #.gitignore# \u251c\u2500\u2500 graph \u251c\u2500\u2500 graph_code.py \u251c\u2500\u2500 Histrogram_Feature_Creation.ipynb \u251c\u2500\u2500 how_to_merge.txt \u251c\u2500\u2500 Imputation.ipynb \u251c\u2500\u2500 Imputation.py \u251c\u2500\u2500 impute \u251c\u2500\u2500 impute_and_filter-Copy0.ipynb \u251c\u2500\u2500 impute_and_filter-Copy1.ipynb \u251c\u2500\u2500 impute_and_filter.ipynb \u251c\u2500\u2500 impute_and_filter.py \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_ISOMAP_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_CLIENT_HEALTH_GENERAL_WEIGHT_0_P.png \u251c\u2500\u2500 intake_to_birth_LLE_CLIENT_HEALTH_PREGNANCY_0_WKS_PR_and_CLIENT_HEALTH_GENERAL_HEIGHT_1_I_and_PREPGBMI_and_PREPGKG_and_NURSE_0_YEAR_NURSING_EXPERIENCE.png \u251c\u2500\u2500 I_S_O_M_A_P_ _m_o_m_w_t_g_a_i_n___a_n_d___b_i_r_t_h_g_m_s_2___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___s_u_m___a_n_d___P_R_E_P_G_B_M_I___a_n_d___t_i_m_e_4___D_U_R_A_T_I_O_N___t_i_m_e___M_A_T_E_R_N_A_L___s_u_m.png \u251c\u2500\u2500 Jeff_Models-Copy0.ipynb \u251c\u2500\u2500 Jeff_recipe.txt \u251c\u2500\u2500 #KMS.txt# \u251c\u2500\u2500 KMS.txt \u251c\u2500\u2500 legend.html \u251c\u2500\u2500 load_data.py \u251c\u2500\u2500 media \u251c\u2500\u2500 merge.py \u251c\u2500\u2500 meta_data \u251c\u2500\u2500 metr \u251c\u2500\u2500 metrics_r_f_d.p \u251c\u2500\u2500 metrics_will_drop.p \u251c\u2500\u2500 model_pipeline_2-Copy0.ipynb \u251c\u2500\u2500 model_pipeline_2.ipynb \u251c\u2500\u2500 model_pipeline_2.py \u251c\u2500\u2500 model_pipeline_3.py \u251c\u2500\u2500 #model_pipeline_5.py# \u251c\u2500\u2500 model_pipeline_5.py \u251c\u2500\u2500 model_run_rf.txt \u251c\u2500\u2500 models \u251c\u2500\u2500 model_without_pca.yaml \u251c\u2500\u2500 model_with_pca.yaml \u251c\u2500\u2500 model.yaml \u251c\u2500\u2500 name_change.pl \u251c\u2500\u2500 nbstripout \u251c\u2500\u2500 N_Features.ipynb \u251c\u2500\u2500 nfp2-public \u251c\u2500\u2500 nfpt2.tree \u251c\u2500\u2500 notes \u251c\u2500\u2500 out \u251c\u2500\u2500 out.txt \u251c\u2500\u2500 paralllel_coordinates.ipynb \u251c\u2500\u2500 pickle_files \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 pipeline_demo1.py \u251c\u2500\u2500 PipeLine_Phase1.ipynb \u251c\u2500\u2500 PipeLine_Phase1.py \u251c\u2500\u2500 pipeline_utilities.py \u251c\u2500\u2500 plot_binary.py \u251c\u2500\u2500 plot.yaml \u251c\u2500\u2500 Precision-Recall_curve_across_all_intervals.png \u251c\u2500\u2500 prediction_set_maker-Copy0.ipynb \u251c\u2500\u2500 prediction_set_maker-Copy1.ipynb \u251c\u2500\u2500 prediction_set_maker_for_dropout.ipynb \u251c\u2500\u2500 prediction_set_maker.ipynb \u251c\u2500\u2500 prediction_set_maker.py \u251c\u2500\u2500 Prep for R.ipynb \u251c\u2500\u2500 project-pipeline.dia \u251c\u2500\u2500 pyensemble \u251c\u2500\u2500 #pyliny_report.txt# \u251c\u2500\u2500 pyliny_report.txt \u251c\u2500\u2500 python_to_nb.py \u251c\u2500\u2500 Rafael_weeks.ipynb \u251c\u2500\u2500 RandomForestClassifier_on_interval_1.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_2.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_3.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_4.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_5.png \u251c\u2500\u2500 RandomForestClassifier_on_interval_6.png \u251c\u2500\u2500 R_code \u251c\u2500\u2500 README.md \u251c\u2500\u2500 Receiver_operating_characteristic_curve_across_all_intervals.png \u251c\u2500\u2500 results_pipeline2-Copy0.ipynb \u251c\u2500\u2500 results_pipeline2.html \u251c\u2500\u2500 results_pipeline2.ipynb \u251c\u2500\u2500 results_pipeline2.py \u251c\u2500\u2500 roc_auc_score_across_all_intervals.png \u251c\u2500\u2500 rollin_visit.ipynb \u251c\u2500\u2500 run.sh \u251c\u2500\u2500 run_sklearn_model.py \u251c\u2500\u2500 run_some_pipelines.sh \u251c\u2500\u2500 run_weka.pl \u251c\u2500\u2500 sanity_check_pipeline.ipynb \u251c\u2500\u2500 sanity_check_Rafael_pipeline.ipynb \u251c\u2500\u2500 sarah_a.ipynb \u251c\u2500\u2500 Secondary_feature_gen.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael-Copy0.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.ipynb \u251c\u2500\u2500 Secondary_feature_gen-Rafael.py \u251c\u2500\u2500 secondary_features.ipynb \u251c\u2500\u2500 secondary_features_on_visit_data.ipynb \u251c\u2500\u2500 see_test_model_results.ipynb \u251c\u2500\u2500 sklearn_DT.yaml \u251c\u2500\u2500 sklearn.yaml \u251c\u2500\u2500 sklearn.yaml_bk \u251c\u2500\u2500 Slicer-Copy0.ipynb \u251c\u2500\u2500 Slicer.ipynb \u251c\u2500\u2500 summary_statistics.ipynb \u251c\u2500\u2500 temporal_data_creation_bk.ipynb \u251c\u2500\u2500 temporal_data_creation.ipynb \u251c\u2500\u2500 temporal_data_creation.py \u251c\u2500\u2500 test.d \u251c\u2500\u2500 testing_fiber_2_split.p \u251c\u2500\u2500 test_model.ipynb \u251c\u2500\u2500 test.py \u251c\u2500\u2500 time_based_cross_validation-Rafael.ipynb \u251c\u2500\u2500 time_cv_impute.ipynb \u251c\u2500\u2500 timeline_creation_driver.py \u251c\u2500\u2500 Timeline_Help.ipynb \u251c\u2500\u2500 tree.dot \u251c\u2500\u2500 tr_te_to_head.py \u251c\u2500\u2500 Untitled0.ipynb \u251c\u2500\u2500 Untitled1.ipynb \u251c\u2500\u2500 Untitled2.ipynb \u251c\u2500\u2500 Untitled3.ipynb \u251c\u2500\u2500 Untitled4.ipynb \u251c\u2500\u2500 Untitled5.ipynb \u251c\u2500\u2500 Untitled6.ipynb \u251c\u2500\u2500 Untitled7.ipynb \u251c\u2500\u2500 utils \u251c\u2500\u2500 Weka.ipynb \u251c\u2500\u2500 weka_to_pr_jeff.py \u251c\u2500\u2500 weka_to_pr_raf.py \u251c\u2500\u2500 weka_to_roc.py \u2514\u2500\u2500 weka_to_roc_time.py","title":"Bad Directory Organization"},{"location":"curriculum/programming_best_practices/reproducible-software/#good-directory-organization","text":". \u251c\u2500\u2500 config \u251c\u2500\u2500 descriptive_stats \u2502 \u251c\u2500\u2500 mains_streets_stats \u2502 \u2514\u2500\u2500 water_work_orders \u251c\u2500\u2500 etl \u2502 \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 geology \u2502 \u251c\u2500\u2500 road_ratings \u2502 \u251c\u2500\u2500 soil \u2502 \u251c\u2500\u2500 street_line_data \u2502 \u251c\u2500\u2500 tax_data \u2502 \u251c\u2500\u2500 updated_main_data \u2502 \u251c\u2500\u2500 waterorders \u2502 \u2514\u2500\u2500 water_system \u251c\u2500\u2500 model \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 log \u251c\u2500\u2500 models_evaluation \u2514\u2500\u2500 results \u2514\u2500\u2500 figures","title":"Good Directory Organization"},{"location":"curriculum/programming_best_practices/reproducible-software/#additional-resourcesinspiration-for-this-tutorial","text":"10 Rules for Robust Software Good Enough Practices in Scientific Computing Best Practices for Scientific Computing Reproducible Research SSI","title":"Additional Resources/Inspiration for this Tutorial"},{"location":"curriculum/programming_best_practices/test-test-test/","text":"Software testing # Slides from 2016 DSSG You've finished your wonderful web prototpe for the Data Fest, your model is awesome and that visualizaiton is going to blow everyone's minds... You get on stage for a live demo and open the webapp only to find that nothing works. Spooky. To prevent that from happening, it's important to write tests. Think about it, when you write a function, when do you know it works \u2122? You probably test it against some ad hoc inputs and, if you get the proper result, you move on. Maybe your function does indeed work today, but what if in the following weeks you change a few lines of the function to make it faster, but you don't want to spend time testing again ( it works \u2122)? Maybe you just introduced a bug and didn't notice. For that reason, it's important to write automated tests to check your code and make sure that everything works consistently , and that you can identify bugs introduced by future changes. I didn't choose the test life, the test life chose me # This tutorial is divided in three parts. The first one addresses unit testing , which is a simple (but powerful) form of testing. The second part is devoted specifically to testing Data Science pipelines. The last section covers Continuous Integration (or CI). CI is a technique to automatically run all of your tests every time you push new code to Github. CI is language-agnostic, but our tutorial specifically addresses Python, because it is the language we use the most. Part 1: Unit Testing with Python (interactive tutorial), How I Learned to Stop Worrying and Love Unit Testing (teachout slides - Kat Rasch ) Part 2: Testing Python Data Science Pipelines Part 3: Continuous Integration External Resources # Software Testing - Udacity course","title":"Writing tests"},{"location":"curriculum/programming_best_practices/test-test-test/#software-testing","text":"Slides from 2016 DSSG You've finished your wonderful web prototpe for the Data Fest, your model is awesome and that visualizaiton is going to blow everyone's minds... You get on stage for a live demo and open the webapp only to find that nothing works. Spooky. To prevent that from happening, it's important to write tests. Think about it, when you write a function, when do you know it works \u2122? You probably test it against some ad hoc inputs and, if you get the proper result, you move on. Maybe your function does indeed work today, but what if in the following weeks you change a few lines of the function to make it faster, but you don't want to spend time testing again ( it works \u2122)? Maybe you just introduced a bug and didn't notice. For that reason, it's important to write automated tests to check your code and make sure that everything works consistently , and that you can identify bugs introduced by future changes.","title":"Software testing"},{"location":"curriculum/programming_best_practices/test-test-test/#i-didnt-choose-the-test-life-the-test-life-chose-me","text":"This tutorial is divided in three parts. The first one addresses unit testing , which is a simple (but powerful) form of testing. The second part is devoted specifically to testing Data Science pipelines. The last section covers Continuous Integration (or CI). CI is a technique to automatically run all of your tests every time you push new code to Github. CI is language-agnostic, but our tutorial specifically addresses Python, because it is the language we use the most. Part 1: Unit Testing with Python (interactive tutorial), How I Learned to Stop Worrying and Love Unit Testing (teachout slides - Kat Rasch ) Part 2: Testing Python Data Science Pipelines Part 3: Continuous Integration","title":"I didn't choose the test life, the test life chose me"},{"location":"curriculum/programming_best_practices/test-test-test/#external-resources","text":"Software Testing - Udacity course","title":"External Resources"},{"location":"curriculum/programming_best_practices/test-test-test/ci/","text":"Continuous integration # Tests are (ideally) cheap to run, so it makes sense to run them every time your code changes. In Part 1 , we ran a test suite using py.test . Running our test suite locally every time your project changes is a good practice, but it's important to also test it on a completely different machine (maybe some of your tests are passing because of local configuration and you don't even know). Furthermore, when working in teams someone may forget to run the tests locally breaking the project. Continuous integration helps you identify those cases (and with proper configuration even reject commits that break the build) just after a push is made. If you are working on an open source project, you can get Continuous Integration for free using Travis CI . In the following sections we'll see how to setup Travis to run your tests every time you push to Github. Using Travis CI # The first step is to create an account and link it to your Github profile. Travis is integrated with Github and requires minimal setup, you just need to create a .travis.yml file (note the leading dot) in your root folder, then go to Travis and activate the repo. Let's see how a .travis.yml file looks like. Travis configuration file for testing scientific Python projects # The content of your configuration file completely depends on your project, but given that most DSSG (if not all) teams use Python + Numpy + Scipy, the following file will work for you with minimal changes. The contents of the file are straightforward and tell Travis how to build your project and run the test suite. language : python python : # list the python version you want to check your tests on - \"2.7\" - \"3.5\" sudo : required install : # this are requirements to install many scentific python packages # such as numpy/scipy, you cannot install them with pip # so we need to use the system package manager - \"sudo apt-get install gfortran python-liblas libblas-dev liblapack-dev libatlas-dev\" # as of June 2, 2016, travis has an outdated version of pip, # this may cause trouble when installing some packages such as sci-kit learn # updating pip before using 'pip install' solves those issues - \"pip install --upgrade pip\" # install specific requirements your test suite uses # e.g. pytest, nose, mock # make sure you have the requirements.txt in your repo's root folder - \"pip install -r requirements.txt\" script : # steps needed to run your scripts, for simple projects # this may be just 'py.test' or 'nosetests', depending on # which library you use - py.test Once Travis is configured it will run your tests every time you push (this is the default configuration but you can change it if you want) and if your tests don't pass it will send you and e-mail (awesome!), you can also see the log to check which tests didn't pass in the Travis website. When configuring Travis you may encounter some issues, feel free to open an issue on this repo if that happens so we can help you out. Project samples using Travis + Scientific Python # Police project sklearn-evaluation - See this if you want to make tests that compare images since it requires a different setup","title":"Continuous integration"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#continuous-integration","text":"Tests are (ideally) cheap to run, so it makes sense to run them every time your code changes. In Part 1 , we ran a test suite using py.test . Running our test suite locally every time your project changes is a good practice, but it's important to also test it on a completely different machine (maybe some of your tests are passing because of local configuration and you don't even know). Furthermore, when working in teams someone may forget to run the tests locally breaking the project. Continuous integration helps you identify those cases (and with proper configuration even reject commits that break the build) just after a push is made. If you are working on an open source project, you can get Continuous Integration for free using Travis CI . In the following sections we'll see how to setup Travis to run your tests every time you push to Github.","title":"Continuous integration"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#using-travis-ci","text":"The first step is to create an account and link it to your Github profile. Travis is integrated with Github and requires minimal setup, you just need to create a .travis.yml file (note the leading dot) in your root folder, then go to Travis and activate the repo. Let's see how a .travis.yml file looks like.","title":"Using Travis CI"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#travis-configuration-file-for-testing-scientific-python-projects","text":"The content of your configuration file completely depends on your project, but given that most DSSG (if not all) teams use Python + Numpy + Scipy, the following file will work for you with minimal changes. The contents of the file are straightforward and tell Travis how to build your project and run the test suite. language : python python : # list the python version you want to check your tests on - \"2.7\" - \"3.5\" sudo : required install : # this are requirements to install many scentific python packages # such as numpy/scipy, you cannot install them with pip # so we need to use the system package manager - \"sudo apt-get install gfortran python-liblas libblas-dev liblapack-dev libatlas-dev\" # as of June 2, 2016, travis has an outdated version of pip, # this may cause trouble when installing some packages such as sci-kit learn # updating pip before using 'pip install' solves those issues - \"pip install --upgrade pip\" # install specific requirements your test suite uses # e.g. pytest, nose, mock # make sure you have the requirements.txt in your repo's root folder - \"pip install -r requirements.txt\" script : # steps needed to run your scripts, for simple projects # this may be just 'py.test' or 'nosetests', depending on # which library you use - py.test Once Travis is configured it will run your tests every time you push (this is the default configuration but you can change it if you want) and if your tests don't pass it will send you and e-mail (awesome!), you can also see the log to check which tests didn't pass in the Travis website. When configuring Travis you may encounter some issues, feel free to open an issue on this repo if that happens so we can help you out.","title":"Travis configuration file for testing scientific Python projects"},{"location":"curriculum/programming_best_practices/test-test-test/ci/#project-samples-using-travis-scientific-python","text":"Police project sklearn-evaluation - See this if you want to make tests that compare images since it requires a different setup","title":"Project samples using Travis + Scientific Python"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/","text":"Testing Python Data Science pipelines # Testing Data Pipelines is hard . By definition you do not expect them to produce the same output over and over again. There isn't a standard way of doing it (or at least I don't know it), but here you'll find some tips to test your pipeline steps. Tips to test your pipeline # Separate the source code and your pipeline steps in different modules # As it was mentioned in Part1 . It's important to separate the source code from your pipeline. Think of source code as the building blocks for processing data, uploading to the database, training models and so on. For example, the training step in your pipeline may look like this: # training step in your pipeline # load features train , test = load_features ([ 'feature1' , 'feature2' , 'feature3' ]) # get a list of models to train to_train = load_models ( 'RandomForest' , 'LogisticRegression' ) # train every model for model in to_train : # fit the model model . fit ( train . y , train . y ) # evaluate some metrics on the test set, # save results in a database, # create HTML/PDF reports evaluate_model ( model , test ) In the training step above load_features , load_models and evaluate_model depend on your project, maybe you are loading data from PostgresSQL, maybe from HDF5. The models you train also depend on your project and the evaluation depends on which metrics are best suited for your project's goal. Those functions are building blocks and the source code for those should be outside your training script. Probably the load_features function does a lot of data transformations, try to divide it in various small functions and run unit tests on them. Make the data assumptions in your code explicit # When working on your pipeline the math/logic may work fine, but what if you are feeding the wrong data? Imagine what would happen if you train a classifier and you forgot to delete the outcome variable in your feature set, that sounds like a dumb mistake, but as your pipeline gets more and more complex, it can happen . To prevent those things from happening you should test your pipeline at runtime , meaning that you should check for red flags while training models. Let's see an example. def load_features ( list_of_features ): \"\"\" This function loads features from the database \"\"\" uri = load_uri () con = db_connection ( uri ) tables = load_from_db ( list_of_features , con ) if 'outcome' in tables : raise Exception ( 'You cannot load the outcome variable as a feature' ) if any ( tables , has_nas ): raise Exception ( 'You cannot load features with NAs' ) return tables In the snippet above we are making two assumptions explicit, the first one is that we shouldn't load a column named 'outcome' in our feature set, the second one means that we cannot load columns with NAs, because this may break the following steps in our pipeline. Test your code with a fake database # Imagine that you implemented a function to perform some complex feature engineering in your database, then you modify some parts to make it faster and you have a test to check that the results hold the same. If your database has millions of rows and a couple hundred features, how long is the test going to take? When testing code that interacts with a lot of data is often a good idea to sample it and put it in a test database, that way you can test faster, but don't force yourself to make your tests run fast. Always remember a fast test is better than slow test, but a slow test is better than not testing at all . How do you change which database your code uses? There are a couple of ways, you can for example define an environment variable to change the behavior of your open_db_connection function. Note: use environmental variables judiciously and don't store any sensitive data. # db.py import os def db_connection (): if os . environ [ 'testing_mode' ]: return connect_to_testing_db () else : return connect_to_production_db () Now, you need to add some custom logic to the script that runs your tests, let's see how to do it: # run_tests.sh export testing_mode = YES py.test # run your tests export testing_mode = NO Dealing with randomness # The hardest part of testing pipelines is dealing with randomness, how do you test a random generator function? or a probabilistic function? One of the simplest ways to do it is to take out the randomness by setting the random seed in your tests. Let's see an example. # random.py def generate_random_number_between ( a , b ): # generate random number using seed value, # a and b return number # test_random.py # set the seed value so you always get random numbers in the same order # during the tests set_random_seed ( 0 ) def test_generate_random_number_below_10 (): assert generate_random_number_between ( 0 , 10 ) == 5 def test_generate_random_number_between_8_12 (): assert generate_random_number_between ( 8 , 12 ) == 10 The example above is a bit naive, but it hopefully gives you an idea on how to take out the randomness by setting the seed, let's see a more robust example. Another approach is to test your function enough number and check the result against an interval and not an specific value. Let's see how to test a function that draws one sample from the normal distribution: # normal_sample.py def normal_dist_sample ( mean = 0 , std = 1 ): # do stuff return sample # test_normal_sample.py import numpy.testing as npt def test_normal_dist_sample_mean (): # draw 1000 samples samples = [ normal_dist_sample () for i in range ( 10000 )] # calculate the mean mean = samples . mean () # check that the mean is almost equal to zero assert npt . assert_almost_equal ( mean , 0 ) As you can see, testing probabilistic code is not trivial, so do it only when your project highly depends on such functions. But make sure you write unit tests and to check the assumptions in your data! Tools for testing # hypothesis - Python library to look for edge cases without explicitly coding them engarde - Use Python decorators to test a function outcome (this project had good potential but it's dead now) feature forge - Testing features for ML models (also seems dead) External resources # Testing for Data Scientists by Trey Causey Where to go from here # Part 3: Continuous Integration","title":"Testing Python Data Science pipelines"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#testing-python-data-science-pipelines","text":"Testing Data Pipelines is hard . By definition you do not expect them to produce the same output over and over again. There isn't a standard way of doing it (or at least I don't know it), but here you'll find some tips to test your pipeline steps.","title":"Testing Python Data Science pipelines"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#tips-to-test-your-pipeline","text":"","title":"Tips to test your pipeline"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#separate-the-source-code-and-your-pipeline-steps-in-different-modules","text":"As it was mentioned in Part1 . It's important to separate the source code from your pipeline. Think of source code as the building blocks for processing data, uploading to the database, training models and so on. For example, the training step in your pipeline may look like this: # training step in your pipeline # load features train , test = load_features ([ 'feature1' , 'feature2' , 'feature3' ]) # get a list of models to train to_train = load_models ( 'RandomForest' , 'LogisticRegression' ) # train every model for model in to_train : # fit the model model . fit ( train . y , train . y ) # evaluate some metrics on the test set, # save results in a database, # create HTML/PDF reports evaluate_model ( model , test ) In the training step above load_features , load_models and evaluate_model depend on your project, maybe you are loading data from PostgresSQL, maybe from HDF5. The models you train also depend on your project and the evaluation depends on which metrics are best suited for your project's goal. Those functions are building blocks and the source code for those should be outside your training script. Probably the load_features function does a lot of data transformations, try to divide it in various small functions and run unit tests on them.","title":"Separate the source code and your pipeline steps in different modules"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#make-the-data-assumptions-in-your-code-explicit","text":"When working on your pipeline the math/logic may work fine, but what if you are feeding the wrong data? Imagine what would happen if you train a classifier and you forgot to delete the outcome variable in your feature set, that sounds like a dumb mistake, but as your pipeline gets more and more complex, it can happen . To prevent those things from happening you should test your pipeline at runtime , meaning that you should check for red flags while training models. Let's see an example. def load_features ( list_of_features ): \"\"\" This function loads features from the database \"\"\" uri = load_uri () con = db_connection ( uri ) tables = load_from_db ( list_of_features , con ) if 'outcome' in tables : raise Exception ( 'You cannot load the outcome variable as a feature' ) if any ( tables , has_nas ): raise Exception ( 'You cannot load features with NAs' ) return tables In the snippet above we are making two assumptions explicit, the first one is that we shouldn't load a column named 'outcome' in our feature set, the second one means that we cannot load columns with NAs, because this may break the following steps in our pipeline.","title":"Make the data assumptions in your code explicit"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#test-your-code-with-a-fake-database","text":"Imagine that you implemented a function to perform some complex feature engineering in your database, then you modify some parts to make it faster and you have a test to check that the results hold the same. If your database has millions of rows and a couple hundred features, how long is the test going to take? When testing code that interacts with a lot of data is often a good idea to sample it and put it in a test database, that way you can test faster, but don't force yourself to make your tests run fast. Always remember a fast test is better than slow test, but a slow test is better than not testing at all . How do you change which database your code uses? There are a couple of ways, you can for example define an environment variable to change the behavior of your open_db_connection function. Note: use environmental variables judiciously and don't store any sensitive data. # db.py import os def db_connection (): if os . environ [ 'testing_mode' ]: return connect_to_testing_db () else : return connect_to_production_db () Now, you need to add some custom logic to the script that runs your tests, let's see how to do it: # run_tests.sh export testing_mode = YES py.test # run your tests export testing_mode = NO","title":"Test your code with a fake database"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#dealing-with-randomness","text":"The hardest part of testing pipelines is dealing with randomness, how do you test a random generator function? or a probabilistic function? One of the simplest ways to do it is to take out the randomness by setting the random seed in your tests. Let's see an example. # random.py def generate_random_number_between ( a , b ): # generate random number using seed value, # a and b return number # test_random.py # set the seed value so you always get random numbers in the same order # during the tests set_random_seed ( 0 ) def test_generate_random_number_below_10 (): assert generate_random_number_between ( 0 , 10 ) == 5 def test_generate_random_number_between_8_12 (): assert generate_random_number_between ( 8 , 12 ) == 10 The example above is a bit naive, but it hopefully gives you an idea on how to take out the randomness by setting the seed, let's see a more robust example. Another approach is to test your function enough number and check the result against an interval and not an specific value. Let's see how to test a function that draws one sample from the normal distribution: # normal_sample.py def normal_dist_sample ( mean = 0 , std = 1 ): # do stuff return sample # test_normal_sample.py import numpy.testing as npt def test_normal_dist_sample_mean (): # draw 1000 samples samples = [ normal_dist_sample () for i in range ( 10000 )] # calculate the mean mean = samples . mean () # check that the mean is almost equal to zero assert npt . assert_almost_equal ( mean , 0 ) As you can see, testing probabilistic code is not trivial, so do it only when your project highly depends on such functions. But make sure you write unit tests and to check the assumptions in your data!","title":"Dealing with randomness"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#tools-for-testing","text":"hypothesis - Python library to look for edge cases without explicitly coding them engarde - Use Python decorators to test a function outcome (this project had good potential but it's dead now) feature forge - Testing features for ML models (also seems dead)","title":"Tools for testing"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#external-resources","text":"Testing for Data Scientists by Trey Causey","title":"External resources"},{"location":"curriculum/programming_best_practices/test-test-test/ds_testing/#where-to-go-from-here","text":"Part 3: Continuous Integration","title":"Where to go from here"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/","text":"Testing Your Python Projects # Your Very First Python Test # Testing in Python is fairly straightforward. Here's an example: Say we wrote a function get_answer , which should return 42 , the answer to life, the universe and everything. (In the real world, the test and the code to be tested would live in different files, but we'll keep them together for simplicity here.) # function to test def get_answer (): return 42 # actual test def test_answer_to_life_is_42 (): assert get_answer () == 42 This is pretty simple. The function is just your everyday Python function, and the test is a separate function that calls the function get_answer . The interesting part here is the keyword assert , which evaluates a condition. If the condition evaluates to True , we say that the test passed ; if it returns False , we say the test failed . There are many types of tests. The case above is called a unit test , which comes from the notion that we are testing a unit of code (not the entire project). The idea is to have one unit test for each unit of code (e.g. a function). Software testing is an entire subject on its own, but keep it simple for now and follow these guidelines when writing your tests: Your tests should check that one thing works (a function for example) Your tests should be independent of other tests (the outcome of one should not affect others) Given the same input, your test should always return the result (when testing a Data Science pipeline this gets tricky given its probabilistic nature) Running Tests with py.test # One of the best tools for testing in Python is pytest , which provides some useful features to reduce the amount of boilerplate code for your tests, as well as a command to automatically find Python files with tests and run them. You can install it with pip : pip install pytest If you want to actually run the test above, you need to get a copy of this repo. git clone https://github.com/dssg/hitchhikers-guide cd hitchhikers-guide To run your tests (note the dot in the middle): py.test pytest will look for all the tests in your project. In our case there's a copy of our test in a file called test_meaning.py on this folder. The output should look like this: ==================== test session starts =========================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: stuff/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_life.py . ==================== 1 passed in 0.01 seconds ======================= Now, imagine that we accidentally modify our function to: # function to test def get_answer (): return 41 If we run py.test again, we'll see the following: ======================== test session starts ==================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: /Users/Edu/development/dsapp/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_meaning.py F ============================== FAILURES ======================== __________________ test_answer_to_file_is_42 __________________ def test_answer_to_file_is_42(): > assert get_answer() == 42 E assert 41 == 42 E + where 41 = get_answer() test_meaning.py:8: AssertionError ===================== 1 failed in 0.02 seconds ================== We can see from the output that our test failed. Now, every time you modify your code just run py.test to make sure your code still works! ( But remember: just because you tested something doesn't necessarily mean it works. You can keep adding new tests as you identify new edge cases and errors). Where to Store Your Tests # The are no strict rules on where to store your tests. In a Data Science project, you are going to have a lot of folders with code for many tasks (e.g. ETL, modeling). The first thing to take into account is to separate your pipeline steps from your source code: simply speaking, source code is functions and classes that you want to reuse in various steps of your pipeline (e.g. creating a database connection). Let's see an example to make this clear: . \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 documentation_here.txt \u251c\u2500\u2500 etl \u2502 \u2514\u2500\u2500 code_to_load_to_db.txt \u251c\u2500\u2500 evaluation \u2502 \u2514\u2500\u2500 code_to_evaluate_models_here.txt \u251c\u2500\u2500 exploration \u2502 \u2514\u2500\u2500 jupyter_notebooks_with_cool_plots_here.txt \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 code_to_generate_features_here.txt \u251c\u2500\u2500 lib \u2502 \u251c\u2500\u2500 lib \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 db \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 load.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 process.py \u2502 \u2502 \u2514\u2500\u2500 util.py \u2502 \u2514\u2500\u2500 tests \u2502 \u251c\u2500\u2500 test_db.py \u2502 \u2514\u2500\u2500 test_util.py \u2514\u2500\u2500 model \u2514\u2500\u2500 code_to_train_models_here.txt In the diagram above, you can see the different steps in your pipeline (ETL, exploration, feature generation, modeling, model evaluation) all those folders will contain a mix of Python, shell and SQL scripts. Then, there's another folder called lib , which stores the source code for this project. Inside such a folder, you'll find another two folders lib and tests , the first one stores the actual source code and the later stores the tests for the code inside lib . Of course, that doesn't mean you should limit your tests to your source code! You should also test your pipeline, but a well designed pipeline will put the complicated parts in the source code, so your pipeline steps are short and simple. The problem with testing a pipeline is that some steps won't be deterministic, but there are some things you can do. Tip: To access the code in lib you can either create a setup.py file to install or add the folder to your PYTHONPATH . # Other Python Testing Tools # unittest or unitest2 if you use Python 3 (part of the Python standard library) nose - another good option to run your tests Where to go From Here # Part 2: Testing Python Data Science pipelines Read the pytest documentation","title":"Testing Your Python Projects"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#testing-your-python-projects","text":"","title":"Testing Your Python Projects"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#your-very-first-python-test","text":"Testing in Python is fairly straightforward. Here's an example: Say we wrote a function get_answer , which should return 42 , the answer to life, the universe and everything. (In the real world, the test and the code to be tested would live in different files, but we'll keep them together for simplicity here.) # function to test def get_answer (): return 42 # actual test def test_answer_to_life_is_42 (): assert get_answer () == 42 This is pretty simple. The function is just your everyday Python function, and the test is a separate function that calls the function get_answer . The interesting part here is the keyword assert , which evaluates a condition. If the condition evaluates to True , we say that the test passed ; if it returns False , we say the test failed . There are many types of tests. The case above is called a unit test , which comes from the notion that we are testing a unit of code (not the entire project). The idea is to have one unit test for each unit of code (e.g. a function). Software testing is an entire subject on its own, but keep it simple for now and follow these guidelines when writing your tests: Your tests should check that one thing works (a function for example) Your tests should be independent of other tests (the outcome of one should not affect others) Given the same input, your test should always return the result (when testing a Data Science pipeline this gets tricky given its probabilistic nature)","title":"Your Very First Python Test"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#running-tests-with-pytest","text":"One of the best tools for testing in Python is pytest , which provides some useful features to reduce the amount of boilerplate code for your tests, as well as a command to automatically find Python files with tests and run them. You can install it with pip : pip install pytest If you want to actually run the test above, you need to get a copy of this repo. git clone https://github.com/dssg/hitchhikers-guide cd hitchhikers-guide To run your tests (note the dot in the middle): py.test pytest will look for all the tests in your project. In our case there's a copy of our test in a file called test_meaning.py on this folder. The output should look like this: ==================== test session starts =========================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: stuff/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_life.py . ==================== 1 passed in 0.01 seconds ======================= Now, imagine that we accidentally modify our function to: # function to test def get_answer (): return 41 If we run py.test again, we'll see the following: ======================== test session starts ==================== platform darwin -- Python 3.5.1, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 rootdir: /Users/Edu/development/dsapp/hitchhikers-guide/tech-tutorials/testtesttest, inifile: collected 1 items test_meaning.py F ============================== FAILURES ======================== __________________ test_answer_to_file_is_42 __________________ def test_answer_to_file_is_42(): > assert get_answer() == 42 E assert 41 == 42 E + where 41 = get_answer() test_meaning.py:8: AssertionError ===================== 1 failed in 0.02 seconds ================== We can see from the output that our test failed. Now, every time you modify your code just run py.test to make sure your code still works! ( But remember: just because you tested something doesn't necessarily mean it works. You can keep adding new tests as you identify new edge cases and errors).","title":"Running Tests with py.test"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#where-to-store-your-tests","text":"The are no strict rules on where to store your tests. In a Data Science project, you are going to have a lot of folders with code for many tasks (e.g. ETL, modeling). The first thing to take into account is to separate your pipeline steps from your source code: simply speaking, source code is functions and classes that you want to reuse in various steps of your pipeline (e.g. creating a database connection). Let's see an example to make this clear: . \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 documentation_here.txt \u251c\u2500\u2500 etl \u2502 \u2514\u2500\u2500 code_to_load_to_db.txt \u251c\u2500\u2500 evaluation \u2502 \u2514\u2500\u2500 code_to_evaluate_models_here.txt \u251c\u2500\u2500 exploration \u2502 \u2514\u2500\u2500 jupyter_notebooks_with_cool_plots_here.txt \u251c\u2500\u2500 features \u2502 \u2514\u2500\u2500 code_to_generate_features_here.txt \u251c\u2500\u2500 lib \u2502 \u251c\u2500\u2500 lib \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 db \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 load.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 process.py \u2502 \u2502 \u2514\u2500\u2500 util.py \u2502 \u2514\u2500\u2500 tests \u2502 \u251c\u2500\u2500 test_db.py \u2502 \u2514\u2500\u2500 test_util.py \u2514\u2500\u2500 model \u2514\u2500\u2500 code_to_train_models_here.txt In the diagram above, you can see the different steps in your pipeline (ETL, exploration, feature generation, modeling, model evaluation) all those folders will contain a mix of Python, shell and SQL scripts. Then, there's another folder called lib , which stores the source code for this project. Inside such a folder, you'll find another two folders lib and tests , the first one stores the actual source code and the later stores the tests for the code inside lib . Of course, that doesn't mean you should limit your tests to your source code! You should also test your pipeline, but a well designed pipeline will put the complicated parts in the source code, so your pipeline steps are short and simple. The problem with testing a pipeline is that some steps won't be deterministic, but there are some things you can do. Tip: To access the code in lib you can either create a setup.py file to install or add the folder to your PYTHONPATH .","title":"Where to Store Your Tests"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#other-python-testing-tools","text":"unittest or unitest2 if you use Python 3 (part of the Python standard library) nose - another good option to run your tests","title":"Other Python Testing Tools"},{"location":"curriculum/programming_best_practices/test-test-test/python_testing/#where-to-go-from-here","text":"Part 2: Testing Python Data Science pipelines Read the pytest documentation","title":"Where to go From Here"},{"location":"curriculum/scoping/analysis_methods/","text":"","title":"Intro to data analysis methods"},{"location":"curriculum/scoping/deliverables/","text":"Deliverables # Technical report # Poster # Data day presentation # Github repository # Github repository (Public version) # How to # Resources # Deliverables checklist Technical report template Poster template Example slides for data day","title":"Project deliverables"},{"location":"curriculum/scoping/deliverables/#deliverables","text":"","title":"Deliverables"},{"location":"curriculum/scoping/deliverables/#technical-report","text":"","title":"Technical report"},{"location":"curriculum/scoping/deliverables/#poster","text":"","title":"Poster"},{"location":"curriculum/scoping/deliverables/#data-day-presentation","text":"","title":"Data day presentation"},{"location":"curriculum/scoping/deliverables/#github-repository","text":"","title":"Github repository"},{"location":"curriculum/scoping/deliverables/#github-repository-public-version","text":"","title":"Github repository (Public version)"},{"location":"curriculum/scoping/deliverables/#how-to","text":"","title":"How to"},{"location":"curriculum/scoping/deliverables/#resources","text":"Deliverables checklist Technical report template Poster template Example slides for data day","title":"Resources"},{"location":"curriculum/scoping/dme/","text":"Data Maturity Evaluation # Resources # Data Maturity Framework","title":"Data Maturity evaluation"},{"location":"curriculum/scoping/dme/#data-maturity-evaluation","text":"","title":"Data Maturity Evaluation"},{"location":"curriculum/scoping/dme/#resources","text":"Data Maturity Framework","title":"Resources"},{"location":"curriculum/scoping/overview/","text":"Overview # Resources # Data Science Project Scoping Guide Blank Project Scoping Worksheet Filled-out Project Scoping Worksheet","title":"Scoping overview"},{"location":"curriculum/scoping/overview/#overview","text":"","title":"Overview"},{"location":"curriculum/scoping/overview/#resources","text":"Data Science Project Scoping Guide Blank Project Scoping Worksheet Filled-out Project Scoping Worksheet","title":"Resources"},{"location":"curriculum/scoping/project_workflow/","text":"","title":"Project workflow"},{"location":"curriculum/setup/cli/","text":"","title":"Cli"},{"location":"curriculum/setup/git/","text":"","title":"Git and github basic"},{"location":"curriculum/setup/good_repos/","text":"","title":"Good repos"},{"location":"curriculum/setup/machines/","text":"","title":"Setting up machines"},{"location":"curriculum/setup/you_need/","text":"","title":"Software you need"},{"location":"curriculum/setup/command-line-tools/","text":"Command Line Tools # Motivation # As data scientists, we often receive data in text-based files. We need to explore these files to understand what they contain, we need to manipulate and clean them and we need to handle them on our file system. The most robust way to do this, even with large files, is the command line. Command line tools are the data scientist's swiss army knife. They are versitile, portable, and have plenty of functions that your not quite sure how to use, but you're sure they'll be useful at some point. From helping you obtain, clean, and explore your data, to helping you build models and manager your workflow, command line tools are essential to every well-built data science pipeline, will be used throughout DSSG, and should be your starting point as you build your data science toolkit. Slides # Here's the presentation from the start of the workshop. Let's talk about the weather # Since there's been so much controversy over weather predictions from paid vs free apps this year, we're going to just do it ourselves and create out own predictions using weather data from NOAA. You can find daily data for the US here: ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz (The documentation is here ) Getting Data from the Command Line # First we have to get the data. For that we're going to use curl. $ curl ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Whoa! Terminal is going crazy! This may impress your less savvy friends, but it's not going to help you answer your question. We need to stop this process. Try control-c. This is the universal escape command in terminal. We obviously didn't use curl right. Let's look up the manual for the command using man . $ man curl Looks like if we want to write this to a file, we've got to pass the -O argument. $ curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Let's check to see if it worked. $ ls -lah Great. Now we need to know the file format so we know what tool to use to unpack it. $ file 2016.csv.gz Looks like it's a gzip so we'll have to use gunzip . $ gunzip 2016.csv.gz $ ls -lah Now we've got a .csv file we can start playing with. Let's see how big it is using wc Viewing Data from the Command Line # The simpilest streaming command is cat . This dumps the whole file, line by line, into standard out and prints. $ cat 2016.csv That's a bit much. Let's see if we can slow it down by viewing the file page by page using more or less . $ less 2016.csv Great. But let's say I just want to see the top of the file to get a sense of it's structure. We can use head for that. $ head 2016.csv $ head -n 3 2016.csv Similarly, if I'm only interested in viewing the end of the file, I can use tail . $ tail 2016.csv These commands all print things out raw and bunched together. I want to take advantage of the fact that I know this is a csv to get a prettier view of the data. This is where csvkit starts to shine. The first command we'll use from csvkit is csvlook . $ csvlook 2016.csv But that's everything again. We just want to see the top. If only we could take the output from head and send it to csvlook . We can! It's called piping , and you do it like this: head 2016.csv | csvlook The output from head was sent to csvlook for processing. Piping and redirection (more on that later) are two of the most important concepts to keep in mind when using command line tools. Because most commands use text as the interface, you can chain commands together to create simple and powerful data processing pipelines! Filtering Data from the Command Line # It looks like in order for us to make sense of the weather dataset, we're going to need to figure out what these station numbers mean. Let's grab the station dictionary from NOAA and take a look at it. $ curl -O https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt $ head ghcnd-stations.txt Looks like the sation description might come in handy. We want to look at just the stations in Chicago. $ grep CHICAGO ghcnd-stations.txt | csvlook -H Let's pick OHARE as the station we'll use for now. Its ID is 'USW00094846' Let's take a look at just the ID column from the weather file. We can do this using cut . $ cut -f 1 2016.csv Looks like cut isn't smart enough to know that we're using a csv. We can either use csvcut, or pass a delimiter argument that specifies comma. $ cut -d , -f 1 2016.csv | head Now let's filter out just the oberservations from OHARE. $ cut -d , -f 1 2016.csv | grep USW00094846 | head Another powerful tool that can do filtering (and much more) is awk . awk treats every file as a set of row-based records and allows you to create contition/{action} pairs for the records in the file. The default {action} in awk is to print the records that meet the condition. Let's try reproducing the above statement using awk . $ cut -d , -f 1 2016.csv | awk '/USW00094846/' | head awk requires familiarity with regular expressions for contitions and has its own language for actions, so man and stack overflow will be your friends if you want to go deep with awk . Editing and Transforming Data # Let's say we want to replace values in the files. PRCP is confusing. Let's change PRCP to RAIN. To do this, we use sed . sed stands for streaming editor, and is very useful for editing large text files because it doesn't have to load all the data into memory to make changes. Here's how we can use sed to replace a string. $ sed s/PRCP/RAIN/ 2016.csv | head Notice the strings have changed! But when we look at the source file $ head 2016.csv Noting has changed. That's because we didn't write it to a file. In fact, none of the changes we've made have. $ sed s/PRCP/RAIN/ 2016.csv > 2016_clean.csv $ head 2016_clean.csv We can also use awk for subsitution, but this time, let's replace \"WSFM\" with \"WINDSPEED\" in all the weather files in the directory. Once again, stackoverflow is your friend here. $ ls -la > files.txt $ awk '$9 ~/2016*/ {gsub(/WSFM/, \"WINDSPEED\"); print;}' files.txt Group Challenges # For group challenges, log onto the training ec2 instance and change directories to /mnt/data/training/yourusername. This should be your working directory for all the excercises. Create a final weather file that just has weather data from OHARE airport for days when it rained, and change PRCP to RAIN. Save the sequence of commands to a shell script so it's replicable by your teammate and push to a training repository you've created on github. Create a separate file with just the weather from OHARE for days when the tempurature was above 70 degrees F. (hint: try using csvgrep to filter a specific column on a range of values) Get ready to explore the relationship between weather and crime in Chicago. Using crime data from 2016 (below), parse the json and convert it to a csv. Explore the fields and cut the dataset down to just day, location, and crime type. Then subset the dataset to just homicides and save as a new file. https://data.cityofchicago.org/resource/6zsd-86xi.json Using just command line tools, can you use the lat and long coordinates of the weather stations to rapidly identify which weather station is closest to the DSSG building? Cheat Sheet # We're going to cover a variety of command line tools that help us obtain, parse, scrub, and explore your data. (The first few steps toward being an OSEMN data scientist). Here's a list of commands and concepts we'll cover: Getting to know you: navigating files and directories in the command line `cd mkdir ls file mv cp rm findit (bonus) Getting, unpacking, and parsing Data curl wget (bonus) gunzip tar (bonus) in2csv json2csv (bonus) Exploring Data wc cat less head tail csvlook Filtering, Slicing, and Transforming grep cut sed awk csvgrep csvcut csvjoin (bonus) jq (bonus; sed for JSON) Exploring & Summarizing csvstat Writing shell scripts Further Resources # Jeroen Janssens wrote the book literally on data science in the command line. Also, check out his post on 7 essential command line tools for data scientists. For command line basics, Learning CLI the Hard Way is, as always, a great resource. Potential Teachouts # tmux : Getting your command line organized tmux is a great way to manage many environments at once. Give it a shot!","title":"Command line intro"},{"location":"curriculum/setup/command-line-tools/#command-line-tools","text":"","title":"Command Line Tools"},{"location":"curriculum/setup/command-line-tools/#motivation","text":"As data scientists, we often receive data in text-based files. We need to explore these files to understand what they contain, we need to manipulate and clean them and we need to handle them on our file system. The most robust way to do this, even with large files, is the command line. Command line tools are the data scientist's swiss army knife. They are versitile, portable, and have plenty of functions that your not quite sure how to use, but you're sure they'll be useful at some point. From helping you obtain, clean, and explore your data, to helping you build models and manager your workflow, command line tools are essential to every well-built data science pipeline, will be used throughout DSSG, and should be your starting point as you build your data science toolkit.","title":"Motivation"},{"location":"curriculum/setup/command-line-tools/#slides","text":"Here's the presentation from the start of the workshop.","title":"Slides"},{"location":"curriculum/setup/command-line-tools/#lets-talk-about-the-weather","text":"Since there's been so much controversy over weather predictions from paid vs free apps this year, we're going to just do it ourselves and create out own predictions using weather data from NOAA. You can find daily data for the US here: ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz (The documentation is here )","title":"Let's talk about the weather"},{"location":"curriculum/setup/command-line-tools/#getting-data-from-the-command-line","text":"First we have to get the data. For that we're going to use curl. $ curl ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Whoa! Terminal is going crazy! This may impress your less savvy friends, but it's not going to help you answer your question. We need to stop this process. Try control-c. This is the universal escape command in terminal. We obviously didn't use curl right. Let's look up the manual for the command using man . $ man curl Looks like if we want to write this to a file, we've got to pass the -O argument. $ curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2016.csv.gz Let's check to see if it worked. $ ls -lah Great. Now we need to know the file format so we know what tool to use to unpack it. $ file 2016.csv.gz Looks like it's a gzip so we'll have to use gunzip . $ gunzip 2016.csv.gz $ ls -lah Now we've got a .csv file we can start playing with. Let's see how big it is using wc","title":"Getting Data from the Command Line"},{"location":"curriculum/setup/command-line-tools/#viewing-data-from-the-command-line","text":"The simpilest streaming command is cat . This dumps the whole file, line by line, into standard out and prints. $ cat 2016.csv That's a bit much. Let's see if we can slow it down by viewing the file page by page using more or less . $ less 2016.csv Great. But let's say I just want to see the top of the file to get a sense of it's structure. We can use head for that. $ head 2016.csv $ head -n 3 2016.csv Similarly, if I'm only interested in viewing the end of the file, I can use tail . $ tail 2016.csv These commands all print things out raw and bunched together. I want to take advantage of the fact that I know this is a csv to get a prettier view of the data. This is where csvkit starts to shine. The first command we'll use from csvkit is csvlook . $ csvlook 2016.csv But that's everything again. We just want to see the top. If only we could take the output from head and send it to csvlook . We can! It's called piping , and you do it like this: head 2016.csv | csvlook The output from head was sent to csvlook for processing. Piping and redirection (more on that later) are two of the most important concepts to keep in mind when using command line tools. Because most commands use text as the interface, you can chain commands together to create simple and powerful data processing pipelines!","title":"Viewing Data from the Command Line"},{"location":"curriculum/setup/command-line-tools/#filtering-data-from-the-command-line","text":"It looks like in order for us to make sense of the weather dataset, we're going to need to figure out what these station numbers mean. Let's grab the station dictionary from NOAA and take a look at it. $ curl -O https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt $ head ghcnd-stations.txt Looks like the sation description might come in handy. We want to look at just the stations in Chicago. $ grep CHICAGO ghcnd-stations.txt | csvlook -H Let's pick OHARE as the station we'll use for now. Its ID is 'USW00094846' Let's take a look at just the ID column from the weather file. We can do this using cut . $ cut -f 1 2016.csv Looks like cut isn't smart enough to know that we're using a csv. We can either use csvcut, or pass a delimiter argument that specifies comma. $ cut -d , -f 1 2016.csv | head Now let's filter out just the oberservations from OHARE. $ cut -d , -f 1 2016.csv | grep USW00094846 | head Another powerful tool that can do filtering (and much more) is awk . awk treats every file as a set of row-based records and allows you to create contition/{action} pairs for the records in the file. The default {action} in awk is to print the records that meet the condition. Let's try reproducing the above statement using awk . $ cut -d , -f 1 2016.csv | awk '/USW00094846/' | head awk requires familiarity with regular expressions for contitions and has its own language for actions, so man and stack overflow will be your friends if you want to go deep with awk .","title":"Filtering Data from the Command Line"},{"location":"curriculum/setup/command-line-tools/#editing-and-transforming-data","text":"Let's say we want to replace values in the files. PRCP is confusing. Let's change PRCP to RAIN. To do this, we use sed . sed stands for streaming editor, and is very useful for editing large text files because it doesn't have to load all the data into memory to make changes. Here's how we can use sed to replace a string. $ sed s/PRCP/RAIN/ 2016.csv | head Notice the strings have changed! But when we look at the source file $ head 2016.csv Noting has changed. That's because we didn't write it to a file. In fact, none of the changes we've made have. $ sed s/PRCP/RAIN/ 2016.csv > 2016_clean.csv $ head 2016_clean.csv We can also use awk for subsitution, but this time, let's replace \"WSFM\" with \"WINDSPEED\" in all the weather files in the directory. Once again, stackoverflow is your friend here. $ ls -la > files.txt $ awk '$9 ~/2016*/ {gsub(/WSFM/, \"WINDSPEED\"); print;}' files.txt","title":"Editing and Transforming Data"},{"location":"curriculum/setup/command-line-tools/#group-challenges","text":"For group challenges, log onto the training ec2 instance and change directories to /mnt/data/training/yourusername. This should be your working directory for all the excercises. Create a final weather file that just has weather data from OHARE airport for days when it rained, and change PRCP to RAIN. Save the sequence of commands to a shell script so it's replicable by your teammate and push to a training repository you've created on github. Create a separate file with just the weather from OHARE for days when the tempurature was above 70 degrees F. (hint: try using csvgrep to filter a specific column on a range of values) Get ready to explore the relationship between weather and crime in Chicago. Using crime data from 2016 (below), parse the json and convert it to a csv. Explore the fields and cut the dataset down to just day, location, and crime type. Then subset the dataset to just homicides and save as a new file. https://data.cityofchicago.org/resource/6zsd-86xi.json Using just command line tools, can you use the lat and long coordinates of the weather stations to rapidly identify which weather station is closest to the DSSG building?","title":"Group Challenges"},{"location":"curriculum/setup/command-line-tools/#cheat-sheet","text":"We're going to cover a variety of command line tools that help us obtain, parse, scrub, and explore your data. (The first few steps toward being an OSEMN data scientist). Here's a list of commands and concepts we'll cover: Getting to know you: navigating files and directories in the command line `cd mkdir ls file mv cp rm findit (bonus) Getting, unpacking, and parsing Data curl wget (bonus) gunzip tar (bonus) in2csv json2csv (bonus) Exploring Data wc cat less head tail csvlook Filtering, Slicing, and Transforming grep cut sed awk csvgrep csvcut csvjoin (bonus) jq (bonus; sed for JSON) Exploring & Summarizing csvstat Writing shell scripts","title":"Cheat Sheet"},{"location":"curriculum/setup/command-line-tools/#further-resources","text":"Jeroen Janssens wrote the book literally on data science in the command line. Also, check out his post on 7 essential command line tools for data scientists. For command line basics, Learning CLI the Hard Way is, as always, a great resource.","title":"Further Resources"},{"location":"curriculum/setup/command-line-tools/#potential-teachouts","text":"tmux : Getting your command line organized tmux is a great way to manage many environments at once. Give it a shot!","title":"Potential Teachouts"},{"location":"curriculum/setup/command-line-tools/cmdline/","text":"CommandLine, Git, and GitHub Tutorial (DSSG 2018) # Introduction # You are most likely comfortable interacting with a computer using point-and-click interfaces, also known as GUIs , or Graphical User Interfaces . But there is another way that came before and will always be: the CLI , or Command Line Interface . The command line interface allows you to communicate with your computer more directly than you can through a GUI, in a REPL , or read-evaluate-print loop format: The user (you!) types a command, then presses the return key. The computer reads the command, evaluates it, and prints out the output. This term comes from the days when we had to use physical printers called \"teleprinters\" to interact with computers. The software you use to communicate with your computer is known as a shell , because it acts as a \"shell\" from the underlying complexity of the operating system. The most popular shell is known as BASH, the Bourne Again Shell . Bash is the default shell for most UNIX systems and Macs. You can also look into more exotic shells like Xonsh, Zsh, plumbum, and fish. I personally like xonsh and zsh. So, why use the command line instead of a GUI? There are several reasons: In the shell you can easily handle data by chaining programs into a cohesive pipeline to handle large amounts of data. You can automate your workflow through scripting to save time and be more productive. A good rule of thumb is if you do something twice -- script it! Your work will be much more reproducible because there will be a digital record of the steps you took to produce your results compared to point and clicking in a program like Excel. If you're working with remote machines, like supercomputers or machines in the cloud, the CLI is often the only interface available. The CLI is usually much faster -- once you get the hang out of it. The learning objectives of this mini-tutorial are the following: Learn how to navigate a UNIX file system. Learn some basic shell commands. Learn how to use to use git and github. The commandline has always been and always will be. Navigating the File System # Now we'll learn how to navigate though a UNIX file system and how to create , edit , rename and copy files and directories within the system. First, a little background: UNIX has a hierarchical (tree-structure) directory organization known as the file system . In the file system our data is organized into files , which are organized in directories . The base of the directory is called the root directory and denoted by a / . All user-available disk space is combined into a single directory under '/'. Here is an example of the directory tree of a typical Linux system. From this figure, we can see all directories are under the root directory ( / ). The folders under the root directory contain information for the configuration and operation of the operating system, so you shouldn't change these (unless you really know what you are doing). The special folder home contains the files for all users. In this example, we have the directories rick , anna , emmy , bob , which contain files created by users rick, anna, emmy, and bob, respectively. To navigate through the file system, or change directories , we use the cd command. If you just type cd , without any arguements it will take you to your home directory. To see where you are within the file system, you use the pwd , or print working directory command: $ cd $ pwd /home/akumar You can also check who is logged into the machine with the whoami command. $ whoami akumar In my case, my home directory is located at /home/akumar , because as we saw above using the whoami command, my username is akumar . Yours will be something different. /home/akumar/ is a path ; in this case, the path leads to my home directory. A path can also be a path to a file name, for example /home/akumar/Documents/justice_league_meetings_notes.txt . Paths that start with / are called absolute paths , because they begin at the root directory. They're absolute because they start at the root (the beginning of the filesystem), so they will always be the same regardless of your current location . Relative paths start at your current location. So the relative path to the file above from my home directory ( /home/akumar ) would be Documents/justice_leage_meetings_notes.txt . Note that / has two meanings: To signify an absolute path (originating in the root directory), and to separate directories and files in the path. Let's assume that I (user akumar ) have the following directory data: To list all the files and subdirectories in a given directory, we can type ls : $ ls data Documents Try the command in your terminal. We see that the contents of akumar 's home directory are two directories called data and Documents , as we'd expect. If we want to see the contents of one of these directories, we can use the same ls command, but specify the directory whose contents we'd like to see as an argument to the ls command.: $ ls Documents/DailyPlanet_Articles superman_saves_the_day.txt Note that this would have the same output as running: $ cd Documents/DailyPlanet_Articles $ ls superman_saves_the_day.txt where we moved into the DailyPlanet_Articles directory and ran the ls command from within the directory. Say we've navigated to the DailyPlanet_Articles directory, and now we want to navigate to the data directory. There are multiple ways to do this: We could use the absolute path: cd /home/akumar/data/ (remember, that will work from anywhere in the filesystem), or we could use a relative path. But how do we use a relative path when data is not in our current directory, and we have to go up the tree to reach our destination? We can use the shorthand . . One . means our current directory, and .. means move up one directory relative to where we currently are in the filesystem: First, we would navigate to the DailyPlant_Articles directory: cd /home/akumar/Documents/DailyPlant_Articles Now let move up two directories into the data directory. cd ../../data So we move up one directory from DailyPlanet_Articles to Documents , then up one more directory to /home/akumar , then down one directory to data . Note: When you start typing in the path the data directory, you can use TAB to auto-complete . The TAB button is your friend on the command-line. If you ever get lost in the file system, remember that the command cd (without specifying another directory) will always drop you into your home directory. $ cd $ pwd /home/akumar Creating, Editing, Moving, Copying, and Deleting Files # Now let's put the UNIX file system to work for us. Let's get started in our home directory: $ cp <source-file> <destination-file> #copy file $ mv <source-file> <destination-file> #move file $ rm <source-file> <destination-file> #remove a file $ find ~/ -name '*.txt' #find all text files in my home directory Note: There is no Recycle Bin or Trash rm can be a weapon of mass destruction Using Git # Git is a version control system that allows you to take snapshots of your project so you can go back in time or safely add features without breaking your code. This is much much better than taking turns working on software and emailing versions to eachother. Git can be used for any project involving plain text files, including code, books, papers, small datasets. Git and GitHub are used for hosting, distributing, and collaborating on a project with others. Through tools like GitHub issues, GitHub Pull Requests, and branches you can manage large scale collaborations. An example of an open-source project here at DSaPP is Triage or Aequitas. Guidelines to keep in mind for effective collaboration # You are a team, work as a team. You are physically located with your teammate so actually work together. Know what eachother is doing. Collaborate, code review (and learn), Pair Program People come before process (most of the time). Say we are starting a new project called \"where_not_to_eat\". About the chicago food inspections. We are going to combine data from different years into a single CSV. First we are going to clone a project using git . Link to the project on github. $ git clone <add link to repo here> We are going to use the github flow for this project. First we want to configure our git profile. # See how your git config looks git config --list Set your workspace # Adding some if you dont have a user.name or user.email git config --global user.name \"Rayid Ghani\" git config --global user.email \"rayid.ghani@dssg.io\" git config --global color.ui \"auto\" git config --global core.editor 'vim' git config --global push.default current Different work flows: Solo Style mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" # hack # more hack GitHub Style Also know as the [[http://endoflineblog.com/gitflow-considered-harmful][/Anti-gitflow/]] [[https://guides.github.com/introduction/flow/][Github Flow]] (explained with images and animation!) Don't code anything if there isn't a need for it. First create good issues. A good issue is clear defined output actionable should only take a few days at most good: fix this bug , add this method (good to write in the imperative voice) bad: it doesn't work for me , finish the project Now do a git pull to fetch any changes in the remote repository and merge into your repository. $ git pull Now create your own \"branch\" of the project where you can make changes separate from the master branch. The master branch should always be pristine. $ git checkout -b <username>-branch Alternativly, you can name branches with the issue number of the issue you are working on (e.g., add_features_issue#44). Now we do a little hacking where we are going to write a bash script that will clean the header of a CSV file and concatenate all the files. Then we are going to push our changes to the remote repo. And create a pull request. Class Exercise # Download data via the commandline and clean it, concatenate files so it can be read into a database. First clone the project: Then create your own branch: $ git checkout -b <username>-branch Link to download data: https://github.com/avishekrk/where_not_to_eat/archive/master.zip Here is my solution: wget https://github.com/avishekrk/where_not_to_eat/archive/master.zip ; unzip master.zip ; rm -v master.zip mv -v where_not_to_eat-master raw cd ./raw/ pwd find -name \"* *\" | while read f ; do echo ${ f } ; new = $( echo $f | sed \"s/ /_/g\" ) ; echo ${ new } ; mv -v \" $f \" $new ; done mkdir -v ./../staging head -1 food_inspection_2018-01-01.csv | tr '[:upper:]' '[:lower:]' | sed -e \"s/#//g\" -e \"s/ ,/,/g\" -e \"s/ /_/g\" -e s \"/^,//g\" > header cat header > all_inspections.csv for f in food_inspection_2018-0*.csv ; do echo ${ f } ; awk 'NR > 1 {print}' ${ f } >> all_inspections.csv ; done mv -v all_inspections.csv ./../staging After saving our script we can commit our changes on our branch: git add clean.sh git commit -m \"Added script to clean and concatenate files git push #push to our remote repository Then you can create a Pull Request.","title":"CommandLine, Git, and GitHub Tutorial (DSSG 2018)"},{"location":"curriculum/setup/command-line-tools/cmdline/#commandline-git-and-github-tutorial-dssg-2018","text":"","title":"CommandLine, Git, and GitHub Tutorial (DSSG 2018)"},{"location":"curriculum/setup/command-line-tools/cmdline/#introduction","text":"You are most likely comfortable interacting with a computer using point-and-click interfaces, also known as GUIs , or Graphical User Interfaces . But there is another way that came before and will always be: the CLI , or Command Line Interface . The command line interface allows you to communicate with your computer more directly than you can through a GUI, in a REPL , or read-evaluate-print loop format: The user (you!) types a command, then presses the return key. The computer reads the command, evaluates it, and prints out the output. This term comes from the days when we had to use physical printers called \"teleprinters\" to interact with computers. The software you use to communicate with your computer is known as a shell , because it acts as a \"shell\" from the underlying complexity of the operating system. The most popular shell is known as BASH, the Bourne Again Shell . Bash is the default shell for most UNIX systems and Macs. You can also look into more exotic shells like Xonsh, Zsh, plumbum, and fish. I personally like xonsh and zsh. So, why use the command line instead of a GUI? There are several reasons: In the shell you can easily handle data by chaining programs into a cohesive pipeline to handle large amounts of data. You can automate your workflow through scripting to save time and be more productive. A good rule of thumb is if you do something twice -- script it! Your work will be much more reproducible because there will be a digital record of the steps you took to produce your results compared to point and clicking in a program like Excel. If you're working with remote machines, like supercomputers or machines in the cloud, the CLI is often the only interface available. The CLI is usually much faster -- once you get the hang out of it. The learning objectives of this mini-tutorial are the following: Learn how to navigate a UNIX file system. Learn some basic shell commands. Learn how to use to use git and github. The commandline has always been and always will be.","title":"Introduction"},{"location":"curriculum/setup/command-line-tools/cmdline/#navigating-the-file-system","text":"Now we'll learn how to navigate though a UNIX file system and how to create , edit , rename and copy files and directories within the system. First, a little background: UNIX has a hierarchical (tree-structure) directory organization known as the file system . In the file system our data is organized into files , which are organized in directories . The base of the directory is called the root directory and denoted by a / . All user-available disk space is combined into a single directory under '/'. Here is an example of the directory tree of a typical Linux system. From this figure, we can see all directories are under the root directory ( / ). The folders under the root directory contain information for the configuration and operation of the operating system, so you shouldn't change these (unless you really know what you are doing). The special folder home contains the files for all users. In this example, we have the directories rick , anna , emmy , bob , which contain files created by users rick, anna, emmy, and bob, respectively. To navigate through the file system, or change directories , we use the cd command. If you just type cd , without any arguements it will take you to your home directory. To see where you are within the file system, you use the pwd , or print working directory command: $ cd $ pwd /home/akumar You can also check who is logged into the machine with the whoami command. $ whoami akumar In my case, my home directory is located at /home/akumar , because as we saw above using the whoami command, my username is akumar . Yours will be something different. /home/akumar/ is a path ; in this case, the path leads to my home directory. A path can also be a path to a file name, for example /home/akumar/Documents/justice_league_meetings_notes.txt . Paths that start with / are called absolute paths , because they begin at the root directory. They're absolute because they start at the root (the beginning of the filesystem), so they will always be the same regardless of your current location . Relative paths start at your current location. So the relative path to the file above from my home directory ( /home/akumar ) would be Documents/justice_leage_meetings_notes.txt . Note that / has two meanings: To signify an absolute path (originating in the root directory), and to separate directories and files in the path. Let's assume that I (user akumar ) have the following directory data: To list all the files and subdirectories in a given directory, we can type ls : $ ls data Documents Try the command in your terminal. We see that the contents of akumar 's home directory are two directories called data and Documents , as we'd expect. If we want to see the contents of one of these directories, we can use the same ls command, but specify the directory whose contents we'd like to see as an argument to the ls command.: $ ls Documents/DailyPlanet_Articles superman_saves_the_day.txt Note that this would have the same output as running: $ cd Documents/DailyPlanet_Articles $ ls superman_saves_the_day.txt where we moved into the DailyPlanet_Articles directory and ran the ls command from within the directory. Say we've navigated to the DailyPlanet_Articles directory, and now we want to navigate to the data directory. There are multiple ways to do this: We could use the absolute path: cd /home/akumar/data/ (remember, that will work from anywhere in the filesystem), or we could use a relative path. But how do we use a relative path when data is not in our current directory, and we have to go up the tree to reach our destination? We can use the shorthand . . One . means our current directory, and .. means move up one directory relative to where we currently are in the filesystem: First, we would navigate to the DailyPlant_Articles directory: cd /home/akumar/Documents/DailyPlant_Articles Now let move up two directories into the data directory. cd ../../data So we move up one directory from DailyPlanet_Articles to Documents , then up one more directory to /home/akumar , then down one directory to data . Note: When you start typing in the path the data directory, you can use TAB to auto-complete . The TAB button is your friend on the command-line. If you ever get lost in the file system, remember that the command cd (without specifying another directory) will always drop you into your home directory. $ cd $ pwd /home/akumar","title":"Navigating the File System"},{"location":"curriculum/setup/command-line-tools/cmdline/#creating-editing-moving-copying-and-deleting-files","text":"Now let's put the UNIX file system to work for us. Let's get started in our home directory: $ cp <source-file> <destination-file> #copy file $ mv <source-file> <destination-file> #move file $ rm <source-file> <destination-file> #remove a file $ find ~/ -name '*.txt' #find all text files in my home directory Note: There is no Recycle Bin or Trash rm can be a weapon of mass destruction","title":"Creating, Editing, Moving, Copying, and Deleting Files"},{"location":"curriculum/setup/command-line-tools/cmdline/#using-git","text":"Git is a version control system that allows you to take snapshots of your project so you can go back in time or safely add features without breaking your code. This is much much better than taking turns working on software and emailing versions to eachother. Git can be used for any project involving plain text files, including code, books, papers, small datasets. Git and GitHub are used for hosting, distributing, and collaborating on a project with others. Through tools like GitHub issues, GitHub Pull Requests, and branches you can manage large scale collaborations. An example of an open-source project here at DSaPP is Triage or Aequitas.","title":"Using Git"},{"location":"curriculum/setup/command-line-tools/cmdline/#guidelines-to-keep-in-mind-for-effective-collaboration","text":"You are a team, work as a team. You are physically located with your teammate so actually work together. Know what eachother is doing. Collaborate, code review (and learn), Pair Program People come before process (most of the time). Say we are starting a new project called \"where_not_to_eat\". About the chicago food inspections. We are going to combine data from different years into a single CSV. First we are going to clone a project using git . Link to the project on github. $ git clone <add link to repo here> We are going to use the github flow for this project. First we want to configure our git profile. # See how your git config looks git config --list Set your workspace # Adding some if you dont have a user.name or user.email git config --global user.name \"Rayid Ghani\" git config --global user.email \"rayid.ghani@dssg.io\" git config --global color.ui \"auto\" git config --global core.editor 'vim' git config --global push.default current Different work flows: Solo Style mkdir my_working_directory cd my_working_directory git init touch some_file.py # hack # hack git add some_file.py git commit -m \"Working with some awesome idea\" # hack # more hack GitHub Style Also know as the [[http://endoflineblog.com/gitflow-considered-harmful][/Anti-gitflow/]] [[https://guides.github.com/introduction/flow/][Github Flow]] (explained with images and animation!) Don't code anything if there isn't a need for it. First create good issues. A good issue is clear defined output actionable should only take a few days at most good: fix this bug , add this method (good to write in the imperative voice) bad: it doesn't work for me , finish the project Now do a git pull to fetch any changes in the remote repository and merge into your repository. $ git pull Now create your own \"branch\" of the project where you can make changes separate from the master branch. The master branch should always be pristine. $ git checkout -b <username>-branch Alternativly, you can name branches with the issue number of the issue you are working on (e.g., add_features_issue#44). Now we do a little hacking where we are going to write a bash script that will clean the header of a CSV file and concatenate all the files. Then we are going to push our changes to the remote repo. And create a pull request.","title":"Guidelines to keep in mind for effective collaboration"},{"location":"curriculum/setup/command-line-tools/cmdline/#class-exercise","text":"Download data via the commandline and clean it, concatenate files so it can be read into a database. First clone the project: Then create your own branch: $ git checkout -b <username>-branch Link to download data: https://github.com/avishekrk/where_not_to_eat/archive/master.zip Here is my solution: wget https://github.com/avishekrk/where_not_to_eat/archive/master.zip ; unzip master.zip ; rm -v master.zip mv -v where_not_to_eat-master raw cd ./raw/ pwd find -name \"* *\" | while read f ; do echo ${ f } ; new = $( echo $f | sed \"s/ /_/g\" ) ; echo ${ new } ; mv -v \" $f \" $new ; done mkdir -v ./../staging head -1 food_inspection_2018-01-01.csv | tr '[:upper:]' '[:lower:]' | sed -e \"s/#//g\" -e \"s/ ,/,/g\" -e \"s/ /_/g\" -e s \"/^,//g\" > header cat header > all_inspections.csv for f in food_inspection_2018-0*.csv ; do echo ${ f } ; awk 'NR > 1 {print}' ${ f } >> all_inspections.csv ; done mv -v all_inspections.csv ./../staging After saving our script we can commit our changes on our branch: git add clean.sh git commit -m \"Added script to clean and concatenate files git push #push to our remote repository Then you can create a Pull Request.","title":"Class Exercise"},{"location":"curriculum/skills/domain_understanding/","text":"","title":"Domain Understanding"},{"location":"curriculum/skills/ethics_bias_fairness/","text":"","title":"Ethics, Bias, Fairness"},{"location":"curriculum/software/","text":"","title":"Home"},{"location":"curriculum/software/basic_python/","text":"","title":"Python"},{"location":"curriculum/software/basic_sql/","text":"","title":"SQL"},{"location":"curriculum/software/good_practices/","text":"","title":"Good software practices"},{"location":"curriculum/software/python_pandas/","text":"","title":"pandas"},{"location":"curriculum/software/sql_data_analysis/","text":"","title":"SQL"},{"location":"curriculum/software/testing/","text":"","title":"Software testing"},{"location":"curriculum/store_data/etl/","text":"","title":"ETL - cleaning, loading"},{"location":"curriculum/tutorial-template/","text":"Tutorial Template Session: Making Meringues # Motivation # A very brief abstract. This tutorial teaches the basics of making meringues. These simple preparations from sugar and eggwhites are an integral part of many french-inspired desserts and surprisingly easy to make. Potential Teachouts # Always include a list of relevant, more advanced topics, and encourage fellows to teach them in the coming weeks. Jane is running the schedule for fellow teachouts. This tutorial only covers the very basics of making meringues. If you are an experienced baker, please share your knowledge and schedule a fellow teachout! Here are some topics that would be helpful: - flavored meringues - marbled meringues - recipes with meringues Content # This section can either contain the content itself, or link to a slide presentation. When linking to a Google Presentation, make sure to allow comments there, and put a PDF copy of your presentation into the tutorial folder itself (and make sure you're not excluding PDFs via your .gitignore ). Google Slides Presentation If you have worksheets for fellows to go through during the session, place them in a worksheets folder, and also include solutions for future reference! Cheat Sheet # This section is only necessary for tech sessions, but important! It should list all relevant commands with very brief explanations for quick future reference. - 1 cup granulated sugar - 1 cup fine sugar - 8 egg whites - ... Further Resources # Links to further tutorials, videos, blog posts... - Basic Meringue Recipe Discussion Notes # Optional. If interesting points come up during the discussion, add them here.","title":"Tutorial Template Session: Making Meringues"},{"location":"curriculum/tutorial-template/#tutorial-template-session-making-meringues","text":"","title":"Tutorial Template Session: Making Meringues"},{"location":"curriculum/tutorial-template/#motivation","text":"A very brief abstract. This tutorial teaches the basics of making meringues. These simple preparations from sugar and eggwhites are an integral part of many french-inspired desserts and surprisingly easy to make.","title":"Motivation"},{"location":"curriculum/tutorial-template/#potential-teachouts","text":"Always include a list of relevant, more advanced topics, and encourage fellows to teach them in the coming weeks. Jane is running the schedule for fellow teachouts. This tutorial only covers the very basics of making meringues. If you are an experienced baker, please share your knowledge and schedule a fellow teachout! Here are some topics that would be helpful: - flavored meringues - marbled meringues - recipes with meringues","title":"Potential Teachouts"},{"location":"curriculum/tutorial-template/#content","text":"This section can either contain the content itself, or link to a slide presentation. When linking to a Google Presentation, make sure to allow comments there, and put a PDF copy of your presentation into the tutorial folder itself (and make sure you're not excluding PDFs via your .gitignore ). Google Slides Presentation If you have worksheets for fellows to go through during the session, place them in a worksheets folder, and also include solutions for future reference!","title":"Content"},{"location":"curriculum/tutorial-template/#cheat-sheet","text":"This section is only necessary for tech sessions, but important! It should list all relevant commands with very brief explanations for quick future reference. - 1 cup granulated sugar - 1 cup fine sugar - 8 egg whites - ...","title":"Cheat Sheet"},{"location":"curriculum/tutorial-template/#further-resources","text":"Links to further tutorials, videos, blog posts... - Basic Meringue Recipe","title":"Further Resources"},{"location":"curriculum/tutorial-template/#discussion-notes","text":"Optional. If interesting points come up during the discussion, add them here.","title":"Discussion Notes"},{"location":"dssg-manual/","text":"We put together this manual for fellows in the Data Science for Social Good program. We are making it public to provide insight into the program to anyone interested in doing data science for social good, including potential fellows, mentors, and project partners, as well as those interested in funding or replicating such a program. The DSSG Manual # Before You Arrive : Prerequisites and Software Setup What to Expect from the Fellowship : Code of Conduct, Culture and Communications and Summer Overview Welcome to the Data Science for Social Good (DSSG) Fellowship program! We hope your experience this summer will help you grow your skills as a data scientist and learn how to apply those skills to solve real-world problems with social impact. This manual outlines our goals in running this fellowship program, our hopes for your experience, and our expectations of the participants. We\u2019ve also outlined how the summer is typically structured and what you can expect from us. Credits # This manual was created collaboratively at the Center for Data Science and Public Policy at the University of Chicago, with lots of help from various sources including those listed below. Contributors include Bridgit Donnelly, Matt Gee, Rayid Ghani, Maya Grever, Lauren Haynes, Jen Helsby, Lindsay Knight, Benedict Kuester, Joe Walsh, and Jane Zanzig. This manual is licensed under the Creative Commons Zero license. Parts of this manual are based on several other policies, including - The Recurse Center User's Manual - AlterConf Code of Conduct - Django Community Code of Conduct - SRCCON Code of Conduct - Citizen Code of Conduct","title":"What is in this manual"},{"location":"dssg-manual/#the-dssg-manual","text":"Before You Arrive : Prerequisites and Software Setup What to Expect from the Fellowship : Code of Conduct, Culture and Communications and Summer Overview Welcome to the Data Science for Social Good (DSSG) Fellowship program! We hope your experience this summer will help you grow your skills as a data scientist and learn how to apply those skills to solve real-world problems with social impact. This manual outlines our goals in running this fellowship program, our hopes for your experience, and our expectations of the participants. We\u2019ve also outlined how the summer is typically structured and what you can expect from us.","title":"The DSSG Manual"},{"location":"dssg-manual/#credits","text":"This manual was created collaboratively at the Center for Data Science and Public Policy at the University of Chicago, with lots of help from various sources including those listed below. Contributors include Bridgit Donnelly, Matt Gee, Rayid Ghani, Maya Grever, Lauren Haynes, Jen Helsby, Lindsay Knight, Benedict Kuester, Joe Walsh, and Jane Zanzig. This manual is licensed under the Creative Commons Zero license. Parts of this manual are based on several other policies, including - The Recurse Center User's Manual - AlterConf Code of Conduct - Django Community Code of Conduct - SRCCON Code of Conduct - Citizen Code of Conduct","title":"Credits"},{"location":"dssg-manual/conduct-culture-and-communications/","text":"Our Code of Conduct applies to all participants in all of our events open to external partners and the public, as well as to the fellowship itself. We want to ensure that everyone who comes into the DSSG space to feel welcome, and we want to foster a safe, productive environment for fellows, staff, and visitors. Important Our Anti-Harassment Policy explicitly outlines the behavior for which we have a zero tolerance policy . If you feel that you or anyone else is being harassed or treated unfairly, or have any other concerns related to this policy, please contact one of your fellow advocates. All contact with the fellow advocates will be confidential. Data Science for Social Good is dedicated to providing a harassment-free experience in all event venues, including talks, parties, and online media, for everyone regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age or religion. We do not tolerate harassment of participants in any form. Harassment includes offensive verbal comments related to gender, sexual orientation, disability, gender identity, age, race, religion, the use or display of sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any such behavior are expected to comply immediately. Participants asked to stop any harassing behavior are expected to comply immediately and may be sanctioned or expelled from the fellowship and/or all related events at the discretion of the organizers. Organizers may take any lawful action we deem appropriate, including but not limited to warning the offender or asking the offender to leave the specific event or the fellowship program as a whole. If you feel anyone has violated this policy, please bring it up directly with the individual or with a DSSG staff member or fellow advocate. If you feel you have been unfairly accused of violating this code of conduct, please contact the fellowship staff team with a concise description of your grievance; all grievances will be considered.","title":"Code of conduct"},{"location":"dssg-manual/conduct-culture-and-communications/environment/","text":"The Space # Each year we rent a space in the host city . While the space changes from year to year, in general the layout is open, and teams sit at desks together. There is private conference space for team meetings and calls with partners. Upon arrival, you will receive the conference room request and reservation policy. We also provide access to a kitchen, with coffee and an espresso maker. We provide a limited supply of snacks and catered meals for special events. Let us know if you have any special food restrictions. You will learn more about space logistics (location, key card access, etc.) by email as they are finalized in the weeks leading up to the fellowship. The People # The foundation of any good project is a good team. We\u2019ve worked to recruit and hand pick a passionate and skilled team of interdisciplinary folks that all bring unique skills to your cohort. The fellowship is comprised of teams of three or four fellows each. Your teammates will be fellow graduate students and recent graduates. We aim to ensure that each team has a mix of backgrounds, from computer science, statistics, math, physical science and engineering, social sciences and public policy. Your team will be assigned a Senior Data Science Mentor . Each Mentor will be working with several teams of fellows, supporting their growth and project. All mentors are experienced data scientists who serve as a resource for you through the project development process, both with hands-on technical problems and implementation questions, as well as through higher-level design decisions. In addition to Technical Mentors, you will have a Project Manager . Each Project Manager oversees several teams and is responsible for managing your relationship with your project partner. They work with you and your partner to set goals and deadlines, ensure proper communication between your team and your partner, and help tackle issues that are blocking your progress. Your Project Manager is also a great resource for questions about organizing teamwork, improving presentation skills, and communicating with the public. If fellows have any kind of questions or concerns that they would like to discuss confidentially, they can address the Fellow Advocate . This is a member of staff who is not involved directly in managing the fellowship, and who is available to help with any conflict between fellows and members of staff. The advocate can also raise concerns or ask for help on behalf of fellows, should they feel uncomfortable doing so themselves. The fellowship has one or more Interns , who help with all organizational and administrative tasks that the summer brings, like setting up the space, helping organize and publicize events, and recording tutorials. In their remaining time, interns might also help teams with their projects or work on their own self-directed data science projects. The Communications Manager prepares media and press releases and manages interactions with the press. The communications manager helps fellows practice and polish their final presentations, and gives feedback on blog posts and other write-ups. We strongly believe in the importance of communicating the work we do, to our project partners, as well as to the broader public. To that end, we will spend a lot of the summer asking fellows to present their work and give them continuous feedback on the presentations. The Fellowship Organizers have spent months planning and preparing the summer program. They select fellows, mentors, project managers, interns, and project partners, find a space, secure funding, prepare the summer\u2019s curriculum, and plan all fellowship events. Over the summer, they will lead some of the fellowship-wide activities (such as the weekly deep dives and stand-ups), and teach some of the workshops. They also supervise the mentors, project managers, interns, and the communications manager. The Curriculum # Our goal is for you to learn a LOT this summer. We want you to feel empowered to drive your education throughout the summer. We see learning opportunities falling into three main buckets: Self-Directed Learning : We will share specific resources and guidelines for topics that we find useful in order to kick-start this learning process. We then encourage you to dive in and learn the skills most applicable to you and your growth. Peer-Directed Learning : We aim to create an environment that facilitates learning among fellows. Whether it\u2019s an informal discussion over lunch or a more formal teaching session, we encourage you to take advantage of the diversity of experiences and skills in the room. Fellowship-Directed Learning : Lastly, we have developed a specific workshop curriculum to cover basic concepts that we believe are essential for the summer. It is up to you to make sure you are seeking the resources you need to learn the skills you want to learn. For example, in the past, groups of fellows have started reading groups to learn about similar topics, like deep learning, together. That being said, if you are lost, speak up. You shouldn't feel that you are unable to make a meaningful contribution to your project. Your technical mentor is available to help you tackle skill gaps. In all of this, we recognize that there is no definitive \u201cData Science for Social Good\u201d curriculum or roadmap. We are charting new territory and developing it together. Throughout the summer, there will be plenty of opportunities for feedback and brainstorming on how to improve learning; for example, in past summers we have held an informal \u201cDunkin\u2019 Discussion\u201d series where we discuss the future of data science for social good over donuts. The Tools # We typically use GitHub for storing our codebase, a cloud service provider like Amazon Web Services for our data storage and analysis, Slack for fellowship-wide communication, and Trello for project management. We also store team-wide and fellowship-wide documents on Google Drive , and we schedule meetings on Google Calendar . You will receive a email address from the host to use for the duration of the fellowship. You are expected to use this for all fellowship-related communication. Call to action! Be sure that you create and share your username to all of this services. Also ask for the service's URL addresses specific to your project! The Communication # Teams will work together to develop specific team norms, but each team will have a daily morning stand up meeting with their project manager and technical mentors. In these meetings, each fellow will have the opportunity to discuss what they did the day before, what they\u2019re planning to do today, and what they\u2019re stuck on. In addition to that daily meeting, teams will have weekly conference calls with their project partner to provide updates, ask questions, and receive feedback on their progress. The Fun # While this is a job \u2014 and we expect you to treat it as such \u2014 we would hate for the summer to be all work and no play. We want to help foster a community among your cohort. We start the summer off with a host of orientation events , including a fellowship-wide picnic, a variety of icebreakers, and a scavenger hunt for you to get to know the city. We host \u201cUn-DSSG\u201d, a day for you to share your side passions (from fondue making to dance) with your new peers. Throughout the summer, we host happy hours every other week, and invite the larger data science, tech, startup, government, and non-profit communities. Nearly daily, we pull out the ping pong table after hours, often leading to intense rivalries tracked on Slack by Pongbot.","title":"The DSSG Environment"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-space","text":"Each year we rent a space in the host city . While the space changes from year to year, in general the layout is open, and teams sit at desks together. There is private conference space for team meetings and calls with partners. Upon arrival, you will receive the conference room request and reservation policy. We also provide access to a kitchen, with coffee and an espresso maker. We provide a limited supply of snacks and catered meals for special events. Let us know if you have any special food restrictions. You will learn more about space logistics (location, key card access, etc.) by email as they are finalized in the weeks leading up to the fellowship.","title":"The Space"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-people","text":"The foundation of any good project is a good team. We\u2019ve worked to recruit and hand pick a passionate and skilled team of interdisciplinary folks that all bring unique skills to your cohort. The fellowship is comprised of teams of three or four fellows each. Your teammates will be fellow graduate students and recent graduates. We aim to ensure that each team has a mix of backgrounds, from computer science, statistics, math, physical science and engineering, social sciences and public policy. Your team will be assigned a Senior Data Science Mentor . Each Mentor will be working with several teams of fellows, supporting their growth and project. All mentors are experienced data scientists who serve as a resource for you through the project development process, both with hands-on technical problems and implementation questions, as well as through higher-level design decisions. In addition to Technical Mentors, you will have a Project Manager . Each Project Manager oversees several teams and is responsible for managing your relationship with your project partner. They work with you and your partner to set goals and deadlines, ensure proper communication between your team and your partner, and help tackle issues that are blocking your progress. Your Project Manager is also a great resource for questions about organizing teamwork, improving presentation skills, and communicating with the public. If fellows have any kind of questions or concerns that they would like to discuss confidentially, they can address the Fellow Advocate . This is a member of staff who is not involved directly in managing the fellowship, and who is available to help with any conflict between fellows and members of staff. The advocate can also raise concerns or ask for help on behalf of fellows, should they feel uncomfortable doing so themselves. The fellowship has one or more Interns , who help with all organizational and administrative tasks that the summer brings, like setting up the space, helping organize and publicize events, and recording tutorials. In their remaining time, interns might also help teams with their projects or work on their own self-directed data science projects. The Communications Manager prepares media and press releases and manages interactions with the press. The communications manager helps fellows practice and polish their final presentations, and gives feedback on blog posts and other write-ups. We strongly believe in the importance of communicating the work we do, to our project partners, as well as to the broader public. To that end, we will spend a lot of the summer asking fellows to present their work and give them continuous feedback on the presentations. The Fellowship Organizers have spent months planning and preparing the summer program. They select fellows, mentors, project managers, interns, and project partners, find a space, secure funding, prepare the summer\u2019s curriculum, and plan all fellowship events. Over the summer, they will lead some of the fellowship-wide activities (such as the weekly deep dives and stand-ups), and teach some of the workshops. They also supervise the mentors, project managers, interns, and the communications manager.","title":"The People"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-curriculum","text":"Our goal is for you to learn a LOT this summer. We want you to feel empowered to drive your education throughout the summer. We see learning opportunities falling into three main buckets: Self-Directed Learning : We will share specific resources and guidelines for topics that we find useful in order to kick-start this learning process. We then encourage you to dive in and learn the skills most applicable to you and your growth. Peer-Directed Learning : We aim to create an environment that facilitates learning among fellows. Whether it\u2019s an informal discussion over lunch or a more formal teaching session, we encourage you to take advantage of the diversity of experiences and skills in the room. Fellowship-Directed Learning : Lastly, we have developed a specific workshop curriculum to cover basic concepts that we believe are essential for the summer. It is up to you to make sure you are seeking the resources you need to learn the skills you want to learn. For example, in the past, groups of fellows have started reading groups to learn about similar topics, like deep learning, together. That being said, if you are lost, speak up. You shouldn't feel that you are unable to make a meaningful contribution to your project. Your technical mentor is available to help you tackle skill gaps. In all of this, we recognize that there is no definitive \u201cData Science for Social Good\u201d curriculum or roadmap. We are charting new territory and developing it together. Throughout the summer, there will be plenty of opportunities for feedback and brainstorming on how to improve learning; for example, in past summers we have held an informal \u201cDunkin\u2019 Discussion\u201d series where we discuss the future of data science for social good over donuts.","title":"The Curriculum"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-tools","text":"We typically use GitHub for storing our codebase, a cloud service provider like Amazon Web Services for our data storage and analysis, Slack for fellowship-wide communication, and Trello for project management. We also store team-wide and fellowship-wide documents on Google Drive , and we schedule meetings on Google Calendar . You will receive a email address from the host to use for the duration of the fellowship. You are expected to use this for all fellowship-related communication. Call to action! Be sure that you create and share your username to all of this services. Also ask for the service's URL addresses specific to your project!","title":"The Tools"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-communication","text":"Teams will work together to develop specific team norms, but each team will have a daily morning stand up meeting with their project manager and technical mentors. In these meetings, each fellow will have the opportunity to discuss what they did the day before, what they\u2019re planning to do today, and what they\u2019re stuck on. In addition to that daily meeting, teams will have weekly conference calls with their project partner to provide updates, ask questions, and receive feedback on their progress.","title":"The Communication"},{"location":"dssg-manual/conduct-culture-and-communications/environment/#the-fun","text":"While this is a job \u2014 and we expect you to treat it as such \u2014 we would hate for the summer to be all work and no play. We want to help foster a community among your cohort. We start the summer off with a host of orientation events , including a fellowship-wide picnic, a variety of icebreakers, and a scavenger hunt for you to get to know the city. We host \u201cUn-DSSG\u201d, a day for you to share your side passions (from fondue making to dance) with your new peers. Throughout the summer, we host happy hours every other week, and invite the larger data science, tech, startup, government, and non-profit communities. Nearly daily, we pull out the ping pong table after hours, often leading to intense rivalries tracked on Slack by Pongbot.","title":"The Fun"},{"location":"dssg-manual/conduct-culture-and-communications/goals/","text":"Goals of the Fellowship # Our long-term, overarching, unifying goal is to see more of this type of work happen in the world, and for our fellows to leave the program better equipped and more likely to use their skills for social good. To achieve this vision, our program focuses on three guiding goals. Our goals are (in order of priority): Training Our Fellows Introducing Data Science to the Social Sector Building a Community of Data Scientists for Social Good We know that these goals are lofty. We do not presume or pretend to know the optimal way to achieve them, but we believe it's worth trying. We want all of you to help us by actively contributing ideas to improve our program and achieve these goals more effectively. Training Our Fellows # First and foremost, DSSG is a training program for fellows. We believe that the best way to learn is by working on real projects and not toy examples, which is why fellows work on real projects for real partners , in teams with other real people . The program provides training in the form of hands-on technical data science experience, but that\u2019s not enough to do real data science for social good. The training aspect of the fellowship also includes working with project partners, understanding social issues, using social science methods, working collaboratively on a team, developing solutions in an agile way, and communicating effectively to technical and non-technical audiences. We start off with a week of intensive orientation, getting everyone acquainted with each other and the structure of the program, as well as making sure all the fellows are up to speed with the tools and methods that are fellowship-wide standards. Training continues throughout the summer with lectures and workshops by staff, guest speakers, and fellow teach-outs. Dedicated full-time data science mentors and project managers will support and guide fellows throughout the summer. In short, we want the summer to be a productive and collaborative experience for you, and will provide you with many resources; however, your biggest resource will be yourself and the other fellows in your cohort . Every fellow contributes their own wealth of experience and expertise, and we aim to foster a learning environment where everyone can share this knowledge and learn from one another. Introducing Data Science to the Social Sector # We believe that data-driven decisionmaking isn\u2019t reserved for companies selling online advertisements or banks trying to detect fraud. We know that data science can help governments, non-profits, and social good organizations do their work more effectively. All of our project partners collect data, and many are already using data in some way, whether to evaluate their programs and write reports for their funders. However, most social good organizations have not used data science to actively inform their ongoing decision processes. Through this program, we aim to increase awareness of the benefits and challenges of data-driven impact work, both among the partners we work with and among non-profits and governments in general. Our project partners are partners, not clients. This means that the fellows work with the partners, not for them. We believe that participating in this program helps both the partners and the fellows develop a common language. Sometimes fellows and partners won't see eye to eye on every decision, or the need to complete work within a deadline will mean you have to adjust your expectations, put GNU/Emacs 1 on the back burner, or sacrifice doing the work exactly the way you had hoped. While fellows' learning is our primary priority, it is important to note that part of what fellows are learning is how to work with partners to produce work that is useful for the partners and delivered on time. Building a Community of Data Scientists for Social Good # We hope -- and expect -- that your impact as a DSSG alum continues beyond your summer tenure. Throughout the summer, we will introduce you to other practitioners within the data science and social good spaces to help you understand these sectors, form relationships, and start to think about potential contributions you could make. You\u2019ll also have the opportunity to network with local data science, tech, government, non-profit, and startup communities through regular fellowship-sponsored happy hours and meetups. Whether it\u2019s continuing to collaborate with your DSSG colleagues on other social good projects, joining the data team at a government agency, or working to recruit other like-minded people to apply their in-demand data science skills to impactful problems, we hope that this is just the beginning of of a lifetime as a data scientist for social good. Our Hopes for Your Fellowship Experience # The fellowship provides you with the opportunity to learn; to work on important, challenging, and unique projects; and to meet a lot of people who share your interests and goals. It is up to you to take advantage of these opportunities. We hope you use the summer to: Meet a group of fellows, staff and project partners who have a wealth of skills and experiences; listen to and learn from them; and make new friends. Embrace gaps in your knowledge as learning opportunities, exploring your limitations with respect to technical skills, new methods from different disciplines, project management, social issues, and teamwork. Learn about the challenges of working on real projects that don\u2019t have clean data, guaranteed results, or elegant solutions. Navigate the triangle between learning technical skills, creating deliverables that are useful and actionable for your project partner, and putting the varied skills within your team to good use. Rise to the challenge of working on a team that will include strong personalities with diverse experiences and strengths Realize the ambiguity and uncertainty that comes with working in a traditionally less tech-savvy sector, and learn how to communicate effectively to bridge this gap. Explore the impact (intentional and unintentional) that working with data from real, often disadvantaged or marginalized, people might have on them. Think deeply about the scope and limitations of technology to improve social problems. Explore existing roles in the field of data science for social good, find one you are best suited for, or create your own. Note Throughout the summer, we encourage you to share your ideas about how to improve the fellowship experience for yourself and others -- and to put them into action. We are constantly trying to improve the fellowship every day throughout the summer, and over time as we learn from each cohort. Our Expectations of Fellows # The fellowship offers a lot of freedom; however, we expect all fellows to stick to the basic principles of conduct listed below at all times. These guidelines apply to everybody at the fellowship, including mentors, project managers, and the fellowship organizers. If you feel that anyone is not behaving in accordance with these guidelines, we invite you to bring it up constructively. If you are unsure who to address, or if you do not feel comfortable doing so directly, you can bring up your concerns with the fellow advocate. Be supportive, open-minded, and willing to compromise. DSSG brings together people from different backgrounds and with different skills. In fact, this might be the best resource the fellowship has to offer! Share your knowledge and your experience with each other. Be patient as you teach each other, and have an open ear for your peers. Be professional. You will be working with NGOs, non-profits, and government institutions as project partners. You will also be presenting your work at and attending events with the general public. In all of these capacities you are acting as a representative of the DSSG community. We expect you to be professional \u2014 that is, respectful, friendly, and on time \u2014 in your conduct with partners and the public alike. Be resourceful and pragmatic. Own your own learning. Seek out resources as necessary. Don't be shy to ask others for help, but be mindful of their time - tell them what what you do understand, where you're stuck, and what you\u2019ve already tried, so they know how they can help. When you notice problems or have ideas for improvements \u2014 be it in your team, your project, or the fellowship \u2014 don\u2019t rely on others to notice or fix them; offer your initiative. Deal with conflicts maturely. There are many potential sources of conflict throughout the fellowship. It is perfectly acceptable, and even expected, that you will run into conflicts with your team, your mentors, your project, or the fellowship organizers. In any case, we ask you to be productive, pragmatic, and mature when dealing with conflicts. Keep an open mind, listen and communicate with everybody involved. Neither your project, nor your team, nor the fellowship will be perfect. Remember that everyone involved has invested a lot in the fellowship and wants all participants to be happy. Show up. We have all committed to be here for the duration of the program. The fellowship runs from approximately from end of May through the end of August 2 . We expect you to be in the office five days a week, to attend all DSSG-wide sessions, occasional special events, and the final event. We recognize that you have a life outside of the fellowship, and if you have any known or potential absences, you must inform DSSG staff upon your acceptance. Any additional conflicts that arise during the summer must be discussed with and agreed between you, your team, and your project manager well ahead of time. Take care of the space. Offices, meeting rooms, and kitchen areas are shared spaces. It\u2019s everybody\u2019s job to keep the space clean and free of messes. This policy also applies when you are attending off-site events. Stay involved and act as a steward. As a member of the DSSG community, we expect a commitment from you to stay involved, even after the summer. We ask you to seek opportunities to present the work you did, whether it's at your university, company, or events in your area. We ask you to assist your team in writing publications about your project, both during and after the fellowship. We will also ask you to help us with the application process in the following years by reviewing applications and interviewing candidates. We ask that you do what you're able to contribute to the DSSG mission and community. DON'T PANIC Regardless of how much experience you have, we admitted you because we believe that you can make a valuable contribution to your cohort, and we think being a DSSG fellow will help prepare you to do data science for social good in the real world. We've made a commitment to you and want to do everything we can to help you succeed. This is really important, so we'll say it again, in bold: If you're reading this, you are here because we want you to be here and believe that you are ready to make an impact. For example, don't worry about how much more or less productive, knowledgable, or experienced other fellows in your cohort might appear to be. It's easy to only pay attention - and compare yourself - to those who seem to be doing particularly well. Know that everyone has their own struggles, and everyone has good and bad days. Or if you prefer learning VIM \u21a9 In US: from Memorial Day through Labor Day, specific dates change by year and location \u21a9","title":"Goals of the Fellowship"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#goals-of-the-fellowship","text":"Our long-term, overarching, unifying goal is to see more of this type of work happen in the world, and for our fellows to leave the program better equipped and more likely to use their skills for social good. To achieve this vision, our program focuses on three guiding goals. Our goals are (in order of priority): Training Our Fellows Introducing Data Science to the Social Sector Building a Community of Data Scientists for Social Good We know that these goals are lofty. We do not presume or pretend to know the optimal way to achieve them, but we believe it's worth trying. We want all of you to help us by actively contributing ideas to improve our program and achieve these goals more effectively.","title":"Goals of the Fellowship"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#training-our-fellows","text":"First and foremost, DSSG is a training program for fellows. We believe that the best way to learn is by working on real projects and not toy examples, which is why fellows work on real projects for real partners , in teams with other real people . The program provides training in the form of hands-on technical data science experience, but that\u2019s not enough to do real data science for social good. The training aspect of the fellowship also includes working with project partners, understanding social issues, using social science methods, working collaboratively on a team, developing solutions in an agile way, and communicating effectively to technical and non-technical audiences. We start off with a week of intensive orientation, getting everyone acquainted with each other and the structure of the program, as well as making sure all the fellows are up to speed with the tools and methods that are fellowship-wide standards. Training continues throughout the summer with lectures and workshops by staff, guest speakers, and fellow teach-outs. Dedicated full-time data science mentors and project managers will support and guide fellows throughout the summer. In short, we want the summer to be a productive and collaborative experience for you, and will provide you with many resources; however, your biggest resource will be yourself and the other fellows in your cohort . Every fellow contributes their own wealth of experience and expertise, and we aim to foster a learning environment where everyone can share this knowledge and learn from one another.","title":"Training Our Fellows"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#introducing-data-science-to-the-social-sector","text":"We believe that data-driven decisionmaking isn\u2019t reserved for companies selling online advertisements or banks trying to detect fraud. We know that data science can help governments, non-profits, and social good organizations do their work more effectively. All of our project partners collect data, and many are already using data in some way, whether to evaluate their programs and write reports for their funders. However, most social good organizations have not used data science to actively inform their ongoing decision processes. Through this program, we aim to increase awareness of the benefits and challenges of data-driven impact work, both among the partners we work with and among non-profits and governments in general. Our project partners are partners, not clients. This means that the fellows work with the partners, not for them. We believe that participating in this program helps both the partners and the fellows develop a common language. Sometimes fellows and partners won't see eye to eye on every decision, or the need to complete work within a deadline will mean you have to adjust your expectations, put GNU/Emacs 1 on the back burner, or sacrifice doing the work exactly the way you had hoped. While fellows' learning is our primary priority, it is important to note that part of what fellows are learning is how to work with partners to produce work that is useful for the partners and delivered on time.","title":"Introducing Data Science to the Social Sector"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#building-a-community-of-data-scientists-for-social-good","text":"We hope -- and expect -- that your impact as a DSSG alum continues beyond your summer tenure. Throughout the summer, we will introduce you to other practitioners within the data science and social good spaces to help you understand these sectors, form relationships, and start to think about potential contributions you could make. You\u2019ll also have the opportunity to network with local data science, tech, government, non-profit, and startup communities through regular fellowship-sponsored happy hours and meetups. Whether it\u2019s continuing to collaborate with your DSSG colleagues on other social good projects, joining the data team at a government agency, or working to recruit other like-minded people to apply their in-demand data science skills to impactful problems, we hope that this is just the beginning of of a lifetime as a data scientist for social good.","title":"Building a Community of Data Scientists for Social Good"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#our-hopes-for-your-fellowship-experience","text":"The fellowship provides you with the opportunity to learn; to work on important, challenging, and unique projects; and to meet a lot of people who share your interests and goals. It is up to you to take advantage of these opportunities. We hope you use the summer to: Meet a group of fellows, staff and project partners who have a wealth of skills and experiences; listen to and learn from them; and make new friends. Embrace gaps in your knowledge as learning opportunities, exploring your limitations with respect to technical skills, new methods from different disciplines, project management, social issues, and teamwork. Learn about the challenges of working on real projects that don\u2019t have clean data, guaranteed results, or elegant solutions. Navigate the triangle between learning technical skills, creating deliverables that are useful and actionable for your project partner, and putting the varied skills within your team to good use. Rise to the challenge of working on a team that will include strong personalities with diverse experiences and strengths Realize the ambiguity and uncertainty that comes with working in a traditionally less tech-savvy sector, and learn how to communicate effectively to bridge this gap. Explore the impact (intentional and unintentional) that working with data from real, often disadvantaged or marginalized, people might have on them. Think deeply about the scope and limitations of technology to improve social problems. Explore existing roles in the field of data science for social good, find one you are best suited for, or create your own. Note Throughout the summer, we encourage you to share your ideas about how to improve the fellowship experience for yourself and others -- and to put them into action. We are constantly trying to improve the fellowship every day throughout the summer, and over time as we learn from each cohort.","title":"Our Hopes for Your Fellowship Experience"},{"location":"dssg-manual/conduct-culture-and-communications/goals/#our-expectations-of-fellows","text":"The fellowship offers a lot of freedom; however, we expect all fellows to stick to the basic principles of conduct listed below at all times. These guidelines apply to everybody at the fellowship, including mentors, project managers, and the fellowship organizers. If you feel that anyone is not behaving in accordance with these guidelines, we invite you to bring it up constructively. If you are unsure who to address, or if you do not feel comfortable doing so directly, you can bring up your concerns with the fellow advocate. Be supportive, open-minded, and willing to compromise. DSSG brings together people from different backgrounds and with different skills. In fact, this might be the best resource the fellowship has to offer! Share your knowledge and your experience with each other. Be patient as you teach each other, and have an open ear for your peers. Be professional. You will be working with NGOs, non-profits, and government institutions as project partners. You will also be presenting your work at and attending events with the general public. In all of these capacities you are acting as a representative of the DSSG community. We expect you to be professional \u2014 that is, respectful, friendly, and on time \u2014 in your conduct with partners and the public alike. Be resourceful and pragmatic. Own your own learning. Seek out resources as necessary. Don't be shy to ask others for help, but be mindful of their time - tell them what what you do understand, where you're stuck, and what you\u2019ve already tried, so they know how they can help. When you notice problems or have ideas for improvements \u2014 be it in your team, your project, or the fellowship \u2014 don\u2019t rely on others to notice or fix them; offer your initiative. Deal with conflicts maturely. There are many potential sources of conflict throughout the fellowship. It is perfectly acceptable, and even expected, that you will run into conflicts with your team, your mentors, your project, or the fellowship organizers. In any case, we ask you to be productive, pragmatic, and mature when dealing with conflicts. Keep an open mind, listen and communicate with everybody involved. Neither your project, nor your team, nor the fellowship will be perfect. Remember that everyone involved has invested a lot in the fellowship and wants all participants to be happy. Show up. We have all committed to be here for the duration of the program. The fellowship runs from approximately from end of May through the end of August 2 . We expect you to be in the office five days a week, to attend all DSSG-wide sessions, occasional special events, and the final event. We recognize that you have a life outside of the fellowship, and if you have any known or potential absences, you must inform DSSG staff upon your acceptance. Any additional conflicts that arise during the summer must be discussed with and agreed between you, your team, and your project manager well ahead of time. Take care of the space. Offices, meeting rooms, and kitchen areas are shared spaces. It\u2019s everybody\u2019s job to keep the space clean and free of messes. This policy also applies when you are attending off-site events. Stay involved and act as a steward. As a member of the DSSG community, we expect a commitment from you to stay involved, even after the summer. We ask you to seek opportunities to present the work you did, whether it's at your university, company, or events in your area. We ask you to assist your team in writing publications about your project, both during and after the fellowship. We will also ask you to help us with the application process in the following years by reviewing applications and interviewing candidates. We ask that you do what you're able to contribute to the DSSG mission and community. DON'T PANIC Regardless of how much experience you have, we admitted you because we believe that you can make a valuable contribution to your cohort, and we think being a DSSG fellow will help prepare you to do data science for social good in the real world. We've made a commitment to you and want to do everything we can to help you succeed. This is really important, so we'll say it again, in bold: If you're reading this, you are here because we want you to be here and believe that you are ready to make an impact. For example, don't worry about how much more or less productive, knowledgable, or experienced other fellows in your cohort might appear to be. It's easy to only pay attention - and compare yourself - to those who seem to be doing particularly well. Know that everyone has their own struggles, and everyone has good and bad days. Or if you prefer learning VIM \u21a9 In US: from Memorial Day through Labor Day, specific dates change by year and location \u21a9","title":"Our Expectations of Fellows"},{"location":"dssg-manual/summer-overview/","text":"What to Expect # While specific schedules will vary from project to project, the summer will follow roughly the structure below. See also high level summer plan for an outline of the flow of the summer, and general concepts that will inform the topics addressed by tutorials and speakers as well as what we'll ask fellows to present about in weekly updates and deep dives. Before the Summer # Prior to your arrival, we provide you with the prerequisites so you can familiarize yourself with the tools you\u2019ll use all summer and equip yourself with the knowledge to be able to follow along with the curriculum. You'll receive a list of software to install before the first day of orientation, programming languages you should brush up on, and tools we suggest you use to manage your data workflow. We will also send you a tentative list of projects and ask for you to respond with your preferences. Based on your preferences, the requirements of each project, and the balance of disciplines within each team, we will assign teams of 3-4 fellows per project. You will learn which project you've been assigned during the second week of the fellowship. Learning about Projects and Partners # Staff at the Center for Data Science and Public Policy (DSaPP) work hard year-round to recruit partners and scope projects. This is a lengthy, complicated process with plenty of logistical hurdles (think legal data sharing agreements and data transfer challenges), which means the list of project partners is usually not finalized until the fellowship begins. You will find out your project and team assignments in the second week of the fellowship. We ask all our project partners to come meet their team in the first two weeks of the summer. During partner visits, you\u2019ll spend a lot of time talking through the problem and the data with them, and they give a presentation to the fellowship. We want you to meet the people you\u2019re working with face to face. This also gives all of the fellows a chance to hear about all of the projects and for the partners to meet other project partners and the other fellows. After orientation, you will spend the first part of the summer getting to know your project partner and their unique challenges. While the projects have already been scoped, you will almost certainly need to refine that scope throughout the summer. For example, we may know your partner\u2019s goal is to find violations of a particular law. Your team would then work with the partner to narrow that to: (1) locations at risk of violations in general; (2) locations at risk of multiple violations; or (3) locations with the most impactful violations. We believe it is important for you to thoroughly understand the problem and the process that gives rise to the data before getting too entrenched in the data itself. A deep understanding of your partner and the problems they face is crucial to knowing what your variables really mean, and defining your outcome and evaluation metrics will depend heavily upon this understanding as well. While we know you are eager to dig into the data, you will find that at least as much of your time is devoted to talking about the data as manipulating it. This is a good thing. You will also be working with real data, which is messy! You will encounter missing values and things that don\u2019t seem to make sense. Talking to your partners and telling them what you see from looking at the data they\u2019ve given you will help you evaluate your own understanding, reconcile inconsistencies (or carry on being aware of them), and identify whether the errors lie in the data itself or in whatever preconceived notions you had. Working on Your Project # Although we aim to have all the data from project partners ready well in advance of the fellowship, there are inevitably data transfer delays and partial data that will continue to be augmented throughout - and sometimes after - the fellowship. As you explore, you'll find holes in the provided information, or identify potential new useful sources of data, and will need to work with your partner to decide whether it\u2019s possible to acquire the data you need in the time that you have; that\u2019s the reality of working with real world partners and sensitive data. Fellows drive the work of every project, learning their subject matter in depth, writing code, and collaborating with their project partners to develop something useful and usable. Over the course of the summer, your team will: Explore the (real) data your partners collect Design your project workflow based on what tools you'll use and how your team works together Identify user stories to make sure what you're creating has a real purpose Develop a machine learning pipeline to turn raw data into analysis that can inform decisions Build relevant models that reflect the subject you're analyzing as closely as possible Add features to your model based on subject matter expertise, available data, and exploratory analysis Evaluate model performance using the metrics that make sense for your project Create an interface for your partner to use your results (API, dashboard applications, etc) Presentations # We believe that our work is only useful if we are able to communicate what we do and why it\u2019s important to our partners, peers, and the general public. As such, an important piece of this training program is learning to present the work you do . Each week, a member of your team will give a 2-3 minute update to the entire fellowship, outlining your recent progress and findings, giving shoutouts to other fellows or staff members who have helped you along the way, and things you're stuck on and are seeking help with. Two teams per week (so each team presents twice throughout the summer) will give a longer 20-minute \"deep dive\" presentation, outlining more technical components of your project and seeking feedback from other fellows and mentors. At the end of the summer\u2026 At the end of the summer, your team will develop two polished presentations : one 5-minute presentation for the DataFest final event, and one 20-minute technical presentation for use at a local tech meetup at the end of the summer and for future presentations at conferences or your home institution. In the last few weeks of the fellowship, each team will present at a local meetup. Each team will elect one team member to deliver the short final presentation at DataFest; however, all team members should feel comfortable delivering all presentations. Our communications staff will work with you on both of these presentations, brainstorming ways to present your work and providing feedback on your delivery. Wrap-Up and Handovers # To make sure the work you do this summer has real impact, a lot more work needs to be done after the official end of the fellowship; some of this will be done by your project partner, and some will be done by the host University (or by the Center for Data Science and Public Policy at the University of Chicago). You will need to transition the work over to your project partners so they can validate, implement, and extend your work. To do this, you will have to document your work throughout the summer and wrap it up neatly at the end of the summer. We ask that you prepare a poster to be displayed at DSSG events and for potential conference poster sessions, a technical report, and an outline of a paper. This makes it easier to collaborate once you and your team mates are no longer working at the same desk every day. We also ask that all your code can be run on a new machine, and that there is sufficient documentation for someone else to replicate and understand your work. Curriculum # Our number one goal is to train the fellows to do data science for social good work. Here is some insight into how we accomplish this throughout the summer. To look through all of our curriculum materials, please see the curriculum section . Orientation # We expect that every incoming fellow has experience programming in Python, a basic working knowledge of statistics and social science, and an interest in doing social good. However, we understand that everyone comes from a different background, so to ensure that everyone is able to contribute as a productive member of the team and the fellowship, we start the first few weeks off with an intensive orientation , (e.g. DSSG 2016) getting everyone \"up to speed\" with the basic skills and tools they'll need. Possible schedule # Week One Prerequisites Software Setup Pipelines and Project Workflow Git and Github Making the Fellowship Skills You Need to do DSSG Command Line Tools Project Management, Partners, and Communications Data Exploration in Python Project Scoping Intro Week Two Usability and User Interfaces CSV to DB Legal Agreements Data Security Primer Legible, Good Code Conduct, Culture, and Communications Week Three Reproducible ETL The Work We Do Record Linkage Databases Quantitative Social Science Ongoing Curriculum # Training continues on throughout the summer in the form of \"lunch and learns\" - less formal lessons over lunch - and teachouts by staff or fellows who have relevant specializations. Sometimes we ask for volunteers to do a teachout on a topic we think is important, like data visualization or inference with observational data, and a few fellows will work together to put together a lesson. Sometimes a DSSGer will suggest a topic that they have a pet interest in, or that they think will be relevant to one or more of the summer projects. We have lunch and learns scheduled twice a week through the summer, and some fellows choose to offer optional teachouts at the end of the workday. Although we don't expect all teams to be working in unison, there is a general structure to the summer that guides how we pace the remaining curriculum - we try to schedule topics so that fellows know about them with enough time to incorporate them into their projects, but not so early that they've forgotten about what they learned by the time the knowledge would be useful. As we get nearer to the end of the summer, there are fewer required topics, so there are more open time slots for fellows to do teachouts. Example topics for the Rest of the Summer Educational Data and Testing Social Good Business Models Basic Web Scraping Pipelines and Evaluation Feature Generation Workshop Test, Test, Test Beyond the Deep Learning Hype Causal Inference with Observational Data Model Evaluation Spatial Analysis Tools Operations Research Theory and Theorizing in the Social Sciences Web Classification Presentation Skills Data Visualization Natural Language Processing Opening Closed Data","title":"Summer overview"},{"location":"dssg-manual/summer-overview/#what-to-expect","text":"While specific schedules will vary from project to project, the summer will follow roughly the structure below. See also high level summer plan for an outline of the flow of the summer, and general concepts that will inform the topics addressed by tutorials and speakers as well as what we'll ask fellows to present about in weekly updates and deep dives.","title":"What to Expect"},{"location":"dssg-manual/summer-overview/#before-the-summer","text":"Prior to your arrival, we provide you with the prerequisites so you can familiarize yourself with the tools you\u2019ll use all summer and equip yourself with the knowledge to be able to follow along with the curriculum. You'll receive a list of software to install before the first day of orientation, programming languages you should brush up on, and tools we suggest you use to manage your data workflow. We will also send you a tentative list of projects and ask for you to respond with your preferences. Based on your preferences, the requirements of each project, and the balance of disciplines within each team, we will assign teams of 3-4 fellows per project. You will learn which project you've been assigned during the second week of the fellowship.","title":"Before the Summer"},{"location":"dssg-manual/summer-overview/#learning-about-projects-and-partners","text":"Staff at the Center for Data Science and Public Policy (DSaPP) work hard year-round to recruit partners and scope projects. This is a lengthy, complicated process with plenty of logistical hurdles (think legal data sharing agreements and data transfer challenges), which means the list of project partners is usually not finalized until the fellowship begins. You will find out your project and team assignments in the second week of the fellowship. We ask all our project partners to come meet their team in the first two weeks of the summer. During partner visits, you\u2019ll spend a lot of time talking through the problem and the data with them, and they give a presentation to the fellowship. We want you to meet the people you\u2019re working with face to face. This also gives all of the fellows a chance to hear about all of the projects and for the partners to meet other project partners and the other fellows. After orientation, you will spend the first part of the summer getting to know your project partner and their unique challenges. While the projects have already been scoped, you will almost certainly need to refine that scope throughout the summer. For example, we may know your partner\u2019s goal is to find violations of a particular law. Your team would then work with the partner to narrow that to: (1) locations at risk of violations in general; (2) locations at risk of multiple violations; or (3) locations with the most impactful violations. We believe it is important for you to thoroughly understand the problem and the process that gives rise to the data before getting too entrenched in the data itself. A deep understanding of your partner and the problems they face is crucial to knowing what your variables really mean, and defining your outcome and evaluation metrics will depend heavily upon this understanding as well. While we know you are eager to dig into the data, you will find that at least as much of your time is devoted to talking about the data as manipulating it. This is a good thing. You will also be working with real data, which is messy! You will encounter missing values and things that don\u2019t seem to make sense. Talking to your partners and telling them what you see from looking at the data they\u2019ve given you will help you evaluate your own understanding, reconcile inconsistencies (or carry on being aware of them), and identify whether the errors lie in the data itself or in whatever preconceived notions you had.","title":"Learning about Projects and Partners"},{"location":"dssg-manual/summer-overview/#working-on-your-project","text":"Although we aim to have all the data from project partners ready well in advance of the fellowship, there are inevitably data transfer delays and partial data that will continue to be augmented throughout - and sometimes after - the fellowship. As you explore, you'll find holes in the provided information, or identify potential new useful sources of data, and will need to work with your partner to decide whether it\u2019s possible to acquire the data you need in the time that you have; that\u2019s the reality of working with real world partners and sensitive data. Fellows drive the work of every project, learning their subject matter in depth, writing code, and collaborating with their project partners to develop something useful and usable. Over the course of the summer, your team will: Explore the (real) data your partners collect Design your project workflow based on what tools you'll use and how your team works together Identify user stories to make sure what you're creating has a real purpose Develop a machine learning pipeline to turn raw data into analysis that can inform decisions Build relevant models that reflect the subject you're analyzing as closely as possible Add features to your model based on subject matter expertise, available data, and exploratory analysis Evaluate model performance using the metrics that make sense for your project Create an interface for your partner to use your results (API, dashboard applications, etc)","title":"Working on Your Project"},{"location":"dssg-manual/summer-overview/#presentations","text":"We believe that our work is only useful if we are able to communicate what we do and why it\u2019s important to our partners, peers, and the general public. As such, an important piece of this training program is learning to present the work you do . Each week, a member of your team will give a 2-3 minute update to the entire fellowship, outlining your recent progress and findings, giving shoutouts to other fellows or staff members who have helped you along the way, and things you're stuck on and are seeking help with. Two teams per week (so each team presents twice throughout the summer) will give a longer 20-minute \"deep dive\" presentation, outlining more technical components of your project and seeking feedback from other fellows and mentors. At the end of the summer\u2026 At the end of the summer, your team will develop two polished presentations : one 5-minute presentation for the DataFest final event, and one 20-minute technical presentation for use at a local tech meetup at the end of the summer and for future presentations at conferences or your home institution. In the last few weeks of the fellowship, each team will present at a local meetup. Each team will elect one team member to deliver the short final presentation at DataFest; however, all team members should feel comfortable delivering all presentations. Our communications staff will work with you on both of these presentations, brainstorming ways to present your work and providing feedback on your delivery.","title":"Presentations"},{"location":"dssg-manual/summer-overview/#wrap-up-and-handovers","text":"To make sure the work you do this summer has real impact, a lot more work needs to be done after the official end of the fellowship; some of this will be done by your project partner, and some will be done by the host University (or by the Center for Data Science and Public Policy at the University of Chicago). You will need to transition the work over to your project partners so they can validate, implement, and extend your work. To do this, you will have to document your work throughout the summer and wrap it up neatly at the end of the summer. We ask that you prepare a poster to be displayed at DSSG events and for potential conference poster sessions, a technical report, and an outline of a paper. This makes it easier to collaborate once you and your team mates are no longer working at the same desk every day. We also ask that all your code can be run on a new machine, and that there is sufficient documentation for someone else to replicate and understand your work.","title":"Wrap-Up and Handovers"},{"location":"dssg-manual/summer-overview/#curriculum","text":"Our number one goal is to train the fellows to do data science for social good work. Here is some insight into how we accomplish this throughout the summer. To look through all of our curriculum materials, please see the curriculum section .","title":"Curriculum"},{"location":"dssg-manual/summer-overview/#orientation","text":"We expect that every incoming fellow has experience programming in Python, a basic working knowledge of statistics and social science, and an interest in doing social good. However, we understand that everyone comes from a different background, so to ensure that everyone is able to contribute as a productive member of the team and the fellowship, we start the first few weeks off with an intensive orientation , (e.g. DSSG 2016) getting everyone \"up to speed\" with the basic skills and tools they'll need.","title":"Orientation"},{"location":"dssg-manual/summer-overview/#possible-schedule","text":"Week One Prerequisites Software Setup Pipelines and Project Workflow Git and Github Making the Fellowship Skills You Need to do DSSG Command Line Tools Project Management, Partners, and Communications Data Exploration in Python Project Scoping Intro Week Two Usability and User Interfaces CSV to DB Legal Agreements Data Security Primer Legible, Good Code Conduct, Culture, and Communications Week Three Reproducible ETL The Work We Do Record Linkage Databases Quantitative Social Science","title":"Possible schedule"},{"location":"dssg-manual/summer-overview/#ongoing-curriculum","text":"Training continues on throughout the summer in the form of \"lunch and learns\" - less formal lessons over lunch - and teachouts by staff or fellows who have relevant specializations. Sometimes we ask for volunteers to do a teachout on a topic we think is important, like data visualization or inference with observational data, and a few fellows will work together to put together a lesson. Sometimes a DSSGer will suggest a topic that they have a pet interest in, or that they think will be relevant to one or more of the summer projects. We have lunch and learns scheduled twice a week through the summer, and some fellows choose to offer optional teachouts at the end of the workday. Although we don't expect all teams to be working in unison, there is a general structure to the summer that guides how we pace the remaining curriculum - we try to schedule topics so that fellows know about them with enough time to incorporate them into their projects, but not so early that they've forgotten about what they learned by the time the knowledge would be useful. As we get nearer to the end of the summer, there are fewer required topics, so there are more open time slots for fellows to do teachouts. Example topics for the Rest of the Summer Educational Data and Testing Social Good Business Models Basic Web Scraping Pipelines and Evaluation Feature Generation Workshop Test, Test, Test Beyond the Deep Learning Hype Causal Inference with Observational Data Model Evaluation Spatial Analysis Tools Operations Research Theory and Theorizing in the Social Sciences Web Classification Presentation Skills Data Visualization Natural Language Processing Opening Closed Data","title":"Ongoing Curriculum"},{"location":"tech-tutorials/model_eval/","text":"Model Evaluation and Bias in Data Science Projects # We have 6 lessons - two sessions a day over three days. One session a day (or the latter half of the second session in a given day) should be hands-on. We can give homework, which could simply be the 'leftovers' of a given day's hands-on session. Day 1 introduction & brief outline of workshop examples of DSaPP projects explanation of common data structure (temporal data, entities, ...) examples of common features (spatiotemporal aggregations, prior behavior, 'static' features like demographics) examples of common outcome definitions ('binarizations') What do we mean by 'successful' predictions? Examples of deployments; explain resource constraints. accuracy, precision, ROC top-k precision, recall-precision curves list stability Detour into caveats: We are not working causally. We are not (most of the time) modeling inspection densities. We are not (most of the time) providing statistical inference, optimality guarantees, or anything like that. Delineate 'science' vs 'engineering' approach (here, we are sharing conceptual and technical tools, not scientific, or even empirical, knowledge). Encourage questions & suggestions throughout workshop. introduce toy/example dataset and have students connect to SQL DB ( maybe in next session? ) simple temporal cross-validation ('as-of' date, label window, feature window, metrics) train, test, evaluation sets (TODO: ensure universal terminology) caveats on temporal cross-validation: incorrect dates, overwritten or backfilled data (example: student dropouts), data updating frequencies (e.g. end-of-year reports), non-stationary derivatives in the data, changes in policy, changes in systems, bias in the labelling process (e.g. biased judges) Exercise: Implement temporal split generator, label generator, feature getter, label-feature join, some evaluation metrics, on toy data set with pre-built features Day 2 difference between incident and label dates, and how that influence label generation; caveat on correlates of the incident that are predictive of the label leakage, and examples of hard-to-find leakage prediction horizons (e.g. in education) prediction frequencies (depending on deployment) overlapping label windows (caveat: dependent 'samples') overlapping feature windows (generally no problem) label rank breaking (unknown k vs additional randomization) 'status list' of entities - finding labels for null entities in a given label window imputation - finding features for null entities in a given feature window; counts/mean/median caveat on leakage dummification, and the problems of knowing the set of all levels upfront \"train however you like, test however you deploy\" sub-sampling of training and/or testing data (?) making train and/or test sets more reprensetative (?) modeling/weighting by P(I) / inverse probability weighting (?) evaluating predictions in field trials / problems of randomized trials on ranked lists (?) novelty vs accuracy (Cincinnati?) stability across temporal splits Exercise: Implement label generator with incident-vs-decision date, in-split imputation, global dummification, plots for temporal stability Day 3 characterizing models via entities, not via labels (back ref: list overlap) cross-tabs on categorical features or discretized continuous features between high-risk/low-risk predictions scatter plots on continuous features simple significance tests (chi-square, KS-test); caveat: multiple testing, dependency between features protected class definitions not sufficient: removing protected features population parity disparate impact test equality extension from equal FNR/FPR to independence of score and class; removing dependency on thresholds (is there an equivalent formulation for equal precision?) caveat about curse of dimensionality - marginal bias is 'easy', but interactions of bias are very many (e.g. difficulties of testing not only for discrimination against age, or race, or gender, but also for discrimination against (age x race x gender) ) science thriller: ProPublica's COMPASS coverage, initial response articles, follow-up articles Exercise: calculating cross-tabs on toy data set; calculating condtioned cross-tabs and scatter plots for disparate impact and test equality; applying chi-square test","title":"Model Evaluation and Bias in Data Science Projects"},{"location":"tech-tutorials/model_eval/#model-evaluation-and-bias-in-data-science-projects","text":"We have 6 lessons - two sessions a day over three days. One session a day (or the latter half of the second session in a given day) should be hands-on. We can give homework, which could simply be the 'leftovers' of a given day's hands-on session. Day 1 introduction & brief outline of workshop examples of DSaPP projects explanation of common data structure (temporal data, entities, ...) examples of common features (spatiotemporal aggregations, prior behavior, 'static' features like demographics) examples of common outcome definitions ('binarizations') What do we mean by 'successful' predictions? Examples of deployments; explain resource constraints. accuracy, precision, ROC top-k precision, recall-precision curves list stability Detour into caveats: We are not working causally. We are not (most of the time) modeling inspection densities. We are not (most of the time) providing statistical inference, optimality guarantees, or anything like that. Delineate 'science' vs 'engineering' approach (here, we are sharing conceptual and technical tools, not scientific, or even empirical, knowledge). Encourage questions & suggestions throughout workshop. introduce toy/example dataset and have students connect to SQL DB ( maybe in next session? ) simple temporal cross-validation ('as-of' date, label window, feature window, metrics) train, test, evaluation sets (TODO: ensure universal terminology) caveats on temporal cross-validation: incorrect dates, overwritten or backfilled data (example: student dropouts), data updating frequencies (e.g. end-of-year reports), non-stationary derivatives in the data, changes in policy, changes in systems, bias in the labelling process (e.g. biased judges) Exercise: Implement temporal split generator, label generator, feature getter, label-feature join, some evaluation metrics, on toy data set with pre-built features Day 2 difference between incident and label dates, and how that influence label generation; caveat on correlates of the incident that are predictive of the label leakage, and examples of hard-to-find leakage prediction horizons (e.g. in education) prediction frequencies (depending on deployment) overlapping label windows (caveat: dependent 'samples') overlapping feature windows (generally no problem) label rank breaking (unknown k vs additional randomization) 'status list' of entities - finding labels for null entities in a given label window imputation - finding features for null entities in a given feature window; counts/mean/median caveat on leakage dummification, and the problems of knowing the set of all levels upfront \"train however you like, test however you deploy\" sub-sampling of training and/or testing data (?) making train and/or test sets more reprensetative (?) modeling/weighting by P(I) / inverse probability weighting (?) evaluating predictions in field trials / problems of randomized trials on ranked lists (?) novelty vs accuracy (Cincinnati?) stability across temporal splits Exercise: Implement label generator with incident-vs-decision date, in-split imputation, global dummification, plots for temporal stability Day 3 characterizing models via entities, not via labels (back ref: list overlap) cross-tabs on categorical features or discretized continuous features between high-risk/low-risk predictions scatter plots on continuous features simple significance tests (chi-square, KS-test); caveat: multiple testing, dependency between features protected class definitions not sufficient: removing protected features population parity disparate impact test equality extension from equal FNR/FPR to independence of score and class; removing dependency on thresholds (is there an equivalent formulation for equal precision?) caveat about curse of dimensionality - marginal bias is 'easy', but interactions of bias are very many (e.g. difficulties of testing not only for discrimination against age, or race, or gender, but also for discrimination against (age x race x gender) ) science thriller: ProPublica's COMPASS coverage, initial response articles, follow-up articles Exercise: calculating cross-tabs on toy data set; calculating condtioned cross-tabs and scatter plots for disparate impact and test equality; applying chi-square test","title":"Model Evaluation and Bias in Data Science Projects"}]}